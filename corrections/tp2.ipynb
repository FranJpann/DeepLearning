{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TE2ItlsI956"
   },
   "source": [
    "# Deep Learning - Introduction à Pytorch \n",
    "\n",
    "\n",
    "## TP2 : Fonctions Dérivables\n",
    "\n",
    "Sylvain Lamprier (sylvain.lamprier@univ-angers.fr)\n",
    "\n",
    "Supports adaptés de Nicolas Baskiotis (nicolas.baskiotis@sorbonne-univeriste.fr) et Benjamin Piwowarski (benjamin.piwowarski@sorbonne-universite.fr) -- MLIA/ISIR, Sorbonne Université"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3Y9YOOHHhJKY",
    "ExecuteTime": {
     "end_time": "2024-02-08T12:03:18.344067889Z",
     "start_time": "2024-02-08T12:03:15.652780720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La version de torch est :  2.2.0+cu121\n",
      "Le calcul GPU est disponible ?  False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"La version de torch est : \",torch.__version__)\n",
    "print(\"Le calcul GPU est disponible ? \", torch.cuda.is_available())\n",
    "\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au TP précédent, nous avons vu comment implémenter une regression linéaire en utilisant les structures Tensor de PyTorch. Cependant, nous exploitions pas du tout la puissance de PyTorch qui permet de faciliter le calcul des gradients via de l'auto-dérivation. Dans le TP précédent nous avions défini un algorithme spécifique à de la regression pour un modèle (linéaire) et un coût (moindres carrés) figés, en définissant à la main le gradient du coût global pour l'ensemble des paramètres. Ce mode de programmation est très peu modulaire et est très difficilement étendable à des architectures plus complexes. Sachant que l'objectif est de développer des architectures neuronales avec des nombreux modules neuronaux enchaînés, il n'est pas possible de travailler de cette façon. \n",
    "\n",
    "Dans ce TP, nous allons voir comment décomposer les choses pour rendre le code plus facilement généralisable. L'objectif est de comprendre le fonctionnement interne de PyTorch (sans en utiliser encore les facilités offertes par l'utilisation d'un graphe de calcul), basé sur l'implémentation d'objets Function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions\n",
    "\n",
    "\n",
    "$\\href{https://pytorch.org/docs/stable/}{\\texttt{PyTorch}}$ utilise une classe abstraite $\\href{https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function}{\\texttt{Function}}$ dont sont héritées toutes les fonctions et qui nécessite l'implémentation de ces deux méthodes :\n",
    "\n",
    "- méthode $\\texttt{forward(ctx, *inputs)}$ : calcule le résultat de l'application de la fonction\n",
    "- méthode $\\texttt{backward(ctx, *grad-outputs)}$ : calcule le gradient partiel par rapport à chaque entrée de la méthode $\\texttt{forward}$; le nombre de $\\texttt{grad-outputs}$ doit être égale aux nombre de sorties de $\\texttt{forward}$ (pourquoi ?) et le nombre de  \n",
    "sorties doit être égale aux nombres de $\\texttt{inputs}$ de $\\texttt{forward}$.\n",
    "\n",
    "\n",
    "Pour des raisons d'implémentation, les deux méthodes doivent être statiques. Le premier paramètre $\\texttt{ctx}$ permet de sauvegarder un contexte lors de la passe $\\texttt{forward}$ (par exemple les tenseurs d'entrées) et il est passé lors de la passe $\\texttt{backward}$ en paramètre afin de récupérer les valeurs. $\\textbf{Attention : }$ le contexte doit être unique pour chaque appel de $\\texttt{forward}$.\n",
    "\n",
    "Compléter le code ci-dessous pour créer des modules MSE (coût moindres carrés) et Linéaire. Les deux cellules en dessous vous serviront à tester votre code: si tout se passe sans plantage, alors vos gradients semblent corrects. Utiliser bien les outils propres à pyTorch, en particulier des Tensor et pas des matrices numpy. Assurez vous que\n",
    "vos fonctions prennent en entrée des batchs d’exemples (matrice 2D) et non un seul exemple (vecteur). N’hésiter pas à prendre un exemple et déterminer les dimensions des différentes matrices en jeu.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T12:03:18.365177525Z",
     "start_time": "2024-02-08T12:03:18.355960942Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import gradcheck\n",
    "\n",
    "class Context:\n",
    "    \"\"\"Un objet contexte très simplifié pour simuler PyTorch\n",
    "    \n",
    "    Un contexte différent doit être utilisé à chaque forward\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._saved_tensors = ()\n",
    "\n",
    "    def save_for_backward(self, *args):\n",
    "        self._saved_tensors = args\n",
    "\n",
    "    @property\n",
    "    def saved_tensors(self):\n",
    "        return self._saved_tensors\n",
    "\n",
    "\n",
    "class MSE(Function):\n",
    "    \"\"\"Début d'implémentation de la fonction MSE\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, yhat, y):\n",
    "        ## Garde les valeurs nécessaires pour le backward\n",
    "        ctx.save_for_backward(yhat, y)\n",
    "        return torch.pow(yhat - y, 2).mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        ## Calcul du gradient du module par rapport à chaque groupe d'entrées\n",
    "        yhat, y = ctx.saved_tensors\n",
    "        grad_yhat = 2 * (yhat - y) / yhat.numel()\n",
    "        grad_y = -2 * (yhat - y) / yhat.numel()\n",
    "        return grad_yhat * grad_output, grad_y * grad_output\n",
    "\n",
    "\n",
    "class Linear(Function):\n",
    "    \"\"\"Début d'implémentation de la fonction Linear\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, W, b):\n",
    "        ctx.save_for_backward(X, W, b)\n",
    "        return (X @ W) + b\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        X, W, b = ctx.saved_tensors\n",
    "        grad_X = grad_output @ W.T\n",
    "        grad_W = (grad_output.t() @ X).t()\n",
    "        grad_b = grad_output.sum(0)\n",
    "        return grad_X, grad_W, grad_b\n",
    "\n",
    "\n",
    "# Utiliser gradcheck\n",
    "\n",
    "mse = MSE.apply\n",
    "linear = Linear.apply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T12:03:18.905869934Z",
     "start_time": "2024-02-08T12:03:18.367881894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test du gradient de MSE \n",
    "yhat = torch.randn(10,5, requires_grad=True, dtype=torch.float64)\n",
    "y = torch.randn(10,5, requires_grad=True, dtype=torch.float64)\n",
    "torch.autograd.gradcheck(mse, (yhat, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T12:03:19.009046430Z",
     "start_time": "2024-02-08T12:03:18.905998064Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test du gradient de Linear (sur le même modèle que MSE)\n",
    "\n",
    "x = torch.randn(13, 5,requires_grad=True,dtype=torch.float64)\n",
    "w = torch.randn(5, 7,requires_grad=True,dtype=torch.float64)\n",
    "b = torch.randn(7,requires_grad=True,dtype=torch.float64)\n",
    "torch.autograd.gradcheck(linear,(x,w,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descente de Gradient\n",
    "\n",
    "Compléter ci-dessous le code pour réaliser la même regression linéaire qu'au TP précédent, mais en utilisant les objets Function déclarés ci-dessus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T12:03:56.723150496Z",
     "start_time": "2024-02-08T12:03:18.990387952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'exemples :  20640 Dimension :  8\n",
      "Nom des attributs :  MedInc, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude\n",
      "Itérations 0: loss 50329.48046875\n",
      "Itérations 100: loss 225.13568115234375\n",
      "Itérations 200: loss 192.34078979492188\n",
      "Itérations 300: loss 166.9883575439453\n",
      "Itérations 400: loss 147.37355041503906\n",
      "Itérations 500: loss 132.18211364746094\n",
      "Itérations 600: loss 120.40037536621094\n",
      "Itérations 700: loss 111.24787139892578\n",
      "Itérations 800: loss 104.1223373413086\n",
      "Itérations 900: loss 98.5596923828125\n",
      "Itérations 1000: loss 94.20233917236328\n",
      "Itérations 1100: loss 90.77433013916016\n",
      "Itérations 1200: loss 88.0631332397461\n",
      "Itérations 1300: loss 85.90484619140625\n",
      "Itérations 1400: loss 84.17312622070312\n",
      "Itérations 1500: loss 82.7706298828125\n",
      "Itérations 1600: loss 81.62216186523438\n",
      "Itérations 1700: loss 80.67002868652344\n",
      "Itérations 1800: loss 79.86949920654297\n",
      "Itérations 1900: loss 79.18610382080078\n",
      "Itérations 2000: loss 78.59352111816406\n",
      "Itérations 2100: loss 78.0713119506836\n",
      "Itérations 2200: loss 77.60345458984375\n",
      "Itérations 2300: loss 77.1778793334961\n",
      "Itérations 2400: loss 76.78549194335938\n",
      "Itérations 2500: loss 76.41883850097656\n",
      "Itérations 2600: loss 76.0725326538086\n",
      "Itérations 2700: loss 75.74221801757812\n",
      "Itérations 2800: loss 75.4244155883789\n",
      "Itérations 2900: loss 75.11651611328125\n",
      "Itérations 3000: loss 74.8166275024414\n",
      "Itérations 3100: loss 74.52339172363281\n",
      "Itérations 3200: loss 74.2353515625\n",
      "Itérations 3300: loss 73.95179748535156\n",
      "Itérations 3400: loss 73.67206573486328\n",
      "Itérations 3500: loss 73.39561462402344\n",
      "Itérations 3600: loss 73.12200927734375\n",
      "Itérations 3700: loss 72.85092163085938\n",
      "Itérations 3800: loss 72.58209991455078\n",
      "Itérations 3900: loss 72.31533813476562\n",
      "Itérations 4000: loss 72.05048370361328\n",
      "Itérations 4100: loss 71.78742980957031\n",
      "Itérations 4200: loss 71.5260238647461\n",
      "Itérations 4300: loss 71.2662582397461\n",
      "Itérations 4400: loss 71.0080337524414\n",
      "Itérations 4500: loss 70.75130462646484\n",
      "Itérations 4600: loss 70.49600982666016\n",
      "Itérations 4700: loss 70.24214935302734\n",
      "Itérations 4800: loss 69.9897232055664\n",
      "Itérations 4900: loss 69.73867797851562\n",
      "Itérations 5000: loss 69.48896789550781\n",
      "Itérations 5100: loss 69.24053192138672\n",
      "Itérations 5200: loss 68.99346923828125\n",
      "Itérations 5300: loss 68.74772644042969\n",
      "Itérations 5400: loss 68.50324249267578\n",
      "Itérations 5500: loss 68.26013946533203\n",
      "Itérations 5600: loss 68.01821899414062\n",
      "Itérations 5700: loss 67.77763366699219\n",
      "Itérations 5800: loss 67.53829956054688\n",
      "Itérations 5900: loss 67.3002700805664\n",
      "Itérations 6000: loss 67.0634765625\n",
      "Itérations 6100: loss 66.82793426513672\n",
      "Itérations 6200: loss 66.59364318847656\n",
      "Itérations 6300: loss 66.36053466796875\n",
      "Itérations 6400: loss 66.1286849975586\n",
      "Itérations 6500: loss 65.89800262451172\n",
      "Itérations 6600: loss 65.66851043701172\n",
      "Itérations 6700: loss 65.4402084350586\n",
      "Itérations 6800: loss 65.21305084228516\n",
      "Itérations 6900: loss 64.98715209960938\n",
      "Itérations 7000: loss 64.76243591308594\n",
      "Itérations 7100: loss 64.53883361816406\n",
      "Itérations 7200: loss 64.31635284423828\n",
      "Itérations 7300: loss 64.09514617919922\n",
      "Itérations 7400: loss 63.875038146972656\n",
      "Itérations 7500: loss 63.656036376953125\n",
      "Itérations 7600: loss 63.43829345703125\n",
      "Itérations 7700: loss 63.22172927856445\n",
      "Itérations 7800: loss 63.0062255859375\n",
      "Itérations 7900: loss 62.791805267333984\n",
      "Itérations 8000: loss 62.57857131958008\n",
      "Itérations 8100: loss 62.3664436340332\n",
      "Itérations 8200: loss 62.15536117553711\n",
      "Itérations 8300: loss 61.94535446166992\n",
      "Itérations 8400: loss 61.73653030395508\n",
      "Itérations 8500: loss 61.52873611450195\n",
      "Itérations 8600: loss 61.32197189331055\n",
      "Itérations 8700: loss 61.116329193115234\n",
      "Itérations 8800: loss 60.911766052246094\n",
      "Itérations 8900: loss 60.70821762084961\n",
      "Itérations 9000: loss 60.50575256347656\n",
      "Itérations 9100: loss 60.30430603027344\n",
      "Itérations 9200: loss 60.10390090942383\n",
      "Itérations 9300: loss 59.904571533203125\n",
      "Itérations 9400: loss 59.706363677978516\n",
      "Itérations 9500: loss 59.509132385253906\n",
      "Itérations 9600: loss 59.31290054321289\n",
      "Itérations 9700: loss 59.117767333984375\n",
      "Itérations 9800: loss 58.92362976074219\n",
      "Itérations 9900: loss 58.7306022644043\n",
      "Itérations 10000: loss 58.53872299194336\n",
      "Itérations 10100: loss 58.34782409667969\n",
      "Itérations 10200: loss 58.15793228149414\n",
      "Itérations 10300: loss 57.969017028808594\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 29\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m## Calcul du backward (grad_w, grad_b)\u001B[39;00m\n\u001B[1;32m     28\u001B[0m grad_mse \u001B[38;5;241m=\u001B[39m MSE\u001B[38;5;241m.\u001B[39mbackward(ctx_mse,\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 29\u001B[0m grad_X, grad_W, grad_b \u001B[38;5;241m=\u001B[39m \u001B[43mLinear\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx_linear\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_mse\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;66;03m## Mise à jour des paramètres du modèle\u001B[39;00m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m#x -= EPS * grad_X\u001B[39;00m\n\u001B[1;32m     33\u001B[0m w \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m EPS \u001B[38;5;241m*\u001B[39m grad_W  \u001B[38;5;66;03m# Update weights\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[2], line 49\u001B[0m, in \u001B[0;36mLinear.backward\u001B[0;34m(ctx, grad_output)\u001B[0m\n\u001B[1;32m     47\u001B[0m X, W, b \u001B[38;5;241m=\u001B[39m ctx\u001B[38;5;241m.\u001B[39msaved_tensors\n\u001B[1;32m     48\u001B[0m grad_X \u001B[38;5;241m=\u001B[39m grad_output \u001B[38;5;241m@\u001B[39m W\u001B[38;5;241m.\u001B[39mT\n\u001B[0;32m---> 49\u001B[0m grad_W \u001B[38;5;241m=\u001B[39m \u001B[43m(\u001B[49m\u001B[43mgrad_output\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m@\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     50\u001B[0m grad_b \u001B[38;5;241m=\u001B[39m grad_output\u001B[38;5;241m.\u001B[39msum(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m grad_X, grad_W, grad_b\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "## Chargement des données Boston et transformation en tensor.\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "boston = fetch_california_housing() ## chargement des données\n",
    "x = torch.tensor(boston['data'],dtype=torch.float)\n",
    "y = torch.tensor(boston['target'],dtype=torch.float).view(-1,1)\n",
    "\n",
    "print(\"Nombre d'exemples : \",x.size(0), \"Dimension : \",x.size(1))\n",
    "print(\"Nom des attributs : \", \", \".join(boston['feature_names']))\n",
    "\n",
    "#initialisation aléatoire de w et b\n",
    "w = torch.randn(x.size(1),1)\n",
    "b =  torch.randn(1,1)\n",
    "\n",
    "EPOCHS = 50000\n",
    "EPS = 1e-7\n",
    "for n_iter in range(EPOCHS):\n",
    "    ## Calcul du forward (loss), avec création de nouveaux Context pour chaque module\n",
    "    ctx_mse = Context()\n",
    "    ctx_linear = Context()\n",
    "\n",
    "    y_pred = Linear.forward(ctx_linear, x, w, b)\n",
    "    loss = MSE.forward(ctx_mse, y_pred, y)\n",
    "\n",
    "    if n_iter % 100 == 0:\n",
    "        print(f\"Itérations {n_iter}: loss {loss.item()}\")\n",
    "\n",
    "    ## Calcul du backward (grad_w, grad_b)\n",
    "    grad_mse = MSE.backward(ctx_mse,1)\n",
    "    grad_X, grad_W, grad_b = Linear.backward(ctx_linear, grad_mse[0])\n",
    "\n",
    "    ## Mise à jour des paramètres du modèle\n",
    "    #x -= EPS * grad_X\n",
    "    w -= EPS * grad_W  # Update weights\n",
    "    b -= EPS * grad_b  # Update bias"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DeepLearning fc TP1 2020-2021-correction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
