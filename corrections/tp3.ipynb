{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TE2ItlsI956"
   },
   "source": [
    "# Deep Learning  \n",
    "\n",
    "\n",
    "## TP3 : Méthodologie, Expérimentations et Régularisation \n",
    "\n",
    "Sylvain Lamprier (sylvain.lamprier@univ-angers.fr)\n",
    "\n",
    "Supports adaptés de Nicolas Baskiotis (nicolas.baskiotis@sorbonne-univeriste.fr) et Benjamin Piwowarski (benjamin.piwowarski@sorbonne-universite.fr) -- MLIA/ISIR, Sorbonne Université"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T12:08:56.861599493Z",
     "start_time": "2024-02-08T12:08:53.171259999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La version de torch est :  2.2.0+cu121\n",
      "Le calcul GPU est disponible ?  False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"La version de torch est : \",torch.__version__)\n",
    "print(\"Le calcul GPU est disponible ? \", torch.cuda.is_available())\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "\n",
    "\n",
    "## Chargement des données Boston et transformation en tensor.\n",
    "boston = fetch_california_housing()\n",
    "boston_x = torch.tensor(boston['data'],dtype=torch.float)\n",
    "boston_y = torch.tensor(boston['target'],dtype=torch.float)\n",
    "Xdim = boston_x.size(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGIDLqItECDu"
   },
   "source": [
    "# Méthodologie expérimentale et boîte à outils\n",
    "Pytorch dispose d'un ensemble d'outils qui permettent de simplifier les démarches expérimentales. Nous allons voir en particulier : \n",
    "* le DataLoader qui permet de gérer le chargement de données, le partitionement et la constitution d'ensembles de test et d'apprentissage; \n",
    "* le checkpointing qui permet de sauvegarder/charger les modèles en cours d'entraînement.\n",
    "* le TensorBoard (qui vient de tensorflow) qui permet de suivre l'évolution en apprentissage de vos modèles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJ4MoJP4k4i0"
   },
   "source": [
    "\n",
    "## DataLoader\n",
    "Le <a href=https://pytorch.org/docs/stable/data.html>**DataLoader**</a> et la classe associée <a href=https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset> **Dataset**</a>  permettent en particulier de :\n",
    "* charger des données\n",
    "* pré-processer les données\n",
    "* de gérer les mini-batchs (sous-ensembles sur lequel on effectue une descente de gradient).\n",
    "\n",
    "La classe **Dataset** est une classe abstraite qui nécessite l'implémentation que d'une seule méthode, ```__getitem__(self,index)``` : elle renvoie le i-ème objet du jeu de données (généralement un couple *(exemple,label)*. \n",
    "\n",
    "La classe **TensorDataset** est l'instanciation la plus courante d'un **Dataset**, elle permet de créer un objet **Dataset** à partir d'une liste de tenseurs qui renvoie pour un index $i$ donné le tuple contenant les $i$-èmes ligne de chaque tenseur.\n",
    "\n",
    "La classe **DataLoader** permet essentiellement de randomiser et de constituer des mini-batchs de façon simple à partir d'une instance de **Dataset**. Chaque mini-batch est constitué d'exemples tirés aléatoirement dans le **Dataset** passé en paramètre et mis bout à bout dans des tenseurs. La méthode ```collate_fn(*args)``` est utilisée pour cela (nous verrons une customization de cette fonction dans une séance ultérieure). C'est ce générateur qui est généralement parcouru lors de l'apprentissage à chaque itération d'optimisation.\n",
    "\n",
    "Voici un exemple de code pour utiliser le DataLoader : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AZaWAFO8k8ze",
    "ExecuteTime": {
     "end_time": "2024-02-08T12:09:09.512005709Z",
     "start_time": "2024-02-08T12:08:56.864485257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20640 (tensor([   4.0368,   52.0000,    4.7617,    1.1036,  413.0000,    2.1399,\n",
      "          37.8500, -122.2500]), tensor(2.6970))\n",
      "1290 [tensor([[ 3.4083e+00,  3.6000e+01,  5.4330e+00,  9.8969e-01,  8.3200e+02,\n",
      "          2.8591e+00,  3.8530e+01, -1.2142e+02],\n",
      "        [ 2.6442e+00,  4.7000e+01,  3.6709e+00,  1.0214e+00,  9.1300e+02,\n",
      "          3.9017e+00,  3.4090e+01, -1.1823e+02],\n",
      "        [ 8.4614e+00,  1.8000e+01,  5.5586e+00,  9.6724e-01,  1.3100e+03,\n",
      "          2.2586e+00,  3.3530e+01, -1.1776e+02],\n",
      "        [ 2.0802e+00,  2.2000e+01,  5.5777e+00,  1.1289e+00,  4.2760e+03,\n",
      "          2.6363e+00,  4.1610e+01, -1.2055e+02],\n",
      "        [ 4.2708e+00,  2.4000e+01,  4.8829e+00,  1.0050e+00,  1.4600e+03,\n",
      "          2.4415e+00,  3.4310e+01, -1.1846e+02],\n",
      "        [ 3.6827e+00,  5.2000e+01,  5.7589e+00,  1.1041e+00,  8.9800e+02,\n",
      "          2.4603e+00,  3.3790e+01, -1.1785e+02],\n",
      "        [ 1.5739e+00,  1.6000e+01,  5.9146e+00,  1.2060e+00,  5.1900e+02,\n",
      "          2.6080e+00,  3.8870e+01, -1.2264e+02],\n",
      "        [ 6.4339e+00,  1.6000e+01,  6.6734e+00,  9.4276e-01,  1.0300e+03,\n",
      "          3.4680e+00,  3.7260e+01, -1.2185e+02],\n",
      "        [ 8.3818e+00,  1.3000e+01,  7.5331e+00,  9.7996e-01,  1.4960e+03,\n",
      "          2.9980e+00,  3.3830e+01, -1.1783e+02],\n",
      "        [ 3.1333e+00,  3.0000e+01,  5.9255e+00,  1.1312e+00,  9.6600e+02,\n",
      "          3.4255e+00,  3.6510e+01, -1.1965e+02],\n",
      "        [ 4.9107e+00,  3.4000e+01,  5.5854e+00,  9.8780e-01,  5.5200e+02,\n",
      "          3.3659e+00,  3.3910e+01, -1.1813e+02],\n",
      "        [ 2.2259e+00,  2.4000e+01,  3.1024e+00,  1.0432e+00,  1.8080e+03,\n",
      "          2.8928e+00,  3.7310e+01, -1.2186e+02],\n",
      "        [ 5.4604e+00,  2.4000e+01,  5.8329e+00,  9.7389e-01,  1.3250e+03,\n",
      "          3.4595e+00,  3.4260e+01, -1.1875e+02],\n",
      "        [ 3.8233e+00,  3.6000e+01,  5.1897e+00,  1.0462e+00,  1.0300e+03,\n",
      "          2.6410e+00,  3.2750e+01, -1.1705e+02],\n",
      "        [ 3.1543e+00,  2.9000e+01,  4.9339e+00,  1.0674e+00,  1.8730e+03,\n",
      "          2.4742e+00,  3.6610e+01, -1.2192e+02],\n",
      "        [ 2.2538e+00,  7.0000e+00,  3.2222e+00,  1.1590e+00,  1.0570e+03,\n",
      "          2.3028e+00,  3.4950e+01, -1.2045e+02]]), tensor([0.7180, 1.3610, 3.9190, 0.4790, 2.1820, 2.3680, 0.7350, 2.8920, 3.7760,\n",
      "        1.0000, 2.2200, 1.1250, 1.9340, 1.3980, 2.5380, 1.6250])]\n",
      "iteration : 0, loss : 8.230142328166222\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader,TensorDataset, Dataset\n",
    "\n",
    "## Création d'un dataset à partir des deux tenseurs d'exemples et de labels\n",
    "train_data = TensorDataset(boston_x,boston_y)\n",
    "## On peut indexer et connaitre la longueur d'un dataset\n",
    "print(len(train_data),train_data[5])\n",
    "\n",
    "## Création d'un DataLoader\n",
    "## tailles de mini-batch de 16, shuffle=True permet de mélanger les exemples\n",
    "# loader est un itérateur sur les mini-batchs des données\n",
    "loader = DataLoader(train_data, batch_size=16,shuffle=True ) \n",
    "\n",
    "#Premier batch (aléatoire) du dataloader :\n",
    "print(len(iter(loader)),next(iter(loader)))\n",
    "EPOCHS = 10\n",
    "EPS=1e-7\n",
    "netSeq = torch.nn.Sequential(torch.nn.Linear(Xdim,5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
    "optim = torch.optim.Adam(params=netSeq.parameters(),lr=EPS)\n",
    "mseloss = torch.nn.MSELoss()\n",
    "# La boucle d'apprentissage :\n",
    "for i in range(EPOCHS):\n",
    "    cumloss = 0\n",
    "    # On parcourt tous les exemples par batch de 16 (paramètre batch_size de DataLoader)\n",
    "    for bx,by in loader:\n",
    "        loss = mseloss(netSeq(bx).view(-1),by)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        cumloss += loss.item()\n",
    "    if i % 100==0: print(f\"iteration : {i}, loss : {cumloss/len(loader)}\")\n",
    "\n",
    "\n",
    "\n",
    "## Exemple d'un Dataset (sans utilité dans le cas présent, TensorDataset permet de faire la même chose)\n",
    "class MyDataSet(Dataset):\n",
    "  def __init__(self, x,y):\n",
    "    self.x = x\n",
    "    self.y = y\n",
    "  def __getitem__(self,i):\n",
    "    return self.x[i],self.y[i]\n",
    "  def __len__(self):\n",
    "    return len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9x2LC_6lCQm"
   },
   "source": [
    "## Checkpointing\n",
    "Les modèles Deep sont généralement long à apprendre. Afin de ne pas perdre des résultats en cours de calcul, il est fortement recommander de faire du **checkpointing**, c'est-à-dire d'enregistrer des points d'étapes du modèle en cours d'apprentissage pour pouvoir reprendre à n'importe quel moment l'apprentissage du modèle en cas de problème.  Il s'agit en pratique de sauvegarder l'état du modèle et de l'optimisateur (et de tout autre objet qui peut servir lors de l'apprentissage) toutes les n itérations. Toutes les variables d'intérêt sont en général disponibles par la méthode **state_dict()** des modèles et de l'optimiseur. \n",
    "\n",
    "En pratique, vous pouvez utilisé un code dérivé de celui ci-dessous.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "URQTq8hrPJO0",
    "ExecuteTime": {
     "end_time": "2024-02-08T12:09:22.035079025Z",
     "start_time": "2024-02-08T12:09:09.515088498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   8.190352948506673\n",
      "1   8.186375943819682\n",
      "2   8.182403252660766\n",
      "3   8.17843779852224\n",
      "4   8.174472572637159\n",
      "5   8.17050896596539\n",
      "6   8.166539699162623\n",
      "7   8.162573479681976\n",
      "8   8.158611945033998\n",
      "9   8.154651355373767\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def save_state(epoch,model,optim,fichier):\n",
    "      \"\"\" sauvegarde du modèle et de l'état de l'optimiseur dans fichier \"\"\"\n",
    "      state = {'epoch' : epoch, 'model_state': model.state_dict(), 'optim_state': optim.state_dict()}\n",
    "      torch.save(state,fichier)\n",
    " \n",
    "def load_state(fichier,model,optim):\n",
    "      \"\"\" Si le fichier existe, on charge le modèle et l'optimiseur \"\"\"\n",
    "      epoch = 0\n",
    "      if os.path.isfile(fichier):\n",
    "          state = torch.load(fichier)\n",
    "          model.load_state_dict(state['model_state'])\n",
    "          optim.load_state_dict(state['optim_state'])\n",
    "          epoch = state['epoch']\n",
    "      return epoch\n",
    "\n",
    "fichier = \"/tmp/netSeq.pth\"\n",
    "save_state(0,netSeq,optim,fichier)\n",
    "\n",
    "\n",
    "#netSeq = torch.nn.Sequential(torch.nn.Linear(Xdim,5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
    "#optim = torch.optim.Adam(params=netSeq.parameters(),lr=EPS)\n",
    "\n",
    "#save_state(0,netSeq,optim,fichier)\n",
    "start_epoch = load_state(fichier,netSeq,optim)\n",
    "for epoch in range(start_epoch,EPOCHS):\n",
    "    \n",
    "    cumloss = 0\n",
    "    for bx,by in loader:\n",
    "        \n",
    "        loss = mseloss(netSeq(bx).view(-1),by)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        cumloss += loss.item()\n",
    "    if epoch % 10 ==0: save_state(epoch,netSeq,optim,fichier)\n",
    "    print(epoch,\" \",cumloss/len(loader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IstQCvKblSvT"
   },
   "source": [
    "\n",
    "## GPU \n",
    "Afin d'utiliser un GPU lors des calculs, il est nécessaire de transférer les données et le modèle sur le GPU par l'intermédiaire de la fonction **to(device)** des tenseurs et des modules.  Il est impossible de faire une opération lorsqu'une partie des tenseurs sont sur GPU et l'autre sur CPU. Il faut que tous les tenseurs et paramètres soient sur le même device ! On doit donc s'assurer que le modèle, les exemples et les labels sont sur GPU pour faire les opérations.\n",
    "\n",
    "Par ailleurs, on peut connaître le device sur lequel est chargé un tenseur par l'intermédiaire de ```.device``` (mais pas pour un modèle, il faut aller voir les paramètres dans ce cas).\n",
    "\n",
    "Une manière simple d'utiliser un GPU quand il existe et donc d'avoir un code agnostique est la suivante : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Fs8s7EwwlWTn",
    "ExecuteTime": {
     "end_time": "2024-02-08T12:09:23.474509782Z",
     "start_time": "2024-02-08T12:09:22.032849707Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device du mini-batch :  cpu\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "loader = DataLoader(TensorDataset(boston_x,boston_y), batch_size=16,shuffle=True ) \n",
    "\n",
    "## On charge le modèle sur GPU\n",
    "## A faire avant la déclaration de l'optimiseur, sinon les paramètres optimisés ne seront pas les mêmes! \n",
    "\n",
    "netSeq = torch.nn.Sequential(torch.nn.Linear(Xdim,5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
    "netSeq = netSeq.to(device)\n",
    "optim = torch.optim.Adam(params=netSeq.parameters(),lr=EPS)\n",
    "\n",
    "for i,(bx,by) in enumerate(loader):\n",
    "    ## On charge le batch sur GPU\n",
    "    bx, by = bx.to(device), by.to(device)\n",
    "    loss = mseloss(netSeq(bx).view(-1),by)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "\n",
    "print(\"Device du mini-batch : \", bx.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5J1b55_lFR-"
   },
   "source": [
    "\n",
    "## TensorBoard\n",
    "\n",
    "Durant l'apprentissage de vos modèles, il est agréable de visualiser de quelle manière évolue le coût, la précision sur l'ensemble de validation ainsi que d'autres éléments. TensorFlow dispose d'un outil très apprécié, le TensorBoard, qui permet de gérer très facilement de tels affichages. On retrouve tensorboard dans **Pytorch** dans ```torch.utils.tensorboard``` qui permet de faire le pont de pytorch vers cet outil. \n",
    "\n",
    "Le principe est le suivant :\n",
    "* tensorboard fait tourner en fait un serveur web local qui va lire les fichiers de log dans un répertoire local. L'affichage se fait dans votre navigateur à partir d'un lien fourni lors du lancement de tensorboard.\n",
    "* Les éléments que vous souhaitez visualiser (scalaire, graphes, distributions, histogrammes) sont écrits dans le fichier de log à partir d'un objet **SummaryWriter** .\n",
    "* la méthode ```add_scalar(tag, valeur, global_step)``` permet de logger une valeur à un step donné, ```add_scalar(tag, tag_scalar_dic, global_step)``` un ensemble de valeurs par l'intermédiaire du dictionnaire ```tag_scalar_dic``` (un regroupement des scalaires est fait en fonction du tag passé, chaque sous-tag séparé par un **/**).\n",
    "\n",
    "Il existe d'autres méthodes ```add_XXX``` pour visualiser par exemple des images, des histogrammes (cf <a href=https://pytorch.org/docs/stable/tensorboard.html>la doc </a>).\n",
    "\n",
    "Le code suivant illustre une manière de l'utiliser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1kIhHDnElQd8",
    "ExecuteTime": {
     "end_time": "2024-02-08T12:09:23.814822101Z",
     "start_time": "2024-02-08T12:09:23.453870277Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorboard'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Pour observer les courbes produites, il faut lancer tensorboard \u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# à la main à partir du shell :  tensorboard --logdir logs\u001B[39;00m\n\u001B[1;32m      3\u001B[0m TB_PATH \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/tmp/logs/deep\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtensorboard\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SummaryWriter\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mDeuxCouches\u001B[39;00m(torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mModule):\n\u001B[1;32m      7\u001B[0m   \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n",
      "File \u001B[0;32m/usr/local/lib64/python3.12/site-packages/torch/utils/tensorboard/__init__.py:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorboard\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_vendor\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpackaging\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mversion\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Version\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(tensorboard, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__version__\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m Version(\n\u001B[1;32m      5\u001B[0m     tensorboard\u001B[38;5;241m.\u001B[39m__version__\n\u001B[1;32m      6\u001B[0m ) \u001B[38;5;241m<\u001B[39m Version(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1.15\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tensorboard'"
     ]
    }
   ],
   "source": [
    "# Pour observer les courbes produites, il faut lancer tensorboard \n",
    "# à la main à partir du shell :  tensorboard --logdir logs\n",
    "TB_PATH = \"/tmp/logs/deep\"\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class DeuxCouches(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(DeuxCouches,self).__init__()\n",
    "    self.un = torch.nn.Linear(Xdim,5)\n",
    "    self.act = torch.nn.Tanh()\n",
    "    self.deux = torch.nn.Linear(5,1)\n",
    "  def forward(self,x):\n",
    "    return self.deux(self.act(self.un(x)))\n",
    "\n",
    "EPS = 1e-7\n",
    "EPOCHS=10\n",
    "netSeq = torch.nn.Sequential(torch.nn.Linear(Xdim,5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
    "netDeuxCouches = DeuxCouches()\n",
    "netSeq.name = \"Sequentiel\"\n",
    "netDeuxCouches.name = \"DeuxCouches\"\n",
    "## Obtention d'un SummaryWriter \n",
    "summary = SummaryWriter(f\"{TB_PATH}/test\")\n",
    "\n",
    "mseloss = torch.nn.MSELoss()\n",
    "for model in [netSeq, netDeuxCouches]:\n",
    "    optim = torch.optim.Adam(params=model.parameters(),lr=EPS) \n",
    "    for i in range(EPOCHS):\n",
    "        cumloss = 0\n",
    "        for bx, by in loader:\n",
    "            loss = mseloss(model(boston_x),boston_y.view(-1,1))\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()  \n",
    "            cumloss+= loss.item()\n",
    "        summary.add_scalar(f\"loss/{model.name}\",cumloss,i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaW5Av4elaBN"
   },
   "source": [
    "## Dernières remarques et exemple typique de code\n",
    "* Le graphe de calcul est instancié de manière dynamique sous pytorch, et cela consomme des ressources. Lorsqu'il n'y a pas de rétropropagation qui intervient - lors de l'évaluation d'un modèle par exemple -, il faut à tout prix éviter de le calculer. L'environnement **torch.no_grad()** permet de désactiver temporairement l'instanciation du graphe. **Toutes les procédures d'évaluation doivent se faire dans cet environnement afin d'économiser du temps !**\n",
    "* Pour certains modules, le comportement est différent entre l'évaluation et l'apprentissage (pour le dropout ou la batchnormalisation par exemple, ou pour les RNNs). Afin d'indiquer à pytorch dans quelle phase on se situe, deux méthodes sont disponibles dans la classe module,  **.train()** et **.eval()** qui permettent de basculer entre les deux environnements.\n",
    "\n",
    "Les deux fonctionalités sont très différentes : **no_grad** agit au niveau du graphe de calcul et désactive sa construction (comme si les variables avaient leur propriété **requires_grad** à False), alors que **eval/train** agissent au niveau du module et influence le comportement du module.\n",
    "\n",
    "Vous trouverez ci-dessous un exemple typique de code pytorch qui reprend l'ensemble des éléments de ce tutoriel. Vous êtes prêt maintenant à expérimenter la puissance de ce framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-08T12:09:23.814453618Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import os\n",
    "TB_PATH = \"/tmp/logs/module1\"\n",
    "MODEL_PATH = \"/tmp/models\"\n",
    "os.makedirs(MODEL_PATH,exist_ok=True)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3TRg2p5ldCJ",
    "ExecuteTime": {
     "end_time": "2024-02-08T12:09:23.896058370Z",
     "start_time": "2024-02-08T12:09:23.823046817Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_state(fichier,epoch,model,optim):\n",
    "    state = {'epoch' : epoch, 'model_state': model.state_dict(), 'optim_state': optim.state_dict()}\n",
    "    torch.save(state,fichier)\n",
    "\n",
    "def load_state(fichier,model,optim):\n",
    "    epoch = 0\n",
    "    if os.path.isfile(fichier):\n",
    "        state = torch.load(fichier)\n",
    "        model.load_state_dict(state['model_state'])\n",
    "        optim.load_state_dict(state['optim_state'])\n",
    "        epoch = state['epoch']\n",
    "    return epoch\n",
    "\n",
    "\n",
    "def train(model, loss, epochs, train_loader, test_loader,lr=1e-3):\n",
    "    # On créé un writer avec la date du modèle pour s'y retrouver\n",
    "    check_file = f\"{MODEL_PATH}/{model.name}.pth\"\n",
    "    summary = SummaryWriter(f\"{TB_PATH}/{model.name}\")\n",
    "    optim = torch.optim.Adam(params=model.parameters(),lr=lr)\n",
    "    start_epoch = load_state(check_file,model,optim)\n",
    "    for epoch in range(start_epoch,epochs):\n",
    "        # Apprentissage\n",
    "        # .train() inutile tant qu'on utilise pas de normalisation ou de récurrent\n",
    "        model.train()\n",
    "        cumloss = 0\n",
    "        for xbatch, ybatch in train_loader:\n",
    "            xbatch, ybatch = xbatch.to(device), ybatch.to(device)\n",
    "            outputs = model(xbatch)\n",
    "            l = loss(outputs.view(-1),ybatch)\n",
    "            optim.zero_grad()\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            cumloss += l.item()\n",
    "        summary.add_scalar(\"loss/train\",  cumloss/len(train_loader),epoch)\n",
    "     \n",
    "        if epoch % 10 == 0: \n",
    "            save_state(check_file,epoch,model,optim)\n",
    "            # Validation\n",
    "            # .eval() inutile tant qu'on utilise pas de normalisation ou de récurrent\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                cumloss = 0\n",
    "                for xbatch, ybatch in test_loader:\n",
    "                    xbatch, ybatch = xbatch.to(device), ybatch.to(device)\n",
    "                    outputs = model(xbatch)\n",
    "                    cumloss += loss(outputs.view(-1),ybatch).item()\n",
    "            summary.add_scalar(\"loss/validation\", cumloss/len(test_loader) ,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-08T12:09:23.833021208Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Datasets\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "boston = fetch_california_housing() ## chargement des données\n",
    "all_data = torch.tensor(boston['data'],dtype=torch.float)\n",
    "all_labels = torch.tensor(boston['target'],dtype=torch.float)\n",
    "\n",
    "# Il est toujours bon de normaliser\n",
    "all_data = (all_data-all_data.mean(0))/all_data.std(0)\n",
    "all_labels = (all_labels-all_labels.mean())/all_labels.std()\n",
    "\n",
    "train_tensor_data = TensorDataset(all_data, all_labels)\n",
    "\n",
    "# Split en 80% apprentissage et 20% test\n",
    "train_size = int(0.8 * len(train_tensor_data))\n",
    "validate_size = len(train_tensor_data) - train_size\n",
    "train_data, valid_data = torch.utils.data.random_split(train_tensor_data, [train_size, validate_size])\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(train_data,batch_size=BATCH_SIZE,shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "net = torch.nn.Sequential(torch.nn.Linear(all_data.size(1),5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
    "net.name = \"mon_premier_reseau\"+time.asctime()\n",
    "\n",
    "net = net.to(device)\n",
    "train(net,torch.nn.MSELoss(),EPOCHS,train_loader,valid_loader,lr=1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expérimentations\n",
    "## Jeu de données MNIST\n",
    "Ce jeu de données est l'équivalent du *Hello world* en programmation. Chaque donnée est un chiffre manuscrit (de 0 à 9). Les lignes suivantes vous permettent de charger le jeu de données.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-08T12:09:23.842795622Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Modules (torch, nn, F et optim)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.utils as vutils\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn.functional import one_hot\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Progrès\n",
    "from tqdm import tqdm\n",
    "from tqdm.autonotebook import tqdm\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "#matpotlib \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import time\n",
    "import os\n",
    "from tensorboard import notebook\n",
    "\n",
    "\n",
    "TB_PATH = \"/tmp/logs/deep_tp3\"\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "MODEL_PATH = \"/tmp/models\"\n",
    "os.makedirs(MODEL_PATH,exist_ok=True)\n",
    "\n",
    "\n",
    "mean=[0.5]\n",
    "std=[0.5]\n",
    "batchsize=128\n",
    "\n",
    "#Transformations à appliquer sur le dataset (transformation des images en tenseurs et normalization pour obtenir des valeurs entre -1 et 1)\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(mean, std)])\n",
    "\n",
    "# Téléchargement des données (via le dataset specifique MNIST de pytorch)\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "print(len(trainset))\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batchsize, pin_memory=True, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batchsize, pin_memory=True, shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation d'une image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-08T12:09:23.850951624Z"
    }
   },
   "outputs": [],
   "source": [
    "def unnormalize(img):\n",
    "  if img.dim()==2 or ((img.dim()==3) and (img.size()[0]==1)):\n",
    "      return img*std[0]+mean[0]\n",
    "  return img * img.new(std).view(3, 1, 1) + img.new(mean).view(3, 1, 1)\n",
    "\n",
    "# Recuperation du premier batch\n",
    "imgs,labs=next(iter(trainloader))\n",
    "# dimension of images (flattened)\n",
    "HEIGHT,WIDTH = imgs.shape[2],imgs.shape[3] # taille de l'image\n",
    "\n",
    "INPUT_DIM = HEIGHT * WIDTH\n",
    "\n",
    "#Visualisation de la première image\n",
    "print(imgs.size())\n",
    "img = unnormalize(imgs[0]) # pour retrouver l'image d'origine (avant normalisation)\n",
    "fig=plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img.squeeze(),cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-08T12:09:23.856296681Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader,TensorDataset, Dataset\n",
    "\n",
    "## On utilise qu'une partie du training test pour mettre en évidence le sur-apprentissage\n",
    "TRAIN_RATIO = 0.01\n",
    "train_length = int(len(trainset)*TRAIN_RATIO)\n",
    "\n",
    "ds_train, ds_test =  torch.utils.data.random_split(trainset, (train_length, len(trainset)- train_length))\n",
    "\n",
    "#On utilise un DataLoader pour faciliter les manipulations, on fixe  la taille du mini batch à 300\n",
    "train_loader = DataLoader(ds_train,batch_size=300,shuffle=True)\n",
    "test_loader = DataLoader(ds_test,batch_size=300,shuffle=False)\n",
    "\n",
    "print(next(iter(train_loader)))\n",
    "def accuracy(yhat,y):\n",
    "    # y encode les indexes, s'assurer de la bonne taille de tenseur\n",
    "    assert len(y.shape)==1 or y.size(1)==1\n",
    "    return (torch.argmax(yhat,1).view(y.size(0),-1)== y.view(-1,1)).double().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <span class=\"alert-success\"> Exercice : Classification multi-labels, nombre de couches, fonction de coût </span>\n",
    "\n",
    "L'objectif est de classer chaque image parmi les 10 chiffres qu'ils représentent. Le réseau aura donc 10 sorties, une par classe, chacune représentant la probabilité d'appartenance à chaque classe. Pour garantir une distribution de probabilité en sortie, il faut utiliser le module <a href=https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html> **Softmax** </a> : $$\\texttt{Softmax}(\\mathbf{x})_i = \\frac{e^{x_i}}{\\sum_{j=1}^d x_j}$$ qui permet de normaliser le vecteur de sortie.\n",
    "\n",
    "* Faites quelques exemples de réseau à 1, 2, 3 couches et en faisant varier les nombre de neurones par couche. Utilisez un coût moindre carré dans un premier temps. Pour superviser ce coût, on doit construire le vecteur one-hot correspondant à la classe : un vecteur qui ne contient que des 0 sauf à l'index de la classe qui contient un 1 (utilisez ```torch.nn.functional.one_hot```).  Comparez les courbes de coût et d'erreurs en apprentissage et en test selon l'architecture.\n",
    "* Le coût privilégié en multi-classe est la *cross-entropy**. Ce coût représente la négative log-vraisemblance : $$NNL(y,\\mathbf{x}) = -x_{y} $$ en notant $y$ l'indice de la classe et $\\mathbf{x}$ le vecteur de log-probabilité inféré. On peut utiliser soit son implémentation par le module <a href=https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss>**NLLLoss**</a>, soit - plus pratique - le module <a href=https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html>**CrossEntropyLoss** <a>  qui combine un *logSoftmax* et la cross entropie, ce qui évite d'avoir à ajouter un module de *Softmax* en sortie du réseau. Utilisez ce dernier coût et observez les changements.\n",
    "* Changez la fonction d'activation en une ReLU et observez l'effet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-08T12:09:23.860752846Z"
    }
   },
   "outputs": [],
   "source": [
    "# Construire un réseau générique NN qui étend nn.Module, dont on la construction se fait par:\n",
    "# LinearMultiClass(inSize, outSize, layers=[], finalActivation=None, activation=nn.Tanh), où:\n",
    "# inSize est la dimension des entrées, outSize la dimension des sorties\n",
    "# liste des tailles des éventuelles couches cachées\n",
    "# finalActivation est la fonction d'activation a appliquer sur la sortie (si pas None)\n",
    "# et activation la fonction d'activation à appliquer après chaque couche cachée\n",
    "\n",
    "class LinearMultiClass(nn.Module):\n",
    "    def __init__(self, inSize, outSize, layers=[], finalActivation=None, activation=nn.Tanh):\n",
    "        super(LinearMultiClass, self).__init__()\n",
    "        \n",
    "        # Liste des couches linéaires\n",
    "        self.neuralNetwork = nn.ModuleList()\n",
    "        \n",
    "        # Couche d'entrée\n",
    "        self.neuralNetwork.append(nn.Linear(inSize, layers[0]) if layers else nn.Linear(inSize, layers[0]))\n",
    "        \n",
    "        # Couches cachées\n",
    "        for i in range(len(layers)-1):\n",
    "            self.neuralNetwork.append(nn.Linear(layers[i], layers[i+1]))\n",
    "        \n",
    "        # Couche de sortie\n",
    "        self.neuralNetwork.append(nn.Linear(layers[-1] if layers else inSize, outSize))\n",
    "        \n",
    "        # Fonction d'activation après chaque couche cachée\n",
    "        self.activation = activation()\n",
    "        \n",
    "        # Fonction d'activation pour la sortie (si présente)\n",
    "        self.finalActivation = finalActivation() if finalActivation else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.neuralNetwork[:-1]:\n",
    "            x = self.activation(layer(x))\n",
    "        \n",
    "        # Couche de sortie avec fonction d'activation\n",
    "        x = self.neuralNetwork[-1](x)\n",
    "        if self.finalActivation:\n",
    "            x = self.finalActivation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T12:09:24.017663756Z",
     "start_time": "2024-02-08T12:09:23.901968949Z"
    }
   },
   "outputs": [],
   "source": [
    "      \n",
    "\n",
    "\n",
    "# Comparer les deux types de coûts décrits si dessus (MSE avec un vecteur one hot cible et CrossEntropie qui paraît plus adapté pour ce cas) \n",
    "# Pour la cross entropie, on utilise nn.CrossEntropyLoss, qui n'a pas besoin que l'on ne rescale les sorties (la cross entropy combine un softmax + NLLloss)\n",
    "# Si on utilisait une BCE loss (qui n'est pas approprié ici, vu que les classes sont dépendantes : une seule classe est visée par exemple),\n",
    "# dans ce cas il faudrait ajouter une sigmoide en derniere couche.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-08T12:09:23.902522997Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "# Fonction pour sauvegarder l'état du modèle\n",
    "def save_state(fichier, epoch, model, optim):\n",
    "    state = {'epoch': epoch, 'model_state': model.state_dict(), 'optim_state': optim.state_dict()}\n",
    "    torch.save(state, fichier)\n",
    "\n",
    "# Fonction pour charger l'état du modèle\n",
    "def load_state(fichier, model, optim):\n",
    "    epoch = 0\n",
    "    if os.path.isfile(fichier):\n",
    "        state = torch.load(fichier)\n",
    "        model.load_state_dict(state['model_state'])\n",
    "        optim.load_state_dict(state['optim_state'])\n",
    "        epoch = state['epoch']\n",
    "    return epoch\n",
    "\n",
    "def train(model, loss, epochs, train_loader, test_loader,lr=1e-3):\n",
    "    check_file = f\"{MODEL_PATH}/{model.name}.pth\"\n",
    "    summary = SummaryWriter(f\"{TB_PATH}/{model.name}\")\n",
    "    optim = torch.optim.Adam(params=model.parameters(),lr=lr)\n",
    "    #start_epoch = load_state(check_file,model,optim)\n",
    "    \n",
    "    for epoch in range(0, epochs):\n",
    "        ###############    TRAIN    ########################\n",
    "        model.train()\n",
    "        cumloss = 0\n",
    "        acc_train = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            optim.zero_grad()\n",
    "            outputs = model(inputs.view(-1, 28 * 28))            \n",
    "            l=None\n",
    "            if isinstance(loss, torch.nn.MSELoss):\n",
    "                one_hot_labels = F.one_hot(labels, num_classes=10).float()\n",
    "                l = loss(outputs, one_hot_labels)\n",
    "            else:\n",
    "                l = loss(outputs, labels)\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            cumloss += l.item()\n",
    "            acc_train += accuracy(outputs, labels)\n",
    "\n",
    "        acc_train /= len(train_loader) \n",
    "        avg_train_loss = cumloss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, \",f\"Train accuracy: {acc_train:.4f}\")\n",
    "        summary.add_scalar(f'{model.name}/Training Loss', avg_train_loss, epoch)\n",
    "        summary.add_scalar(f'{model.name}/Training Accuracy', acc_train, epoch)\n",
    "        save_state(check_file, epoch, model, optim)\n",
    "\n",
    "        ###############    TEST    ########################\n",
    "        \n",
    "        model.eval()\n",
    "        cumloss = 0\n",
    "        acc_test = 0\n",
    "        model.eval() \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs.view(-1, 28 * 28))\n",
    "                l=None\n",
    "                if isinstance(loss, torch.nn.MSELoss):\n",
    "                    one_hot_labels = F.one_hot(labels, num_classes=10).float()\n",
    "                    l = loss(outputs, one_hot_labels)\n",
    "                else:\n",
    "                    l = loss(outputs, labels)\n",
    "                cumloss += l.item()\n",
    "                acc_test += accuracy(outputs, labels)\n",
    "\n",
    "        acc_test /= len(test_loader) \n",
    "        avg_test_loss = cumloss / len(test_loader)\n",
    "        summary.add_scalar(f'{model.name}/Test Loss', avg_test_loss, epoch)\n",
    "        summary.add_scalar(f'{model.name}/Test Accuracy', acc_test, epoch)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, \",f\"Test accuracy: {acc_test:.4f}\")\n",
    "                \n",
    "    \n",
    "\n",
    "architectures = [\n",
    "    {\"name\": \"model1\", \"layers\": [128], \"loss\": nn.CrossEntropyLoss(),\"finalActivation\": None, \"activation\": nn.ReLU},\n",
    "    {\"name\": \"model2\", \"layers\": [128], \"loss\": nn.CrossEntropyLoss(), \"finalActivation\": None, \"activation\": nn.Tanh},\n",
    "    {\"name\": \"model3\", \"layers\": [128], \"loss\": nn.MSELoss(), \"finalActivation\": nn.Softmax, \"activation\": nn.ReLU},\n",
    "    {\"name\": \"model4\", \"layers\": [128], \"loss\": nn.MSELoss(), \"finalActivation\": nn.Softmax, \"activation\": nn.Tanh},\n",
    "    {\"name\": \"model5\", \"layers\": [64, 32], \"loss\": nn.MSELoss(),\"finalActivation\": nn.Softmax, \"activation\": nn.ReLU},\n",
    "    {\"name\": \"model6\", \"layers\": [64, 32], \"loss\": nn.CrossEntropyLoss(),\"finalActivation\": None, \"activation\": nn.ReLU},\n",
    "    {\"name\": \"model7\", \"layers\": [128, 64, 32], \"loss\": nn.CrossEntropyLoss(),\"finalActivation\": None, \"activation\": nn.ReLU},\n",
    "]\n",
    "\n",
    "in_size=INPUT_DIM\n",
    "out_size = 10\n",
    "EPOCHS = 10\n",
    "TRAIN_RATIO = 0.8\n",
    "train_length = int(len(trainset)*TRAIN_RATIO)\n",
    "ds_train, ds_test =  torch.utils.data.random_split(trainset, (train_length, len(trainset)- train_length))\n",
    "train_loader = DataLoader(ds_train,batch_size=300,shuffle=True)\n",
    "test_loader = DataLoader(ds_test,batch_size=300,shuffle=False)\n",
    "\n",
    "for arch in architectures:\n",
    "    print(f\"\\nTraining model {arch['name']}\")\n",
    "    model = LinearMultiClass(in_size, out_size, layers=arch.get(\"layers\"), finalActivation=arch.get(\"finalActivation\"), activation=arch.get(\"activation\"))\n",
    "    model.name = arch.get(\"name\")\n",
    "    model.to(device)\n",
    "    print(model)\n",
    "    loss = arch.get(\"loss\")\n",
    "    train(model,loss, EPOCHS,train_loader, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <span class=\"alert-success\"> Exercice : Régularisation des réseaux </span>\n",
    "\n",
    "### Pénalisation des couches\n",
    "Une première technique pour éviter le sur-apprentissage est de régulariser chaque couche par une pénalisation sur les poids, i.e. de favoriser des poids faibles. On parle de pénalisation L1 lorsque la pénalité est de la forme $\\|W\\|_1$ et L2 lorsque la norme L2 est utilisée : $\\|W\\|_2^2$. En pratique, cela consiste à rajouter à la fonction de coût globale du réseau un terme en $\\lambda Pen(W)$ pour les paramètres de chaque couche que l'on veut régulariser.\n",
    "\n",
    "Expérimentez avec une norme L2 dans $\\{0,10^{-5},10^{-4},10^{-3},10^{-2},\\}$, l'évolution de la pénalisation et du coût en fonction du nombre d'époques. Vous pouvez aussi observer les histogrammes de la distribution des poids des différentes couches en utilisant la fonction addWeightsHisto ci dessous.  Utilisez pour ces experiences un réseau à 3 couches chacune de taille 100 et un coût de CrossEntropy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-08T12:09:23.902975218Z"
    }
   },
   "outputs": [],
   "source": [
    "# requiert que les modules soient enregistrés dans une liste model.layers\n",
    "def addWeightsHisto(writer,model,epoch):                \n",
    "    ix = 0\n",
    "    for module in model.layers:\n",
    "        if isinstance(module, nn.Linear):\n",
    "           writer.add_histogram(f'linear/{ix}/weight',module.weight, epoch)\n",
    "           ix += 1\n",
    "\n",
    "class LinearMultiClass(nn.Module):\n",
    "    def __init__(self, inSize, outSize, layers=[], finalActivation=None, activation=nn.Tanh):\n",
    "        super(LinearMultiClass, self).__init__()\n",
    "        \n",
    "        # Liste des couches linéaires\n",
    "        self.neuralNetwork = nn.ModuleList()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Couche d'entrée\n",
    "        input_layer = nn.Linear(inSize, layers[0]) if layers else nn.Linear(inSize, layers[0])\n",
    "        self.neuralNetwork.append(input_layer)\n",
    "        self.layers.append(input_layer)\n",
    "\n",
    "        # Couches cachées\n",
    "        for i in range(len(layers)-1):\n",
    "            hidden_layer = nn.Linear(layers[i], layers[i+1])\n",
    "            self.neuralNetwork.append(hidden_layer)\n",
    "            self.layers.append(hidden_layer)\n",
    "\n",
    "        # Couche de sortie\n",
    "        output_layer = nn.Linear(layers[-1] if layers else inSize, outSize)\n",
    "        self.neuralNetwork.append(output_layer)\n",
    "        self.layers.append(output_layer)\n",
    "\n",
    "        # Fonction d'activation après chaque couche cachée\n",
    "        self.activation = activation()\n",
    "        \n",
    "        # Fonction d'activation pour la sortie (si présente)\n",
    "        self.finalActivation = finalActivation() if finalActivation else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.neuralNetwork[:-1]:\n",
    "            x = self.activation(layer(x))\n",
    "        \n",
    "        # Couche de sortie avec fonction d'activation\n",
    "        x = self.neuralNetwork[-1](x)\n",
    "        if self.finalActivation:\n",
    "            x = self.finalActivation(x)\n",
    "        return x\n",
    "\n",
    "def train(model, loss, epochs, train_loader, test_loader,lr=1e-3,L2_norm=0):\n",
    "    check_file = f\"{MODEL_PATH}/{model.name}.pth\"\n",
    "    summary = SummaryWriter(f\"{TB_PATH}/{model.name}\")\n",
    "    optim = torch.optim.Adam(params=model.parameters(),lr=lr,weight_decay=L2_norm)\n",
    "    #start_epoch = load_state(check_file,model,optim)\n",
    "    \n",
    "    for epoch in range(0, epochs):\n",
    "        ###############    TRAIN    ########################\n",
    "        model.train()\n",
    "        cumloss = 0\n",
    "        acc_train = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            optim.zero_grad()\n",
    "            outputs = model(inputs.view(-1, 28 * 28))            \n",
    "            l=None\n",
    "            if isinstance(loss, torch.nn.MSELoss):\n",
    "                one_hot_labels = F.one_hot(labels, num_classes=10).float()\n",
    "                l = loss(outputs, one_hot_labels)\n",
    "            else:\n",
    "                l = loss(outputs, labels)\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            cumloss += l.item()\n",
    "            acc_train += accuracy(outputs, labels)\n",
    "\n",
    "        acc_train /= len(train_loader) \n",
    "        avg_train_loss = cumloss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, \",f\"Train accuracy: {acc_train:.4f}\")\n",
    "        summary.add_scalar(f'{model.name}/Training Loss', avg_train_loss, epoch)\n",
    "        summary.add_scalar(f'{model.name}/Training Accuracy', acc_train, epoch)\n",
    "        save_state(check_file, epoch, model, optim)\n",
    "        addWeightsHisto(summary, model, epoch)\n",
    "        ###############    TEST    ########################\n",
    "        \n",
    "        model.eval()\n",
    "        cumloss = 0\n",
    "        acc_test = 0\n",
    "        model.eval() \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs.view(-1, 28 * 28))\n",
    "                l=None\n",
    "                if isinstance(loss, torch.nn.MSELoss):\n",
    "                    one_hot_labels = F.one_hot(labels, num_classes=10).float()\n",
    "                    l = loss(outputs, one_hot_labels)\n",
    "                else:\n",
    "                    l = loss(outputs, labels)\n",
    "                cumloss += l.item()\n",
    "                acc_test += accuracy(outputs, labels)\n",
    "\n",
    "        acc_test /= len(test_loader) \n",
    "        avg_test_loss = cumloss / len(test_loader)\n",
    "        summary.add_scalar(f'{model.name}/Test Loss', avg_test_loss, epoch)\n",
    "        summary.add_scalar(f'{model.name}/Test Accuracy', acc_test, epoch)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, \",f\"Test accuracy: {acc_test:.4f}\")\n",
    "                \n",
    "\n",
    "in_size=INPUT_DIM\n",
    "out_size = 10\n",
    "EPOCHS = 10\n",
    "TRAIN_RATIO = 0.8\n",
    "train_length = int(len(trainset)*TRAIN_RATIO)\n",
    "ds_train, ds_test =  torch.utils.data.random_split(trainset, (train_length, len(trainset)- train_length))\n",
    "train_loader = DataLoader(ds_train,batch_size=300,shuffle=True)\n",
    "test_loader = DataLoader(ds_test,batch_size=300,shuffle=False)\n",
    "\n",
    "for L2_norm_value in [0, 1e-5, 1e-4, 1e-3, 1e-2]:    \n",
    "    model = LinearMultiClass(in_size, out_size, layers=[100, 100, 100], finalActivation=None, activation=nn.Tanh)\n",
    "    model.name = f\"Model_L2_{L2_norm_value}\"\n",
    "    print(model)\n",
    "    train(model, nn.CrossEntropyLoss(), EPOCHS, train_loader, test_loader, lr=1e-5, L2_norm=L2_norm_value)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "Une autre technique très utilisée est le <a href=https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html> **Dropout** </a>. L’idée du Dropout est proche du moyennage de modèle : en entraînant k modèles de manière indépendante, on réduit la variance du modèle. Entraîner k modèles présente un surcoût non négligeable, et l’intérêt du Dropout est de réduire la complexité mémoire/temps de calcul. Le Dropout consiste à chaque itération à *geler* certains neurones aléatoirement dans le réseau en fixant leur sortie à zéro. Cela a pour conséquence de rendre plus robuste le réseau.\n",
    "\n",
    "Le comportement du réseau est donc différent en apprentissage et en inférence. Il est obligatoire d'utiliser ```model.train()``` et ```model.eval()``` pour différencier les comportements.\n",
    "Testez sur quelques réseaux pour voir l'effet du dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-08T12:09:23.903348023Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class LinearMultiClass(nn.Module):\n",
    "    def __init__(self, inSize, outSize, layers=[], finalActivation=None, activation=nn.Tanh, dropout_prob=0):\n",
    "        super(LinearMultiClass, self).__init__()\n",
    "\n",
    "        # Liste des couches linéaires\n",
    "        self.neuralNetwork = nn.ModuleList()\n",
    "\n",
    "        # Liste supplémentaire pour enregistrer les modules\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Couche d'entrée\n",
    "        input_layer = nn.Linear(inSize, layers[0]) if layers else nn.Linear(inSize, layers[0])\n",
    "        self.neuralNetwork.append(input_layer)\n",
    "        self.layers.append(input_layer)\n",
    "\n",
    "        # Couches cachées avec Dropout\n",
    "        for i in range(len(layers)-1):\n",
    "            hidden_layer = nn.Linear(layers[i], layers[i+1])\n",
    "            dropout = nn.Dropout(p=dropout_prob)\n",
    "            self.neuralNetwork.extend([hidden_layer, dropout])\n",
    "            self.layers.extend([hidden_layer, dropout])\n",
    "\n",
    "        # Couche de sortie\n",
    "        output_layer = nn.Linear(layers[-1] if layers else inSize, outSize)\n",
    "        self.neuralNetwork.append(output_layer)\n",
    "        self.layers.append(output_layer)\n",
    "\n",
    "        # Fonction d'activation après chaque couche cachée\n",
    "        self.activation = activation()\n",
    "\n",
    "        # Fonction d'activation pour la sortie (si présente)\n",
    "        self.finalActivation = finalActivation() if finalActivation else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.neuralNetwork[:-1]:\n",
    "            if isinstance(layer, nn.Dropout):\n",
    "                x = F.dropout(x, training=self.training)\n",
    "            else:\n",
    "                x = self.activation(layer(x))\n",
    "\n",
    "        # Couche de sortie avec fonction d'activation\n",
    "        x = self.neuralNetwork[-1](x)\n",
    "        if self.finalActivation:\n",
    "            x = self.finalActivation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "architectures = [\n",
    "    {\"name\": \"model1\", \"layers\": [128], \"loss\": nn.CrossEntropyLoss(),\"finalActivation\": None, \"activation\": nn.ReLU, \"dropoutProb\": 0.5},\n",
    "    {\"name\": \"model2\", \"layers\": [128], \"loss\": nn.CrossEntropyLoss(),\"finalActivation\": None, \"activation\": nn.ReLU, \"dropoutProb\": 0.2},\n",
    "    {\"name\": \"model3\", \"layers\": [128], \"loss\": nn.CrossEntropyLoss(),\"finalActivation\": None, \"activation\": nn.ReLU, \"dropoutProb\": 0.1}\n",
    "]\n",
    "\n",
    "in_size=INPUT_DIM\n",
    "out_size = 10\n",
    "EPOCHS = 10\n",
    "TRAIN_RATIO = 0.8\n",
    "train_length = int(len(trainset)*TRAIN_RATIO)\n",
    "ds_train, ds_test =  torch.utils.data.random_split(trainset, (train_length, len(trainset)- train_length))\n",
    "train_loader = DataLoader(ds_train,batch_size=300,shuffle=True)\n",
    "test_loader = DataLoader(ds_test,batch_size=300,shuffle=False)\n",
    "\n",
    "for arch in architectures:\n",
    "    print(f\"\\nTraining model {arch['name']}\")\n",
    "    model = LinearMultiClass(in_size, out_size, layers=arch.get(\"layers\"), finalActivation=arch.get(\"finalActivation\"), activation=arch.get(\"activation\"), dropout_prob=arch.get(\"dropoutProb\"))\n",
    "    model.name = arch.get(\"name\")\n",
    "    model.to(device)\n",
    "    print(model)\n",
    "    loss = arch.get(\"loss\")\n",
    "    train(model,loss, EPOCHS,train_loader, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BatchNorm\n",
    "\n",
    "On sait que les données centrées réduites permettent un apprentissage plus rapide et stable d’un modèle ; bien qu’on puisse faire en sorte que les données en entrées soient centrées réduites, cela est plus délicat pour les couches internes d’un réseau de neurones. La technique de <a href=https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html> **BatchNorm**</a> consiste à ajouter une couche qui a pour but de centrer/réduire les données en utilisant une moyenne/variance glissante (en inférence) et les statistiques du batch (en\n",
    "apprentissage).\n",
    "\n",
    "Tout comme pour le dropout, il est nécessaire d'utiliser ```model.train()``` et ```model.eval()```. \n",
    "Expérimentez la batchnorm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-08T12:09:23.903694728Z"
    }
   },
   "outputs": [],
   "source": [
    "class LinearMultiClass(nn.Module):\n",
    "    def __init__(self, inSize, outSize, layers=[], finalActivation=None, activation=nn.Tanh, dropout_prob=0, use_batchnorm=False):\n",
    "        super(LinearMultiClass, self).__init__()\n",
    "\n",
    "        # Liste des couches linéaires\n",
    "        self.neuralNetwork = nn.ModuleList()\n",
    "\n",
    "        # Liste supplémentaire pour enregistrer les modules\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Couche d'entrée\n",
    "        input_layer = nn.Linear(inSize, layers[0]) if layers else nn.Linear(inSize, layers[0])\n",
    "        self.neuralNetwork.append(input_layer)\n",
    "        self.layers.append(input_layer)\n",
    "\n",
    "        # Ajout de Batch Normalization après chaque couche cachée (si activé)\n",
    "        if use_batchnorm:\n",
    "            for i in range(len(layers)-1):\n",
    "                batchnorm_layer = nn.BatchNorm1d(layers[i])\n",
    "                hidden_layer = nn.Linear(layers[i], layers[i+1])\n",
    "                dropout = nn.Dropout(p=dropout_prob)\n",
    "                self.neuralNetwork.extend([hidden_layer, dropout,batchnorm_layer])\n",
    "                self.layers.extend([hidden_layer,dropout, batchnorm_layer])\n",
    "        else:\n",
    "            # Couches cachées sans Batch Normalization\n",
    "            for i in range(len(layers)-1):\n",
    "                hidden_layer = nn.Linear(layers[i], layers[i+1])\n",
    "                dropout = nn.Dropout(p=dropout_prob)\n",
    "                self.neuralNetwork.extend([hidden_layer, dropout])\n",
    "                self.layers.extend([hidden_layer, dropout])\n",
    "\n",
    "        # Couche de sortie\n",
    "        output_layer = nn.Linear(layers[-1] if layers else inSize, outSize)\n",
    "        self.neuralNetwork.append(output_layer)\n",
    "        self.layers.append(output_layer)\n",
    "\n",
    "        # Fonction d'activation après chaque couche cachée\n",
    "        self.activation = activation()\n",
    "\n",
    "        # Fonction d'activation pour la sortie (si présente)\n",
    "        self.finalActivation = finalActivation() if finalActivation else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.neuralNetwork[:-1]:\n",
    "            if isinstance(layer, nn.BatchNorm1d):\n",
    "                x = layer(x)\n",
    "            else:\n",
    "                x = self.activation(layer(x))\n",
    "\n",
    "        # Couche de sortie avec fonction d'activation\n",
    "        x = self.neuralNetwork[-1](x)\n",
    "        if self.finalActivation:\n",
    "            x = self.finalActivation(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "architectures = [\n",
    "    {\"name\": \"model1\", \"layers\": [128], \"loss\": nn.CrossEntropyLoss(),\"finalActivation\": None, \"activation\": nn.ReLU, \"dropoutProb\": 0, \"useBatchNorm\": True},\n",
    "    {\"name\": \"model2\", \"layers\": [128], \"loss\": nn.CrossEntropyLoss(),\"finalActivation\": None, \"activation\": nn.ReLU, \"dropoutProb\": 0, \"useBatchNorm\": False },\n",
    "]\n",
    "\n",
    "in_size=INPUT_DIM\n",
    "out_size = 10\n",
    "EPOCHS = 10\n",
    "TRAIN_RATIO = 0.8\n",
    "train_length = int(len(trainset)*TRAIN_RATIO)\n",
    "ds_train, ds_test =  torch.utils.data.random_split(trainset, (train_length, len(trainset)- train_length))\n",
    "train_loader = DataLoader(ds_train,batch_size=300,shuffle=True)\n",
    "test_loader = DataLoader(ds_test,batch_size=300,shuffle=False)\n",
    "\n",
    "for arch in architectures:\n",
    "    print(f\"\\nTraining model {arch['name']}\")\n",
    "    model = LinearMultiClass(in_size, out_size, layers=arch.get(\"layers\"), finalActivation=arch.get(\"finalActivation\"), activation=arch.get(\"activation\"), dropout_prob=arch.get(\"dropoutProb\"), use_batchnorm=arch.get(\"useBatchNorm\"))\n",
    "    model.name = arch.get(\"name\")\n",
    "    model.to(device)\n",
    "    print(model)\n",
    "    loss = arch.get(\"loss\")\n",
    "    train(model,loss, EPOCHS,train_loader, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DeepLearning fc TP1 2020-2021-correction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
