{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TE2ItlsI956"
   },
   "source": [
    "# Deep Learning - Introduction à Pytorch \n",
    "\n",
    "\n",
    "## TP2 : Fonctions Dérivables\n",
    "\n",
    "Sylvain Lamprier (sylvain.lamprier@univ-angers.fr)\n",
    "\n",
    "Supports adaptés de Nicolas Baskiotis (nicolas.baskiotis@sorbonne-univeriste.fr) et Benjamin Piwowarski (benjamin.piwowarski@sorbonne-universite.fr) -- MLIA/ISIR, Sorbonne Université"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3Y9YOOHHhJKY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La version de torch est :  1.12.1\n",
      "Le calcul GPU est disponible ?  False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"La version de torch est : \",torch.__version__)\n",
    "print(\"Le calcul GPU est disponible ? \", torch.cuda.is_available())\n",
    "\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au TP précédent, nous avons vu comment implémenter une regression linéaire en utilisant les structures Tensor de PyTorch. Cependant, nous exploitions pas du tout la puissance de PyTorch qui permet de faciliter le calcul des gradients via de l'auto-dérivation. Dans le TP précédent nous avions défini un algorithme spécifique à de la regression pour un modèle (linéaire) et un coût (moindres carrés) figés, en définissant à la main le gradient du coût global pour l'ensemble des paramètres. Ce mode de programmation est très peu modulaire et est très difficilement étendable à des architectures plus complexes. Sachant que l'objectif est de développer des architectures neuronales avec des nombreux modules neuronaux enchaînés, il n'est pas possible de travailler de cette façon. \n",
    "\n",
    "Dans ce TP, nous allons voir comment décomposer les choses pour rendre le code plus facilement généralisable. L'objectif est de comprendre le fonctionnement interne de PyTorch (sans en utiliser encore les facilités offertes par l'utilisation d'un graphe de calcul), basé sur l'implémentation d'objets Function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions\n",
    "\n",
    "\n",
    "$\\href{https://pytorch.org/docs/stable/}{\\texttt{PyTorch}}$ utilise une classe abstraite $\\href{https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function}{\\texttt{Function}}$ dont sont héritées toutes les fonctions et qui nécessite l'implémentation de ces deux méthodes :\n",
    "\n",
    "- méthode $\\texttt{forward(ctx, *inputs)}$ : calcule le résultat de l'application de la fonction\n",
    "- méthode $\\texttt{backward(ctx, *grad-outputs)}$ : calcule le gradient partiel par rapport à chaque entrée de la méthode $\\texttt{forward}$; le nombre de $\\texttt{grad-outputs}$ doit être égale aux nombre de sorties de $\\texttt{forward}$ (pourquoi ?) et le nombre de  \n",
    "sorties doit être égale aux nombres de $\\texttt{inputs}$ de $\\texttt{forward}$.\n",
    "\n",
    "\n",
    "Pour des raisons d'implémentation, les deux méthodes doivent être statiques. Le premier paramètre $\\texttt{ctx}$ permet de sauvegarder un contexte lors de la passe $\\texttt{forward}$ (par exemple les tenseurs d'entrées) et il est passé lors de la passe $\\texttt{backward}$ en paramètre afin de récupérer les valeurs. $\\textbf{Attention : }$ le contexte doit être unique pour chaque appel de $\\texttt{forward}$.\n",
    "\n",
    "Compléter le code ci-dessous pour créer des modules MSE (coût moindres carrés) et Linéaire. Les deux cellules en dessous vous serviront à tester votre code: si tout se passe sans plantage, alors vos gradients semblent corrects. Utiliser bien les outils propres à pyTorch, en particulier des Tensor et pas des matrices numpy. Assurez vous que\n",
    "vos fonctions prennent en entrée des batchs d’exemples (matrice 2D) et non un seul exemple (vecteur). N’hésiter pas à prendre un exemple et déterminer les dimensions des différentes matrices en jeu.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import gradcheck\n",
    "\n",
    "\n",
    "class Context:\n",
    "    \"\"\"Un objet contexte très simplifié pour simuler PyTorch\n",
    "    \n",
    "    Un contexte différent doit être utilisé à chaque forward\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._saved_tensors = ()\n",
    "    def save_for_backward(self, *args):\n",
    "        self._saved_tensors = args\n",
    "    @property\n",
    "    def saved_tensors(self):\n",
    "        return self._saved_tensors\n",
    "\n",
    "\n",
    "class MSE(Function):\n",
    "    \"\"\"Début d'implementation de la fonction MSE\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, yhat, y):\n",
    "        ## Garde les valeurs nécessaires pour le backwards\n",
    "        ctx.save_for_backward(yhat, y)\n",
    "\n",
    "        # [[STUDENT]] Renvoyer la valeur de la fonction\n",
    "        \n",
    "        loss = torch.mean((yhat - y)**2)\n",
    "        return loss\n",
    "    \n",
    "        # [[/STUDENT]]\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        ## Calcul du gradient du module par rapport à chaque groupe d'entrées\n",
    "        yhat, y = ctx.saved_tensors\n",
    "\n",
    "        # Calcul des dérivées partielles\n",
    "        grad_yhat = 2 * (yhat - y) / yhat.numel()\n",
    "        grad_y = -2 * (yhat - y) / yhat.numel()\n",
    "\n",
    "        return grad_yhat * grad_output, grad_y * grad_output\n",
    "        \n",
    "        # [[/STUDENT]]\n",
    "\n",
    "# [[STUDENT]] Implémenter la fonction Linear(X, W, b)sur le même modèle que MSE\n",
    "\n",
    "class Linear(Function):\n",
    "    \"\"\"Implémentation de la fonction Linear\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, W, b):\n",
    "        ctx.save_for_backward(X, W, b)\n",
    "        output = torch.matmul(X, W) + b\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        X, W, b = ctx.saved_tensors\n",
    "        \n",
    "        grad_X = torch.matmul(grad_output, W.t())\n",
    "        grad_W = torch.matmul(X.t(), grad_output)\n",
    "        grad_b = torch.sum(grad_output, dim=0)\n",
    "        return grad_X, grad_W, grad_b\n",
    "\n",
    "# [[/STUDENT]]\n",
    "\n",
    "\n",
    "linear = Linear.apply\n",
    "mse = MSE.apply\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test du gradient de MSE \n",
    "yhat = torch.randn(10,5, requires_grad=True, dtype=torch.float64)\n",
    "y = torch.randn(10,5, requires_grad=True, dtype=torch.float64)\n",
    "torch.autograd.gradcheck(mse, (yhat, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test du gradient de Linear (sur le même modèle que MSE)\n",
    "\n",
    "x = torch.randn(13, 5,requires_grad=True,dtype=torch.float64)\n",
    "w = torch.randn(5, 7,requires_grad=True,dtype=torch.float64)\n",
    "b = torch.randn(7,requires_grad=True,dtype=torch.float64)\n",
    "torch.autograd.gradcheck(linear,(x,w,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descente de Gradient\n",
    "\n",
    "Compléter ci-dessous le code pour réaliser la même regression linéaire qu'au TP précédent, mais en utilisant les objets Function déclarés ci-dessus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'exemples :  20640 Dimension :  8\n",
      "Nom des attributs :  MedInc, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Linear.forward() takes 4 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m mse_context \u001b[38;5;241m=\u001b[39m Context()\n\u001b[0;32m     20\u001b[0m linear_context \u001b[38;5;241m=\u001b[39m Context()\n\u001b[1;32m---> 22\u001b[0m yhat \u001b[38;5;241m=\u001b[39m \u001b[43mLinear\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlinear_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m MSE\u001b[38;5;241m.\u001b[39mapply(mse_context, yhat, y)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# [[/STUDENT]]\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# `loss` doit correspondre au coût MSE calculé à cette itération\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: Linear.forward() takes 4 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "## Chargement des données Boston et transformation en tensor.\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "x = torch.tensor(housing['data'],dtype=torch.float)\n",
    "y = torch.tensor(housing['target'],dtype=torch.float).view(-1,1)\n",
    "\n",
    "print(\"Nombre d'exemples : \",x.size(0), \"Dimension : \",x.size(1))\n",
    "print(\"Nom des attributs : \", \", \".join(housing['feature_names']))\n",
    "\n",
    "#initialisation aléatoire de w et b\n",
    "w = torch.randn(x.size(1),1)\n",
    "b =  torch.randn(1,1)\n",
    "\n",
    "EPOCHS = 5000\n",
    "EPS = 1e-6\n",
    "for n_iter in range(EPOCHS):\n",
    "    ## [[STUDENT]] Calcul du forward (loss), avec creation de nouveaux Context pour chaque module\n",
    "    \n",
    "    mse_context = Context()\n",
    "    linear_context = Context()\n",
    "\n",
    "    yhat = Linear.apply(linear_context, x, w, b)\n",
    "    loss = MSE.apply(mse_context, yhat, y)\n",
    "    \n",
    "    # [[/STUDENT]]\n",
    "\n",
    "    # `loss` doit correspondre au coût MSE calculé à cette itération\n",
    "    if n_iter % 100==0: \n",
    "        print(f\"Itérations {n_iter}: loss {loss}\")\n",
    "\n",
    "    ## [[STUDENT]] Calcul du backward (grad_w, grad_b)\n",
    "    grad_w, grad_b = torch.autograd.grad(loss, (w, b), retain_graph=True)\n",
    "    # [[/STUDENT]]\n",
    "\n",
    "    ## [[STUDENT]] Mise à jour des paramètres du modèle\n",
    "    \n",
    "    # [[/STUDENT]]\n",
    "    with torch.no_grad():\n",
    "        w -= EPS * grad_w\n",
    "        b -= EPS * grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DeepLearning fc TP1 2020-2021-correction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
