{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TE2ItlsI956"
   },
   "source": [
    "# Deep Learning - Introduction à Pytorch \n",
    "\n",
    "\n",
    "## TP2 : Fonctions Dérivables\n",
    "\n",
    "Sylvain Lamprier (sylvain.lamprier@univ-angers.fr)\n",
    "\n",
    "Supports adaptés de Nicolas Baskiotis (nicolas.baskiotis@sorbonne-univeriste.fr) et Benjamin Piwowarski (benjamin.piwowarski@sorbonne-universite.fr) -- MLIA/ISIR, Sorbonne Université"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La version de torch est :  2.2.0+cu121\n",
      "Le calcul GPU est disponible ?  False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"La version de torch est : \",torch.__version__)\n",
    "print(\"Le calcul GPU est disponible ? \", torch.cuda.is_available())\n",
    "\n",
    "import numpy as np\n",
    "import sklearn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T12:03:32.109311043Z",
     "start_time": "2024-02-08T12:03:31.895843809Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au TP précédent, nous avons vu comment implémenter une regression linéaire en utilisant les structures Tensor de PyTorch. Cependant, nous exploitions pas du tout la puissance de PyTorch qui permet de faciliter le calcul des gradients via de l'auto-dérivation. Dans le TP précédent nous avions défini un algorithme spécifique à de la regression pour un modèle (linéaire) et un coût (moindres carrés) figés, en définissant à la main le gradient du coût global pour l'ensemble des paramètres. Ce mode de programmation est très peu modulaire et est très difficilement étendable à des architectures plus complexes. Sachant que l'objectif est de développer des architectures neuronales avec des nombreux modules neuronaux enchaînés, il n'est pas possible de travailler de cette façon. \n",
    "\n",
    "Dans ce TP, nous allons voir comment décomposer les choses pour rendre le code plus facilement généralisable. L'objectif est de comprendre le fonctionnement interne de PyTorch (sans en utiliser encore les facilités offertes par l'utilisation d'un graphe de calcul), basé sur l'implémentation d'objets Function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions\n",
    "\n",
    "\n",
    "$\\href{https://pytorch.org/docs/stable/}{\\texttt{PyTorch}}$ utilise une classe abstraite $\\href{https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function}{\\texttt{Function}}$ dont sont héritées toutes les fonctions et qui nécessite l'implémentation de ces deux méthodes :\n",
    "\n",
    "- méthode $\\texttt{forward(ctx, *inputs)}$ : calcule le résultat de l'application de la fonction\n",
    "- méthode $\\texttt{backward(ctx, *grad-outputs)}$ : calcule le gradient partiel par rapport à chaque entrée de la méthode $\\texttt{forward}$; le nombre de $\\texttt{grad-outputs}$ doit être égale aux nombre de sorties de $\\texttt{forward}$ (pourquoi ?) et le nombre de  \n",
    "sorties doit être égale aux nombres de $\\texttt{inputs}$ de $\\texttt{forward}$.\n",
    "\n",
    "\n",
    "Pour des raisons d'implémentation, les deux méthodes doivent être statiques. Le premier paramètre $\\texttt{ctx}$ permet de sauvegarder un contexte lors de la passe $\\texttt{forward}$ (par exemple les tenseurs d'entrées) et il est passé lors de la passe $\\texttt{backward}$ en paramètre afin de récupérer les valeurs. $\\textbf{Attention : }$ le contexte doit être unique pour chaque appel de $\\texttt{forward}$.\n",
    "\n",
    "Compléter le code ci-dessous pour créer des modules MSE (coût moindres carrés) et Linéaire. Les deux cellules en dessous vous serviront à tester votre code: si tout se passe sans plantage, alors vos gradients semblent corrects. Utiliser bien les outils propres à pyTorch, en particulier des Tensor et pas des matrices numpy. Assurez vous que\n",
    "vos fonctions prennent en entrée des batchs d’exemples (matrice 2D) et non un seul exemple (vecteur). N’hésiter pas à prendre un exemple et déterminer les dimensions des différentes matrices en jeu.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T12:03:32.376485736Z",
     "start_time": "2024-02-08T12:03:32.125556529Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import gradcheck\n",
    "\n",
    "\n",
    "class Context:\n",
    "    \"\"\"Un objet contexte très simplifié pour simuler PyTorch\n",
    "    \n",
    "    Un contexte différent doit être utilisé à chaque forward\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._saved_tensors = ()\n",
    "    def save_for_backward(self, *args):\n",
    "        self._saved_tensors = args\n",
    "    @property\n",
    "    def saved_tensors(self):\n",
    "        return self._saved_tensors\n",
    "\n",
    "\n",
    "class MSE(Function):\n",
    "    \"\"\"Début d'implementation de la fonction MSE\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, yhat, y):\n",
    "        ctx.save_for_backward(yhat, y)\n",
    "        return torch.pow(yhat - y, 2).mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        yhat, y = ctx.saved_tensors\n",
    "        grad_yhat = (2 * (yhat - y) / yhat.numel()) * grad_output\n",
    "        grad_y = (-2 * (yhat - y) / yhat.numel()) * grad_output\n",
    "        return grad_yhat, grad_y\n",
    "\n",
    "# [[STUDENT]] Implémenter la fonction Linear(X, W, b)sur le même modèle que MSE\n",
    "\n",
    "class Linear(Function):\n",
    "    \"\"\"Implémentation de la fonction Linear\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, W, b):\n",
    "        ctx.save_for_backward(X, W, b)\n",
    "        return (X @ W) + b\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        X, W, b = ctx.saved_tensors\n",
    "        grad_X = grad_output @ W.T\n",
    "        grad_W = (grad_output.t() @ X).t()\n",
    "        grad_b = grad_output.sum(0)\n",
    "        return grad_X, grad_W, grad_b\n",
    "\n",
    "# [[/STUDENT]]\n",
    "\n",
    "\n",
    "linear = Linear.apply\n",
    "mse = MSE.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T12:03:32.419224726Z",
     "start_time": "2024-02-08T12:03:32.197770244Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test du gradient de MSE \n",
    "yhat = torch.randn(10,5, requires_grad=True, dtype=torch.float64)\n",
    "y = torch.randn(10,5, requires_grad=True, dtype=torch.float64)\n",
    "torch.autograd.gradcheck(mse, (yhat, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T12:03:32.784595721Z",
     "start_time": "2024-02-08T12:03:32.353598188Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test du gradient de Linear (sur le même modèle que MSE)\n",
    "\n",
    "x = torch.randn(13, 5,requires_grad=True,dtype=torch.float64)\n",
    "w = torch.randn(5, 7,requires_grad=True,dtype=torch.float64)\n",
    "b = torch.randn(7,requires_grad=True,dtype=torch.float64)\n",
    "torch.autograd.gradcheck(linear,(x,w,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descente de Gradient\n",
    "\n",
    "Compléter ci-dessous le code pour réaliser la même regression linéaire qu'au TP précédent, mais en utilisant les objets Function déclarés ci-dessus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T12:04:12.090264600Z",
     "start_time": "2024-02-08T12:03:32.634040597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'exemples :  20640 Dimension :  8\n",
      "Nom des attributs :  MedInc, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude\n",
      "Itérations 0: loss 43846.203125\n",
      "Itérations 100: loss 1523.577880859375\n",
      "Itérations 200: loss 1278.374755859375\n",
      "Itérations 300: loss 1088.8912353515625\n",
      "Itérations 400: loss 942.36083984375\n",
      "Itérations 500: loss 828.9429321289062\n",
      "Itérations 600: loss 741.0527954101562\n",
      "Itérations 700: loss 672.8435668945312\n",
      "Itérations 800: loss 619.8068237304688\n",
      "Itérations 900: loss 578.4686279296875\n",
      "Itérations 1000: loss 546.1500244140625\n",
      "Itérations 1100: loss 520.7866821289062\n",
      "Itérations 1200: loss 500.786865234375\n",
      "Itérations 1300: loss 484.9237060546875\n",
      "Itérations 1400: loss 472.2508850097656\n",
      "Itérations 1500: loss 462.0400695800781\n",
      "Itérations 1600: loss 453.7286682128906\n",
      "Itérations 1700: loss 446.8838806152344\n",
      "Itérations 1800: loss 441.1713562011719\n",
      "Itérations 1900: loss 436.33392333984375\n",
      "Itérations 2000: loss 432.1722717285156\n",
      "Itérations 2100: loss 428.53472900390625\n",
      "Itérations 2200: loss 425.3022766113281\n",
      "Itérations 2300: loss 422.3843994140625\n",
      "Itérations 2400: loss 419.7110595703125\n",
      "Itérations 2500: loss 417.22784423828125\n",
      "Itérations 2600: loss 414.8932189941406\n",
      "Itérations 2700: loss 412.6751403808594\n",
      "Itérations 2800: loss 410.5487060546875\n",
      "Itérations 2900: loss 408.4949951171875\n",
      "Itérations 3000: loss 406.49932861328125\n",
      "Itérations 3100: loss 404.5501403808594\n",
      "Itérations 3200: loss 402.638671875\n",
      "Itérations 3300: loss 400.758056640625\n",
      "Itérations 3400: loss 398.90313720703125\n",
      "Itérations 3500: loss 397.0694885253906\n",
      "Itérations 3600: loss 395.2547302246094\n",
      "Itérations 3700: loss 393.45660400390625\n",
      "Itérations 3800: loss 391.6725769042969\n",
      "Itérations 3900: loss 389.9010314941406\n",
      "Itérations 4000: loss 388.140869140625\n",
      "Itérations 4100: loss 386.3923034667969\n",
      "Itérations 4200: loss 384.65380859375\n",
      "Itérations 4300: loss 382.9246520996094\n",
      "Itérations 4400: loss 381.20404052734375\n",
      "Itérations 4500: loss 379.49346923828125\n",
      "Itérations 4600: loss 377.7910461425781\n",
      "Itérations 4700: loss 376.0965881347656\n",
      "Itérations 4800: loss 374.4104309082031\n",
      "Itérations 4900: loss 372.7330627441406\n",
      "Itérations 5000: loss 371.0630187988281\n",
      "Itérations 5100: loss 369.4002685546875\n",
      "Itérations 5200: loss 367.7464294433594\n",
      "Itérations 5300: loss 366.09979248046875\n",
      "Itérations 5400: loss 364.4601135253906\n",
      "Itérations 5500: loss 362.8290710449219\n",
      "Itérations 5600: loss 361.2052307128906\n",
      "Itérations 5700: loss 359.5881652832031\n",
      "Itérations 5800: loss 357.9794006347656\n",
      "Itérations 5900: loss 356.37786865234375\n",
      "Itérations 6000: loss 354.7829284667969\n",
      "Itérations 6100: loss 353.1962585449219\n",
      "Itérations 6200: loss 351.6167297363281\n",
      "Itérations 6300: loss 350.04364013671875\n",
      "Itérations 6400: loss 348.4786376953125\n",
      "Itérations 6500: loss 346.9206237792969\n",
      "Itérations 6600: loss 345.36883544921875\n",
      "Itérations 6700: loss 343.82537841796875\n",
      "Itérations 6800: loss 342.28863525390625\n",
      "Itérations 6900: loss 340.7579345703125\n",
      "Itérations 7000: loss 339.235595703125\n",
      "Itérations 7100: loss 337.7196044921875\n",
      "Itérations 7200: loss 336.20989990234375\n",
      "Itérations 7300: loss 334.70855712890625\n",
      "Itérations 7400: loss 333.2130432128906\n",
      "Itérations 7500: loss 331.72412109375\n",
      "Itérations 7600: loss 330.2430419921875\n",
      "Itérations 7700: loss 328.7679138183594\n",
      "Itérations 7800: loss 327.2994384765625\n",
      "Itérations 7900: loss 325.83843994140625\n",
      "Itérations 8000: loss 324.3832092285156\n",
      "Itérations 8100: loss 322.93511962890625\n",
      "Itérations 8200: loss 321.4936828613281\n",
      "Itérations 8300: loss 320.05810546875\n",
      "Itérations 8400: loss 318.6302185058594\n",
      "Itérations 8500: loss 317.20819091796875\n",
      "Itérations 8600: loss 315.79248046875\n",
      "Itérations 8700: loss 314.38409423828125\n",
      "Itérations 8800: loss 312.98114013671875\n",
      "Itérations 8900: loss 311.5849609375\n",
      "Itérations 9000: loss 310.1955261230469\n",
      "Itérations 9100: loss 308.8114929199219\n",
      "Itérations 9200: loss 307.43487548828125\n",
      "Itérations 9300: loss 306.06396484375\n",
      "Itérations 9400: loss 304.6989440917969\n",
      "Itérations 9500: loss 303.3411560058594\n",
      "Itérations 9600: loss 301.988525390625\n",
      "Itérations 9700: loss 300.6427001953125\n",
      "Itérations 9800: loss 299.302978515625\n",
      "Itérations 9900: loss 297.9684753417969\n",
      "Itérations 10000: loss 296.6415710449219\n",
      "Itérations 10100: loss 295.3197937011719\n",
      "Itérations 10200: loss 294.00421142578125\n",
      "Itérations 10300: loss 292.6948547363281\n",
      "Itérations 10400: loss 291.3905334472656\n",
      "Itérations 10500: loss 290.0935974121094\n",
      "Itérations 10600: loss 288.8016052246094\n",
      "Itérations 10700: loss 287.5155944824219\n",
      "Itérations 10800: loss 286.23602294921875\n",
      "Itérations 10900: loss 284.9610595703125\n",
      "Itérations 11000: loss 283.69342041015625\n",
      "Itérations 11100: loss 282.4306945800781\n",
      "Itérations 11200: loss 281.173828125\n",
      "Itérations 11300: loss 279.9228820800781\n",
      "Itérations 11400: loss 278.6767578125\n",
      "Itérations 11500: loss 277.4378356933594\n",
      "Itérations 11600: loss 276.2034606933594\n",
      "Itérations 11700: loss 274.9752197265625\n",
      "Itérations 11800: loss 273.7525939941406\n",
      "Itérations 11900: loss 272.5348205566406\n",
      "Itérations 12000: loss 271.32366943359375\n",
      "Itérations 12100: loss 270.1169738769531\n",
      "Itérations 12200: loss 268.916748046875\n",
      "Itérations 12300: loss 267.7214050292969\n",
      "Itérations 12400: loss 266.5314025878906\n",
      "Itérations 12500: loss 265.34735107421875\n",
      "Itérations 12600: loss 264.1676025390625\n",
      "Itérations 12700: loss 262.9947509765625\n",
      "Itérations 12800: loss 261.8259582519531\n",
      "Itérations 12900: loss 260.6632385253906\n",
      "Itérations 13000: loss 259.50555419921875\n",
      "Itérations 13100: loss 258.3529052734375\n",
      "Itérations 13200: loss 257.2061767578125\n",
      "Itérations 13300: loss 256.0634765625\n",
      "Itérations 13400: loss 254.9275360107422\n",
      "Itérations 13500: loss 253.79556274414062\n",
      "Itérations 13600: loss 252.66949462890625\n",
      "Itérations 13700: loss 251.54811096191406\n",
      "Itérations 13800: loss 250.4318389892578\n",
      "Itérations 13900: loss 249.3209991455078\n",
      "Itérations 14000: loss 248.21446228027344\n",
      "Itérations 14100: loss 247.11399841308594\n",
      "Itérations 14200: loss 246.01730346679688\n",
      "Itérations 14300: loss 244.92686462402344\n",
      "Itérations 14400: loss 243.84042358398438\n",
      "Itérations 14500: loss 242.75955200195312\n",
      "Itérations 14600: loss 241.68324279785156\n",
      "Itérations 14700: loss 240.6117706298828\n",
      "Itérations 14800: loss 239.54551696777344\n",
      "Itérations 14900: loss 238.48350524902344\n",
      "Itérations 15000: loss 237.42727661132812\n",
      "Itérations 15100: loss 236.3745574951172\n",
      "Itérations 15200: loss 235.32827758789062\n",
      "Itérations 15300: loss 234.2852783203125\n",
      "Itérations 15400: loss 233.2480926513672\n",
      "Itérations 15500: loss 232.21490478515625\n",
      "Itérations 15600: loss 231.1868438720703\n",
      "Itérations 15700: loss 230.16317749023438\n",
      "Itérations 15800: loss 229.1441650390625\n",
      "Itérations 15900: loss 228.12998962402344\n",
      "Itérations 16000: loss 227.11996459960938\n",
      "Itérations 16100: loss 226.115234375\n",
      "Itérations 16200: loss 225.11412048339844\n",
      "Itérations 16300: loss 224.11868286132812\n",
      "Itérations 16400: loss 223.12643432617188\n",
      "Itérations 16500: loss 222.14013671875\n",
      "Itérations 16600: loss 221.15704345703125\n",
      "Itérations 16700: loss 220.17971801757812\n",
      "Itérations 16800: loss 219.20562744140625\n",
      "Itérations 16900: loss 218.2370147705078\n",
      "Itérations 17000: loss 217.2718963623047\n",
      "Itérations 17100: loss 216.31187438964844\n",
      "Itérations 17200: loss 215.35565185546875\n",
      "Itérations 17300: loss 214.4042205810547\n",
      "Itérations 17400: loss 213.4567413330078\n",
      "Itérations 17500: loss 212.51373291015625\n",
      "Itérations 17600: loss 211.5749969482422\n",
      "Itérations 17700: loss 210.64047241210938\n",
      "Itérations 17800: loss 209.71034240722656\n",
      "Itérations 17900: loss 208.7841796875\n",
      "Itérations 18000: loss 207.862548828125\n",
      "Itérations 18100: loss 206.9447021484375\n",
      "Itérations 18200: loss 206.03150939941406\n",
      "Itérations 18300: loss 205.1219024658203\n",
      "Itérations 18400: loss 204.21701049804688\n",
      "Itérations 18500: loss 203.3155975341797\n",
      "Itérations 18600: loss 202.41903686523438\n",
      "Itérations 18700: loss 201.52581787109375\n",
      "Itérations 18800: loss 200.63734436035156\n",
      "Itérations 18900: loss 199.75218200683594\n",
      "Itérations 19000: loss 198.87179565429688\n",
      "Itérations 19100: loss 197.99459838867188\n",
      "Itérations 19200: loss 197.12216186523438\n",
      "Itérations 19300: loss 196.25294494628906\n",
      "Itérations 19400: loss 195.3883514404297\n",
      "Itérations 19500: loss 194.52699279785156\n",
      "Itérations 19600: loss 193.67027282714844\n",
      "Itérations 19700: loss 192.81674194335938\n",
      "Itérations 19800: loss 191.96768188476562\n",
      "Itérations 19900: loss 191.1218719482422\n",
      "Itérations 20000: loss 190.280517578125\n",
      "Itérations 20100: loss 189.4424591064453\n",
      "Itérations 20200: loss 188.60861206054688\n",
      "Itérations 20300: loss 187.7782745361328\n",
      "Itérations 20400: loss 186.9518585205078\n",
      "Itérations 20500: loss 186.12908935546875\n",
      "Itérations 20600: loss 185.31005859375\n",
      "Itérations 20700: loss 184.49485778808594\n",
      "Itérations 20800: loss 183.68310546875\n",
      "Itérations 20900: loss 182.8752899169922\n",
      "Itérations 21000: loss 182.0707550048828\n",
      "Itérations 21100: loss 181.27052307128906\n",
      "Itérations 21200: loss 180.4730682373047\n",
      "Itérations 21300: loss 179.68019104003906\n",
      "Itérations 21400: loss 178.88978576660156\n",
      "Itérations 21500: loss 178.1041717529297\n",
      "Itérations 21600: loss 177.32077026367188\n",
      "Itérations 21700: loss 176.54244995117188\n",
      "Itérations 21800: loss 175.76614379882812\n",
      "Itérations 21900: loss 174.9947967529297\n",
      "Itérations 22000: loss 174.2257080078125\n",
      "Itérations 22100: loss 173.4611053466797\n",
      "Itérations 22200: loss 172.6991729736328\n",
      "Itérations 22300: loss 171.94126892089844\n",
      "Itérations 22400: loss 171.18643188476562\n",
      "Itérations 22500: loss 170.43511962890625\n",
      "Itérations 22600: loss 169.6873321533203\n",
      "Itérations 22700: loss 168.9425506591797\n",
      "Itérations 22800: loss 168.20169067382812\n",
      "Itérations 22900: loss 167.46334838867188\n",
      "Itérations 23000: loss 166.7294921875\n",
      "Itérations 23100: loss 165.99752807617188\n",
      "Itérations 23200: loss 165.27053833007812\n",
      "Itérations 23300: loss 164.54551696777344\n",
      "Itérations 23400: loss 163.8247833251953\n",
      "Itérations 23500: loss 163.1065216064453\n",
      "Itérations 23600: loss 162.39195251464844\n",
      "Itérations 23700: loss 161.680419921875\n",
      "Itérations 23800: loss 160.9719696044922\n",
      "Itérations 23900: loss 160.2671661376953\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 33\u001B[0m\n\u001B[1;32m     30\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mItérations \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mn_iter\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: loss \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# Calcul du backward (grad_w, grad_b)\u001B[39;00m\n\u001B[0;32m---> 33\u001B[0m grad_mse \u001B[38;5;241m=\u001B[39m \u001B[43mMSE\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmse_context\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     34\u001B[0m grad_X, grad_W, grad_b \u001B[38;5;241m=\u001B[39m Linear\u001B[38;5;241m.\u001B[39mbackward(linear_context, grad_mse[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# Mise à jour des paramètres du modèle\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[7], line 31\u001B[0m, in \u001B[0;36mMSE.backward\u001B[0;34m(ctx, grad_output)\u001B[0m\n\u001B[1;32m     29\u001B[0m yhat, y \u001B[38;5;241m=\u001B[39m ctx\u001B[38;5;241m.\u001B[39msaved_tensors\n\u001B[1;32m     30\u001B[0m grad_yhat \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m (yhat \u001B[38;5;241m-\u001B[39m y) \u001B[38;5;241m/\u001B[39m yhat\u001B[38;5;241m.\u001B[39mnumel()) \u001B[38;5;241m*\u001B[39m grad_output\n\u001B[0;32m---> 31\u001B[0m grad_y \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m (yhat \u001B[38;5;241m-\u001B[39m y) \u001B[38;5;241m/\u001B[39m \u001B[43myhat\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m) \u001B[38;5;241m*\u001B[39m grad_output\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m grad_yhat, grad_y\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import torch\n",
    "\n",
    "# Chargement des données Boston et transformation en tensor\n",
    "housing = fetch_california_housing()\n",
    "x = torch.tensor(housing['data'], dtype=torch.float)\n",
    "y = torch.tensor(housing['target'], dtype=torch.float).view(-1, 1)\n",
    "\n",
    "print(\"Nombre d'exemples : \", x.size(0), \"Dimension : \", x.size(1))\n",
    "print(\"Nom des attributs : \", \", \".join(housing['feature_names']))\n",
    "\n",
    "# Initialisation aléatoire de w et b\n",
    "w = torch.randn(x.size(1),1)\n",
    "b =  torch.randn(1,1)\n",
    "\n",
    "EPOCHS = 50000\n",
    "EPS = 1e-7\n",
    "\n",
    "for n_iter in range(EPOCHS):\n",
    "    # Calcul du forward (loss), avec création de nouveaux Context pour chaque module\n",
    "\n",
    "    mse_context = Context()\n",
    "    linear_context = Context()\n",
    "    \n",
    "    y_pred = Linear.forward(linear_context, x, w, b)\n",
    "    loss = MSE.forward(mse_context, y_pred, y)\n",
    "\n",
    "    # Affichage de la loss à chaque 100 itérations\n",
    "    if n_iter % 100 == 0:\n",
    "        print(f\"Itérations {n_iter}: loss {loss}\")\n",
    "\n",
    "    # Calcul du backward (grad_w, grad_b)\n",
    "    grad_mse = MSE.backward(mse_context, 1)\n",
    "    grad_X, grad_W, grad_b = Linear.backward(linear_context, grad_mse[0])\n",
    "    \n",
    "    # Mise à jour des paramètres du modèle\n",
    "    w -= EPS * grad_W  # Update weights\n",
    "    b -= EPS * grad_b  # Update bias"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DeepLearning fc TP1 2020-2021-correction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
