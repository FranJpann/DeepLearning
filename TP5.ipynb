{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1099e37",
   "metadata": {},
   "source": [
    "# Deep learning pour le texte\n",
    "L'objectif de ce TP est d'apprivoiser les embeddings de mots, de tokens, ainsi que les architectures Transformer. Voici ce que nous allons aborder : \n",
    "- Chargement des embeddings pré-entrainés\n",
    "- Manipulation des embeddings\n",
    "- Visualuation des embeddings\n",
    "- Classification de phrases courtes, en fonction de différentes représentations et architectures\n",
    "- Classification de textes : Finetuning d'un Transformer pre-entraîné sur le corpus IMDB \n",
    "\n",
    "Vous aurez besoin des installations suivantes (en plus de pytorch et numpy): \n",
    "- conda install pandas\n",
    "- conda install matplotlib\n",
    "- conda install -c anaconda scikit-learn\n",
    "- conda install ipywidgets\n",
    "- conda install gensim\n",
    "- conda install datasets \n",
    "- conda install -c huggingface transformers\n",
    "- conda install tqdm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2235348f",
   "metadata": {},
   "source": [
    " ## Configuration environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52e4b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# pour colab, decommenter:    \n",
    "# !pip install torch==1.8.0+cu111 torchtext==0.9.0 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fba3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Progrès\n",
    "from tqdm import tqdm\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cca82b1",
   "metadata": {},
   "source": [
    "## Chargement d'embeddings et calculs de similarité\n",
    "\n",
    "Pour commencer à travailler sur le texte, nous allons dans un premier temps utiliser un ensemble de vecteurs d'embeddings appris via le modèle Glove (similaire à Word2Vec vu en cours), que nous allons charger via la librairie gensim (il s'agit d'un ensemble d'embeddings parmi d'autres, de nombreux autres, appris sur des corpus différents existent dans gensim ou d'autres librairies comme spacy). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dcc32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "embeds = api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "# affichage de l'embedding du mot \"book\"\n",
    "print(embeds['book']) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c340e517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methode most_similar permet d'obtenir les mots les plus proches d'un mot dans l'espace d'embeddings. sa specification peut être obtenue en executant la ligne suivante : \n",
    "help(embeds.most_similar) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f09ced",
   "metadata": {},
   "source": [
    "Afficher les 5 mots les plus proches de \"cat\" via la méthode model.most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aa034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[[Student/]]\n",
    "\n",
    "#[[/Student]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472f7436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La similarité est calculée selon une mesure de cosinus dans l'espace des embeddings\n",
    "# par exemple\n",
    "print(embeds.similarity(\"apple\", \"banana\"))\n",
    "print(embeds.similarity(\"apple\", \"dog\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc58f9d",
   "metadata": {},
   "source": [
    "### Out Of Distribution \n",
    "Attention, bien sûr tous les mots possibles ne sont pas inclus dans le dictionnaire d'embeddings, leur sémantique dépend notamment fortement du corpus sur lequel ils ont été appris.\n",
    "\n",
    "Par exemple le mot \"covid\" n'est pas présent, l'execution de model['covid'] ferait planter l'execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7372fd",
   "metadata": {},
   "source": [
    "Pour éviter ce genre de problème par exemple pour le traitement d'un texte ne contenant pas certains mots, ils convient de vérifier leur présence dans le vocabulaire (et alors ignorer les mots correspondants). Ceci peut se faire via vocab.keys comme ci-dessous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826df032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le vocabulaire du modèle peut être obtenus via vocab.keys, qui retourne l'ensemble des tokens (mots) pour lesquels il existe un embedding dans le modèle\n",
    "vocab = embeds.key_to_index.keys()\n",
    "np.random.choice(list(vocab), 5)\n",
    "\n",
    "x=\"covid\"\n",
    "if x in vocab:\n",
    "    print(embeds[x])\n",
    "else: print(\"oov\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c69e3e",
   "metadata": {},
   "source": [
    "### Visualisation des embeddings\n",
    "\n",
    "On souhaite maintenant visualiser les embeddings dans un espace en 2D. Pour cela, on utilise t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85c8d6d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "words=['cat','mouse','dog','car','truck','motorcycle','bike','lion','plane','deers']\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def tsne_plot(model, words,n_components=2,perplexity=40):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    vocab = model.key_to_index.keys()\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocab: \n",
    "            tokens.append(model[word])\n",
    "            labels.append(word)\n",
    "\n",
    "    tsne_model = TSNE(perplexity=perplexity, n_components=n_components, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "    fig = plt.figure(figsize=(16, 16))\n",
    "    if n_components==3:\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(new_values[:,0],new_values[:,1],new_values[:,2],c=\"r\",marker=\"o\")\n",
    "        for i in range(len(new_values)):\n",
    "            ax.text(new_values[i][0],new_values[i][1],new_values[i][2],labels[i])\n",
    "    else:\n",
    "        plt.scatter(new_values[:,0],new_values[:,1])\n",
    "        for i in range(len(new_values)):\n",
    "            plt.annotate(labels[i],\n",
    "                        xy=(new_values[i][0],new_values[i][1]),\n",
    "                        xytext=(5, 2),\n",
    "                        textcoords='offset points',\n",
    "                        ha='right',\n",
    "                        va='bottom')\n",
    "    return new_values,labels\n",
    "\n",
    "new_values,labels = tsne_plot(embeds,words,n_components=2,perplexity=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90ddc86",
   "metadata": {},
   "source": [
    "### Index des Embeddings\n",
    "\n",
    "Lors de l'établissement de modèles utilisant les embeddings du vocabulaire, il est utile de considérer des indices des mots dans le vocabulaire plutôt que les mots eux mêmes (pour les traiter par exemple dans des tenseurs avant de les remplacer par leurs vecteurs de poids). Ceci se fait dans gensim par l'utilisation des instructions suivantes:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0493c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeds.key_to_index[\"book\"])\n",
    "print(embeds.index_to_key[539])\n",
    "\n",
    "# Deux manières equivalentes de recupérer les poids\n",
    "print(embeds[\"book\"])   # via le mot \n",
    "print(embeds.vectors[embeds.key_to_index[\"book\"]]) #via l'index\n",
    "\n",
    "\n",
    "# taille du vocabulaire: \n",
    "print(embeds.vectors.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3818f91c",
   "metadata": {},
   "source": [
    "#### Exercice 1 : Similarité de phrases\n",
    "\n",
    "On souhaite calculer la matrice de similarité de différentes phrases du dataset suivant:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607bb700",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = pd.DataFrame([\n",
    "    ['the road is straight', 'Y'],\n",
    "    ['the black cat plays with a ball', 'N'],\n",
    "    ['a big dog with a ball', 'N'],\n",
    "    ['dog and cat are together', 'N'],\n",
    "    ['traffic jam on the 6th road', 'Y'],\n",
    "    ['white bird on a big tree', 'N'],\n",
    "    ['a big truck', 'Y'],\n",
    "    ['two cars crashed', 'Y'],\n",
    "    ['two deers in a field', 'N'],\n",
    "    ['I like ridding my bike','Y'],\n",
    "    ['a lion in the savane','N'],\n",
    "    ['a motorcycle rides on the road','Y'],\n",
    "    ['it is a bike, it is not a flamingo', 'Y'], \n",
    "    ['it is not a bike, it is a flamingo', 'N'],\n",
    "    ['a mouse bitten by a cat','N'],\n",
    "    ['two pigs in the mood','N'],\n",
    "    ['take a plane is sometimes slower than taking train','Y'],\n",
    "    ['take the highway','Y']\n",
    "], columns=['text', 'label'])\n",
    "tdf\n",
    "\n",
    "#\n",
    "#  \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4b3abb",
   "metadata": {},
   "source": [
    "Pour chaque phrase, on va moyenner les embeddings de ces mots. Chaque vecteur de phrase sera normalisé (x.norm()). Les phrases pourront être comparées deux à deux par un produit scalaire.\n",
    "\n",
    "- chaque phrase doit être découpée : (\"the\", \"cat\", \"is\" , \"on\", \"the\",\"bank\")\n",
    "- on définit une fonction getvectors qui à partir de la liste de phrases découpées : moyenne les vecteurs d'embeddings (pensez à utiliser les tenseurs de torch), normalise le resultat et le retourne. \n",
    "- on peut ensuite calculer la matrice de similarité qui sera donnée à la fonction visual_similarity_matrix() fournie ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f3927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Premiere possibilité :\n",
    "def getlistwordsentence(text):\n",
    "    ret=[]\n",
    "    for sentence in text:\n",
    "        print(sentence)\n",
    "        ret.append(sentence.split(\" \"))\n",
    "    return ret\n",
    "\n",
    "\n",
    "# Deuxième possibilité (plus robuste, gère la ponctuation, etc.):\n",
    "import gensim\n",
    "def getlistwordsentence2(text):\n",
    "    ret=[]\n",
    "    for sentence in text:\n",
    "        ret.append(list(gensim.utils.tokenize(sentence)))\n",
    "    return ret\n",
    "\n",
    "text_wordlist = getlistwordsentence2(tdf['text'])\n",
    "print(text_wordlist)\n",
    "\n",
    "def getvectors(wordslist, normalize=True):\n",
    "    #Answer[[\n",
    "    \n",
    "    #]]Answer\n",
    "    \n",
    "\n",
    "x, sentences = getvectors(text_wordlist,True)\n",
    "\n",
    "print(x)\n",
    "\n",
    "def getSims(x):\n",
    "    #Answer[[\n",
    "    \n",
    "    #]]Answer\n",
    "\n",
    "innerproducts=getSims(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c265e3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_similarity_matrix(innerproducts):\n",
    "  fig, ax = plt.subplots()\n",
    "  im = ax.imshow(innerproducts)\n",
    "\n",
    "  # We want to show all ticks...\n",
    "  ax.set_xticks(np.arange(len(sentences)))\n",
    "  ax.set_yticks(np.arange(len(sentences)))\n",
    "  # ... and label them with the respective list entries\n",
    "  ax.set_xticklabels(sentences)\n",
    "  ax.set_yticklabels(sentences)\n",
    "\n",
    "  # Rotate the tick labels and set their alignment.\n",
    "  plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "  # Loop over data dimensions and create text annotations.\n",
    "  for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "      text = ax.text(j, i, \"%.2f\" % innerproducts[i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "  ax.set_title(\"Produit scalaire des phrases\")\n",
    "  fig.tight_layout()\n",
    "  fig.set_size_inches(38.5, 38.5)\n",
    "  plt.show()\n",
    "\n",
    "visual_similarity_matrix(innerproducts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac350f2",
   "metadata": {},
   "source": [
    "## Utilisation des embeddings pour une tâche de classification\n",
    "\n",
    "On s'intéresse maintenant à apprendre un classifieur sur les données de l'exemple jouet défini ci dessus. On utilisera le jeu de données suivant comme jeu de validation :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b90129",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdf = pd.DataFrame([\n",
    "    ['the bike drives on the road', 'Y'],\n",
    "    ['a lion and a cat in a tree', 'N'],\n",
    "    ['two cars crashed', 'Y'],\n",
    "    ['i always go to work by bike', 'Y'],\n",
    "    ['i have no animal at home', 'N'],\n",
    "    ['dogs like cheese', 'N'], \n",
    "    ['a pink flamingo','N'],\n",
    "    ['trucks','Y'],\n",
    "    ['truckks','Y'],\n",
    "    ['truckmegatruck', 'Y'], \n",
    "    ['a text about trucks, not animals','Y'], \n",
    "    ['a text about animals, not trucks','N'],\n",
    "    ['doggs','N']\n",
    "], columns=['text', 'label'])\n",
    "vdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21637614",
   "metadata": {},
   "source": [
    "La première étape  consiste à transformer les données (label numérique, tokenization, construction d'un batch.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7054c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_dataset = Dataset.from_pandas(tdf)\n",
    "validation_dataset = Dataset.from_pandas(vdf)\n",
    "print(train_dataset,validation_dataset)\n",
    "print(train_dataset[\"text\"])\n",
    "\n",
    "# Creation d'un index de padding pour les positions a ignorer par le modèle (e.g., mots inconnus)\n",
    "pad_idx=embeds.vectors.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    vocab=embeds.key_to_index\n",
    "    inputs= getlistwordsentence2(examples[\"text\"])\n",
    "    \n",
    "    inputs = [[(vocab[word] if word in vocab else pad_idx) for word in sentence] for sentence in inputs] \n",
    "    \n",
    "    labels = [(1 if l=='Y' else 0) for l in examples[\"label\"]] \n",
    "    ret = {}\n",
    "    ret[\"input_ids\"]=inputs\n",
    "    ret[\"labels\"]=labels\n",
    "    \n",
    "    return ret\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.features.keys(), load_from_cache_file=True)\n",
    "print(train_dataset)\n",
    "print(train_dataset[\"input_ids\"])\n",
    "validation_dataset = validation_dataset.map(preprocess_function, batched=True, remove_columns=validation_dataset.features.keys(), load_from_cache_file=True)\n",
    "print(validation_dataset)\n",
    "print(validation_dataset[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa696ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers.trainer_pt_utils import LengthGroupedSampler,Sampler,get_length_grouped_indices\n",
    "\n",
    "batchsize=4\n",
    "megabatch_mul=16\n",
    "\n",
    "\n",
    "# Un sampler permet de générer les batchs en selectionnant des indices d'échantillons de manière aléatoire\n",
    "# dans les données d'entrée\n",
    "# Ici on souhaite minimiser le padding, donc on choisit de regrouper au maximum les sequences par longueur \n",
    "# dans les batchs\n",
    "# C'est fait selon les étapes suivantes :  \n",
    "#     1 - Tirage d'une permutation des indices de manière aléatoire\n",
    "#     2 - Découpage de la liste en megabatchs de taille batch_size*megabatch_mul\n",
    "#     3 - Tri des éléments par ordre de longueur déscendante à l'intérieur de chaque megabatch\n",
    "#     4 - Concaténation des listes d'indices retriées localement \n",
    "#     5 - Découpage en batch de taille batchsize\n",
    "#\n",
    "# Ainsi:\n",
    "#     - si megabatch_mul trop grand, alors aucun aléatoire (ce qui peu être gênant pour l'apprentissage dans certains cas)\n",
    "#                                   ==> les batchs seront toujours les mêmes (données triées par longueur, minimisation optimale du padding)\n",
    "#     - si megabatch_mul trop petit (e.g. = 1), alors batchs complètement aléatoires (séquences simplement ordonnées à l'intérieur du batch, pas d'optimisation sur la minimisation du padding) \n",
    "#                                   ==> beaucoup de padding possible (rajoute de la complexité à l'apprentissage)\n",
    "#\n",
    "# Il s'agit de trouver un bon compromis, jouer en tp avec les valeurs de megabatch_mul et observer les effets\n",
    "class LengthGroupedSampler(Sampler):\n",
    "    r\"\"\"\n",
    "    Sampler that samples indices in a way that groups together features of the dataset of roughly the same length while\n",
    "    keeping a bit of randomness.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,batch_size,dataset, megabatch_mul=None):\n",
    "        self.batch_size = batch_size\n",
    "        lengths = [len(sample[\"input_ids\"]) for sample in dataset]\n",
    "        self.lengths = lengths\n",
    "        self.megabatch_mul=megabatch_mul\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lengths)\n",
    "\n",
    "    def __iter__(self):\n",
    "        indices = get_length_grouped_indices(self.lengths, self.batch_size, self.megabatch_mul)\n",
    "        return iter(indices)\n",
    "\n",
    "    \n",
    "# Fonction qui produit des tenseurs pytorch a partir d'un batch de données issu du sampler\n",
    "#\n",
    "# ici on crée deux tenseurs par batch :\n",
    "#         - un tenseur pour les labels\n",
    "#         - un tenseur pour les index de mots des séquences, complétées avec l'idex de padding pad_idx pour les séquences plus courtes (afin d'avoir des données de même taille sur chaque ligne du tenseur, ce qui est requis pour leur création) \n",
    "def data_collator(batch):\n",
    "    first = batch[0]\n",
    "    ret = {}\n",
    "    dtype = torch.long if type(first[\"labels\"]) is int else torch.float\n",
    "    ret[\"labels\"] = torch.tensor([f[\"labels\"] for f in batch], dtype=dtype)\n",
    "    longest=max([len(l[\"input_ids\"]) for l in batch])\n",
    "    s = np.stack([np.pad(x[\"input_ids\"], (0, longest - len(x[\"input_ids\"])), constant_values=pad_idx) for x in batch])\n",
    "    ret[\"input_ids\"] = torch.tensor(s)\n",
    "    return ret\n",
    "\n",
    "\n",
    "train_sampler = LengthGroupedSampler(batchsize, train_dataset, megabatch_mul)\n",
    "\n",
    "train_loader=DataLoader(train_dataset,batchsize,sampler=train_sampler,collate_fn=data_collator,pin_memory=True, shuffle=False, num_workers=0)\n",
    "\n",
    "for data in train_loader:\n",
    "    print(data)\n",
    "\n",
    "test_sampler = LengthGroupedSampler(batchsize, validation_dataset, megabatch_mul)    \n",
    "    \n",
    "test_loader=DataLoader(validation_dataset,batchsize,sampler=test_sampler,collate_fn=data_collator,pin_memory=True, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "for data in test_loader:\n",
    "    print(data)\n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a976a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### On peut créer une fonction de décodage des batchs\n",
    "def batch_decode(input_ids):\n",
    "    x=input_ids.data\n",
    "    x=[\" \".join([embeds.index_to_key[i] for i in s if i!=pad_idx]) for s in x]\n",
    "    return x\n",
    "\n",
    "for data in test_loader:\n",
    "    print(data)\n",
    "    print(batch_decode(data[\"input_ids\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b42c9e",
   "metadata": {},
   "source": [
    "On note que le dernier texte \"truckmegatruck\" est décodé sous la chaîne vide: il ne contient que des mots inconnus, donc ignorés... \n",
    "\n",
    "### Modèle de classifcation simple\n",
    "\n",
    "On construit ensuite le modèle de classification avec torch. La couche d'entrée correspond aux embeddings, implémentés par nn.Embedding(). nn.Embedding() permet de construire la matrice d'embeddings sur l'ensemble du vocabulaire. Ils sont ensuite \"activés\" en fonction du texte d'entrée. Le modèle à construire fait une simple moyenne des embeddings des mots des textes, applique une activation tanh et envoie le resultat à travers un Linear à deux sorties. Penser à indiquer l'index de padding à la construction de nn.Embedding, car çà permet de ne pas les prendre en compte dans les calculs de gradients (et de conserver ces embeddings vides à zeros pour ne pas en dépendre => invariance par rapport à la longueur). Penser aussi à ajouter de la l2 sur les embeddings (très importants pour que les représentations ne s'éparpillent pas aux confins de l'espace de représentation!). On pourra aussi ajouter du Dropout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c1ea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "        \n",
    "class MyModel(nn.Module):\n",
    "    #Answer[[\n",
    "    \n",
    "    #]]Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbe0f84",
   "metadata": {},
   "source": [
    "On entraîne et teste le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad8b49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(train_iter, test_iter, model, loss_function, optimizer, epochs, clip=-1):\n",
    "  for epoch in range(epochs):\n",
    "      epoch_loss = 0\n",
    "      epoch_accuracy = 0\n",
    "      model.train()\n",
    "      nb_samples=0\n",
    "      for batch in train_iter:\n",
    "          optimizer.zero_grad()\n",
    "          #print(\"text shape \",batch.text.T.shape)\n",
    "          prediction = model(batch[\"input_ids\"])\n",
    "          if not isinstance(prediction,torch.Tensor):  \n",
    "                prediction = prediction[\"logits\"]\n",
    "          #print(prediction)\n",
    "          loss = loss_function(prediction, batch[\"labels\"])\n",
    "\n",
    "          loss.backward()\n",
    "          if clip>0:\n",
    "              torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "          optimizer.step()\n",
    "          nb_samples+=prediction.shape[0]\n",
    "          epoch_loss+=loss.item()*prediction.shape[0]\n",
    "          preds=(prediction[:,1]>prediction[:,0])*1.0\n",
    "          accuracy=(preds==batch[\"labels\"]).sum()\n",
    "          epoch_accuracy+=accuracy.item()\n",
    "      print('train loss on epoch {} : {:.3f}'.format(epoch, epoch_loss/nb_samples))\n",
    "      print('train accuracy on epoch {}: {:.3f}'.format(epoch, epoch_accuracy/nb_samples))\n",
    "      \n",
    "      model.eval()\n",
    "      test_loss = 0\n",
    "      test_accuracy = 0\n",
    "      nb_samples=0\n",
    "      accuracy=0\n",
    "      for batch in test_iter:\n",
    "          #print(\"test \",batch)\n",
    "          with torch.no_grad():\n",
    "              optimizer.zero_grad()\n",
    "                \n",
    "              prediction = model(batch[\"input_ids\"])\n",
    "              if not isinstance(prediction,torch.Tensor):\n",
    "                    prediction = prediction[\"logits\"]\n",
    "              loss = loss_function(prediction, batch[\"labels\"])\n",
    "              nb_samples+=prediction.shape[0]\n",
    "              test_loss+=loss.item()*prediction.shape[0]\n",
    "              #print(batch_decode(batch[\"input_ids\"]))\n",
    "              preds=(prediction[:,1]>prediction[:,0])*1.0\n",
    "              accuracy=(preds==batch[\"labels\"]).sum()\n",
    "              test_accuracy+=accuracy.item()  \n",
    "              #print(preds,accuracy)\n",
    "      print('test loss on epoch {}: {:.3f}'.format(epoch, test_loss/nb_samples))\n",
    "      print('test accuracy on epoch {}: {:.3f}'.format(epoch, test_accuracy/nb_samples))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1051f628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# penser à ajouter un embedding pour pad_idx        \n",
    " \n",
    "#[[Answer \n",
    "# net = ...\n",
    "\n",
    "#/Answer]]\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(net.parameters(), lr=0.01,weight_decay=0.01)\n",
    "epochs = 5000\n",
    "train_test(train_loader, test_loader, net, loss_function, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384b3da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence,model):\n",
    "    examples={}\n",
    "    examples[\"text\"]=[sentence]\n",
    "    examples[\"label\"]=[\"Y\"]\n",
    "    data=preprocess_function(examples)\n",
    "    #print(data)\n",
    "    data=data_collator([data])\n",
    "    #print(data[\"input_ids\"])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model(data[\"input_ids\"][0])\n",
    "        if not isinstance(prediction,torch.Tensor):\n",
    "            prediction = prediction[\"logits\"]\n",
    "        preds=(prediction[:,1]>prediction[:,0])*1.0\n",
    "    return {\"logits\":prediction,\"prediction\":preds}\n",
    "    \n",
    "def predict_from_pandas(datap,net):\n",
    "    data=Dataset.from_pandas(vdf)\n",
    "    accuracy=0\n",
    "    size=len(data[\"text\"])\n",
    "    for i in range(size):\n",
    "        s=data[\"text\"][i]\n",
    "        l=data[\"label\"][i]\n",
    "        l=(1 if l=='Y' else 0)\n",
    "        p=predict(s,net)\n",
    "        print(s,p, \"truth=\",l)\n",
    "        accuracy+=(p[\"prediction\"]==l)\n",
    "    return accuracy/size\n",
    "\n",
    "print(predict_from_pandas(vdf,net))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97509ac7",
   "metadata": {},
   "source": [
    "On observe beaucoup de sur-apprentissage. Normal étant donné la taille du corpus d'apprentissage...\n",
    "\n",
    "\n",
    "### Même modèle mais avec des embeddings pré-entraînés\n",
    "\n",
    " Voyons ce que celà donne avec les représentations pre-entrainées (que l'on freeze) \n",
    "\n",
    "#### Méthode 1 : directement fournir les embeddings construits à partir du vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9367343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#[[Answer \n",
    "# net = ...\n",
    "# weights=\n",
    "\n",
    "#/Answer]]\n",
    "\n",
    "net.embedding.weight.data=weights   # on charge les pre-train \n",
    "net.embedding.weight.requires_grad=False  # on freeze\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(net.parameters(), lr=0.01,weight_decay=0.0)\n",
    "epochs = 5000\n",
    "\n",
    "train_test(train_loader, test_loader, net, loss_function, optimizer, epochs,clip=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714f4f78",
   "metadata": {},
   "source": [
    "#### Méthode 2 : avec la méthode nn.Embedding.from_pretrained()\n",
    "\n",
    "nn.Embedding permet d'importer directement une matrice de poids (embeddings pré-entraînés) grâce à la méthode from_pretrained() :  \n",
    "```\n",
    "weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\n",
    "embedding = nn.Embedding.from_pretrained(weight)\n",
    "```\n",
    "Plus particulièrement, la matrice de poids correspond à notre élément vocab.vectors qui va permettre d'initialiser la matrice d'embeddings.\n",
    "\n",
    "Par défaut, les embeddings sont \"gelés\" : ils ne sont pas modifiés avec la backpropagation, mais il est possible de les modifier avec le paramètre freeze=False. Cela revient à \"fine-tuner\" les embeddings sur la tâche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cfc2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[[Answer \n",
    "\n",
    "#/Answer]]\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(net.parameters(), lr=0.01,weight_decay=0.0)\n",
    "epochs = 5000\n",
    "train_test(train_loader, test_loader, net, loss_function, optimizer, epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6345cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_from_pandas(vdf,net))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978edd51",
   "metadata": {},
   "source": [
    "Ok c'est beaucoup mieux, mais on a toujours des limites importantes: \n",
    "- Pas de prise en compte de l'ordre des mots : on voit que \"a text about trucks, not animals\" et \"a text about animals, not trucks\" retournent exactement les mêmes scores de prediction\n",
    "- Pas de gestion des mots hors vocabulaire. Exemple truckmegatruck n'est pas géré, et retourne exactement les mêmes predictions que les deux textes avec fautes de frappe truckks et doggs \n",
    "\n",
    "###  Tokenizers \n",
    "\n",
    "Pour aller plus loin, on propose maintenant d'utiliser des tokens issus d'un tokenizer plus évolué, du type Byte Pair Encoding vu en cours, pour voir si cela pourrait améliorer les performances. \n",
    "\n",
    "On commence par récupérer un tokenizer pré-entraîné sur un corpus : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a167e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# De la même manière qu'avec Glove, on peut observer le vocabulaire : \n",
    "print(tokenizer.vocab)\n",
    "\n",
    "vocab_size=len(tokenizer.vocab)\n",
    "print(\"Size of Vocab : \",vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3f3994",
   "metadata": {},
   "source": [
    "Pour pouvoir utiliser ce tokenizer dans nos modèles, il faut recréer le datasets. \n",
    "\n",
    "Donner ci-dessous le code de la fonction preprocess_function à utiliser maintenant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636f1d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On recrée nos datasets initiaux que l'on va traiter différemment : \n",
    "train_dataset = Dataset.from_pandas(tdf)\n",
    "validation_dataset = Dataset.from_pandas(vdf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "#[[Answer\n",
    "    \n",
    "#/Answer]]\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.features.keys(), load_from_cache_file=True)\n",
    "print(train_dataset)\n",
    "print(train_dataset[\"input_ids\"])\n",
    "validation_dataset = validation_dataset.map(preprocess_function, batched=True, remove_columns=validation_dataset.features.keys(), load_from_cache_file=True)\n",
    "print(validation_dataset)\n",
    "print(validation_dataset[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b04198",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx=tokenizer.pad_token_id  # On met à jour l'index de padding et on recrée les dataloaders\n",
    "\n",
    "train_sampler = LengthGroupedSampler(batchsize, train_dataset, megabatch_mul)\n",
    "\n",
    "train_loader=DataLoader(train_dataset,batchsize,sampler=train_sampler,collate_fn=data_collator,pin_memory=True, shuffle=False, num_workers=0)\n",
    "\n",
    "for data in train_loader:\n",
    "    print(data)\n",
    "    print(tokenizer.batch_decode(data[\"input_ids\"]))\n",
    "\n",
    "test_sampler = LengthGroupedSampler(batchsize, validation_dataset, megabatch_mul)    \n",
    "    \n",
    "test_loader=DataLoader(validation_dataset,batchsize,sampler=test_sampler,collate_fn=data_collator,pin_memory=True, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "for data in test_loader:\n",
    "    print(data)\n",
    "    print(tokenizer.batch_decode(data[\"input_ids\"]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71eca84",
   "metadata": {},
   "source": [
    "On note que truckmegatruck n'est plus complètement ignoré mais découpé en tokens (on retrouve par exemple le token truck d'id 4744 au début du mot). \n",
    "\n",
    "On a donc un nouvel encodage de notre corpus, mais nous ne possédons pas d'embeddings pour les tokens correspondants. On peut essayer d'en récupérer en chargeant un modèle transformer pre-entraîné : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e37a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "distil_bert = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "print(distil_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3329584a",
   "metadata": {},
   "source": [
    "On se propose d'extraire les embeddings de ce modèle pour les utiliser directement dans notre modèle simple MyModel défini plus haut. Donner la procédure pour charger ces embeddings et lancer l'entraînement du modèle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c2fac8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#[[Answer\n",
    "\n",
    "#/Answer]]\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(net.parameters(), lr=0.01,weight_decay=0.000)\n",
    "epochs = 50000\n",
    "\n",
    "train_test(train_loader, test_loader, net, loss_function, optimizer, epochs) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974783d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_from_pandas(vdf,net))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad2751e",
   "metadata": {},
   "source": [
    "Ok pas vraiment mieux qu'avec les embeddings de mots (et même plutôt moins bien!). Pourquoi d'après vous ? \n",
    "\n",
    "\\#[[Answer\n",
    "\n",
    "\n",
    "\n",
    "\\#/Answer]]\n",
    "\n",
    "# Adaptation d'un modèle pre-entraîné : DistillBert\n",
    "\n",
    "On va plutôt essayer de re-utiliser le modèle DistillBert complet, en ne fine-tunant que la dernière couche linéaire. Donner la procédure pour réaliser cela et lancer l'entraînement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474f4cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ANSWER[[\n",
    "\n",
    "#/ANSWER]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d45edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_from_pandas(vdf,net))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d84ced0",
   "metadata": {},
   "source": [
    "Ok... on sent qu'avec beaucoup plus de données on pourrait arriver à une très bonne qualité mais le dataset d'apprentissage est vraiment trop petit pour un modèle aussi gros.\n",
    "\n",
    "# Module Transformer\n",
    "\n",
    "Essayons maintenant de créer un petit réseau Transformer d'un seul layer d'encoder, qui reutilise les embeddings de DistilBert (seulement ceux des mots dans un premier temps), avec 8 têtes de self-attention. Attention, par défaut la taille des séquences est la première dimension des entrées attendues par le module TransformerEncoderLayer. Pour donner plutôt sous une forme (taille du batch, longueur des sequences, taille des embeddings), utiliser l'option batch_first=True. Le token CLS à utiliser pour faire la classification est le premier de chaque sequence (c'est sur lui qu'on branche la tête de classification, qui correspond à une simple couche linéaire, précédée d'une activation tanh) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fddfbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "        \n",
    "class TransfoModel(nn.Module):\n",
    "    #Answer[[\n",
    "    \n",
    "    #]]Answer\n",
    "    \n",
    "embeds=distil_bert.distilbert.embeddings.word_embeddings\n",
    "\n",
    "net = TransfoModel(vocab_size,embeds.weight.shape[-1],pad_idx,0.5,1)\n",
    "net.embedding=nn.Embedding.from_pretrained(embeds.weight)   # on charge les pre-train (en supposant que le module Embedding du modèle soit dans une variable embeddings)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(net.parameters(), lr=0.001,weight_decay=0.0)\n",
    "epochs = 50000\n",
    "\n",
    "train_test(train_loader, test_loader, net, loss_function, optimizer, epochs, clip=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f9dd63",
   "metadata": {},
   "source": [
    "Ca a l'air mieux ! Voyons les predictions sur nos exemples précédents : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901bc88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_from_pandas(vdf,net))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fab311",
   "metadata": {},
   "source": [
    "Ok, les fautes de frappes ont l'air mieux gérées. Le mot complexe également.. \n",
    "Par contre on a toujours la limite de l'ordre, avec les deux phrases \"a text about trucks, not animals\" et \"a text about animals, not trucks\" retournant exactemenent les mêmes prédictions. Normal, un transformer est de base invariant à l'ordre ! \n",
    "\n",
    "# Positionnal Embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0040ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "#from sentence_transformers import SentenceTransformer\n",
    "        \n",
    "class TransfoModel(nn.Module):\n",
    "    #Answer[[\n",
    "    \n",
    "    #]]Answer\n",
    "    \n",
    "embeds=distil_bert.distilbert.embeddings.word_embeddings\n",
    "\n",
    "net = TransfoModel(vocab_size,embeds.weight.shape[-1],pad_idx,0.2,1)\n",
    "net.embedding=nn.Embedding.from_pretrained(embeds.weight)   # on charge les pre-train (en supposant que le module Embedding du modèle soit dans une variable embeddings)\n",
    "net.pos_embedding=nn.Embedding.from_pretrained(distil_bert.distilbert.embeddings.position_embeddings.weight)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(net.parameters(), lr=0.001,weight_decay=0.0)\n",
    "epochs = 50000\n",
    "\n",
    "train_test(train_loader, test_loader, net, loss_function, optimizer, epochs,clip=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb642eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_from_pandas(vdf,net))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df43c9ee",
   "metadata": {},
   "source": [
    "Super ! Bon çà reste des resultats fragiles vue la taille du jeu de données, mais on semble quand même obtenir une capacité de traitement bien supérieure aux modèles précédents. On note en particulier la capacité à distinguer les phrases possédant des mots identiques mais dans un ordre différent. \n",
    "\n",
    "# Experimentations sur un Jeu de Données Réel : IMDB\n",
    "\n",
    "Pour terminer ce TP, on s'intéresse à l'adaptation du modèle DistillBert utilisé ci-dessus, sur un jeu de données  beaucoup plus conséquent: le corpus IMDB (base de données de commentaires sur des films). Pour cette partie, il est largement conseillé de travailler sur GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ddfaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "dataset = datasets.load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cb3ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d928beb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraction=0.2            \n",
    "train_dataset=dataset[\"train\"]\n",
    "train_dataset = train_dataset.train_test_split(test_size=1-train_fraction)[\"train\"]\n",
    "test_fraction=0.002\n",
    "validation_dataset=dataset[\"test\"]\n",
    "validation_dataset = validation_dataset.train_test_split(test_size=1-test_fraction)[\"train\"]\n",
    "\n",
    "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "#[[Answer\n",
    "    \n",
    "#/Answer]]\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.features.keys(), load_from_cache_file=True)\n",
    "print(train_dataset)\n",
    "\n",
    "validation_dataset = validation_dataset.map(preprocess_function, batched=True, remove_columns=validation_dataset.features.keys(), load_from_cache_file=True)\n",
    "print(validation_dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bc4e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx=tokenizer.pad_token_id  # On met à jour l'index de padding et on recrée les dataloaders\n",
    "\n",
    "batchsize=32\n",
    "megamul=4\n",
    "\n",
    "train_sampler = LengthGroupedSampler(batchsize, train_dataset, megabatch_mul)\n",
    "\n",
    "train_loader=DataLoader(train_dataset,batchsize,sampler=train_sampler,collate_fn=data_collator,pin_memory=True, shuffle=False, num_workers=0)\n",
    "\n",
    "for data in train_loader:\n",
    "    print(data)\n",
    "    print(tokenizer.batch_decode(data[\"input_ids\"]))\n",
    "    break\n",
    "\n",
    "test_sampler = LengthGroupedSampler(batchsize, validation_dataset, megabatch_mul)    \n",
    "    \n",
    "test_loader=DataLoader(validation_dataset,batchsize,sampler=test_sampler,collate_fn=data_collator,pin_memory=True, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "for data in test_loader:\n",
    "    print(data)\n",
    "    print(tokenizer.batch_decode(data[\"input_ids\"]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5052182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(trainloader, testloader, model, loss_function, optimizer, epochs, clip=-1, test_rate=1):\n",
    "  it=0\n",
    "\n",
    "  \n",
    "\n",
    "  for epoch in range(epochs):\n",
    "      \n",
    "      \n",
    "      epoch_loss = 0\n",
    "      epoch_accuracy = 0\n",
    "      \n",
    "     \n",
    "    \n",
    "      nb_samples=0\n",
    "      t = tqdm(iter(trainloader), total=len(trainloader), dynamic_ncols=True, position=0)\n",
    "      train_loss_log = tqdm(total=0, position=4, bar_format='{desc}')\n",
    "      test_log = tqdm(total=0, position=2, bar_format='{desc}')\n",
    "      accuracy_log = tqdm(total=0, position=3, bar_format='{desc}')\n",
    "      for batch in t:\n",
    "          it+=1\n",
    "          if it%test_rate==0:\n",
    "            test(testloader, model, loss_function, epoch,(test_log,accuracy_log))\n",
    "          model.train()\n",
    "          optimizer.zero_grad()\n",
    "          #print(\"text shape \",batch.text.T.shape)\n",
    "          prediction = model(batch[\"input_ids\"].to(model.device))\n",
    "          if not isinstance(prediction,torch.Tensor):  \n",
    "                prediction = prediction[\"logits\"]\n",
    "          #print(prediction)\n",
    "          loss = loss_function(prediction, batch[\"labels\"].to(model.device))\n",
    "          train_loss_log.set_description_str(\"loss train {:.3f} \".format(loss.item()))\n",
    "          loss.backward()\n",
    "          if clip>0:\n",
    "              torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "          optimizer.step()\n",
    "          nb_samples+=prediction.shape[0]\n",
    "          epoch_loss+=loss.item()*prediction.shape[0]\n",
    "          preds=(prediction[:,1]>prediction[:,0])*1.0\n",
    "          accuracy=(preds==batch[\"labels\"].to(model.device)).sum()\n",
    "          epoch_accuracy+=accuracy.item()\n",
    "      print('train loss on epoch {} : {:.3f}'.format(epoch, epoch_loss/nb_samples))\n",
    "      print('train accuracy on epoch {}: {:.3f}'.format(epoch, epoch_accuracy/nb_samples))\n",
    "      \n",
    "def test(testloader, model, loss_function, epoch, descs):\n",
    "      model.eval()\n",
    "      test_loss = 0\n",
    "      test_accuracy = 0\n",
    "      nb_samples=0\n",
    "      accuracy=0\n",
    "      t = tqdm(iter(testloader), total=len(testloader), position=1, leave=False)\n",
    "      tl,al=descs\n",
    "      #test_log = tqdm(total=0, position=2, bar_format='{desc}')\n",
    "      #accuracy_log = tqdm(total=0, position=3, bar_format='{desc}')\n",
    "      for batch in t:\n",
    "          #print(\"test \",batch)\n",
    "          with torch.no_grad():\n",
    "              optimizer.zero_grad()\n",
    "              inputs=batch[\"input_ids\"].to(model.device)\n",
    "              prediction = model(inputs)\n",
    "              if not isinstance(prediction,torch.Tensor):\n",
    "                    prediction = prediction[\"logits\"]\n",
    "              loss = loss_function(prediction, batch[\"labels\"].to(model.device))\n",
    "              nb_samples+=prediction.shape[0]\n",
    "              test_loss+=loss.item()*prediction.shape[0]\n",
    "              #print(batch_decode(batch[\"input_ids\"]))\n",
    "              preds=(prediction[:,1]>prediction[:,0])*1.0\n",
    "              accuracy=(preds==batch[\"labels\"].to(model.device)).sum()\n",
    "              test_accuracy+=accuracy.item()  \n",
    "              #print(preds,accuracy)\n",
    "      tl.set_description_str('test loss on epoch {}: {:.3f}'.format(epoch, test_loss/nb_samples))\n",
    "      al.set_description_str('test accuracy on epoch {}: {:.3f}'.format(epoch, test_accuracy/nb_samples))\n",
    "      #print('test loss on epoch {}: {:.3f}'.format(epoch, test_loss/nb_samples))\n",
    "      #print('test accuracy on epoch {}: {:.3f}'.format(epoch, test_accuracy/nb_samples))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7834920",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "distil_bert = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "net = distil_bert\n",
    "def set_parameter_requires_grad(module, requires_grad=True):\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = requires_grad\n",
    "\n",
    "\n",
    "net=net.to(device)\n",
    "\n",
    "# Fixons d'abord les poids du réseau :\n",
    "set_parameter_requires_grad(net,False)\n",
    "#set_parameter_requires_grad(net.distilbert.embeddings,False)\n",
    "set_parameter_requires_grad(net.classifier,True)\n",
    "\n",
    "params_to_update = []\n",
    "for name,param in net.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        \n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(params_to_update, lr=0.0001,weight_decay=0.000)\n",
    "epochs = 50000\n",
    "\n",
    "\n",
    "train_test(train_loader, test_loader, net, loss_function, optimizer, epochs, test_rate=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec048fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ebe51d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}