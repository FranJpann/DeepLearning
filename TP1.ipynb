{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TE2ItlsI956"
   },
   "source": [
    "# Deep Learning - Introduction à Pytorch \n",
    "\n",
    "\n",
    "## TP1 : Prise en main de Pytorch\n",
    "\n",
    "Sylvain Lamprier (sylvain.lamprier@univ-angers.fr)\n",
    "\n",
    "Supports adaptés de Nicolas Baskiotis (nicolas.baskiotis@sorbonne-univeriste.fr) et Benjamin Piwowarski (benjamin.piwowarski@sorbonne-universite.fr) -- MLIA/ISIR, Sorbonne Université"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAER7frwJu9L"
   },
   "source": [
    "\n",
    "\n",
    "Les lignes suivantes permettent d'importer pytorch et vérifier qu'un GPU est disponible. Il est recommandé d'utiliser un manager d'environnement python type conda pour l'ensemble des tps. Après la création de votre environnement (via  $\\texttt{conda create --name <nom_env>}$) et son activation (via $\\texttt{conda activate <nom_env>}$), installer pytorch selon la commande donnée sur le site de $\\href{https://pytorch.org/}{\\texttt{PyTorch}}$  (choisir la version en fonction de votre GPU et sa version de cuda).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3Y9YOOHHhJKY",
    "ExecuteTime": {
     "end_time": "2024-02-06T13:30:00.289434876Z",
     "start_time": "2024-02-06T13:29:58.895477201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La version de torch est :  2.2.0+cu121\n",
      "Le calcul GPU est disponible ?  False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"La version de torch est : \",torch.__version__)\n",
    "print(\"Le calcul GPU est disponible ? \", torch.cuda.is_available())\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2LqFo3wzwYP"
   },
   "source": [
    "### <span class=\"alert-success\"> Exercice : Syntaxe\n",
    "\n",
    "Le principal objet manipulé sous Pytorch est **torch.Tensor** qui correspond à un tenseur mathématique (généralisation de la notion de matrice en $n$-dimensions), très proche dans l'utilisation de **numpy.array**.   Cet objet est optimisé pour les calculs sur GPU ce qui implique quelques contraintes plus importantes que sous **numpy**. En particulier :\n",
    "* le type du tenseur manipulé est très important et les conversions ne sont pas automatique (**FloatTensor** de type **torch.float**, **DoubleTensor** de type **torch.double**,  **ByteTensor** de type **torch.byte**, **IntTensor** de type **torch.int**, **LongTensor** de type **torch.long**). Pour un tenseur **t** La conversion se fait très simplement en utilisant les fonctions : **t.double()**, **t.float()**, **t.long()** ...\n",
    "* la plupart des opérations ont une version *inplace*, c'est-à-dire qui modifie le tenseur plutôt que de renvoyer un nouveau tenseur; elles sont suffixées par **_** (**add_** par exemple).\n",
    "\n",
    "Donner des exemples d'instructions correspondant aux commentaires ci-dessous. N'hésitez pas à vous référez à la [documentation officielle](https://pytorch.org/docs/stable/tensors.html) pour la liste exhaustive des opérations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "code",
    "id": "VZxNfy1b1u43",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-02-06T13:30:00.290471923Z",
     "start_time": "2024-02-06T13:30:00.255244332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [2., 3., 4.]])\n",
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[14., 11., 14.],\n",
      "        [13., 10., 11.]])\n",
      "tensor([[1.0948, 1.0705, 0.9232],\n",
      "        [1.0208, 0.9961, 0.7396]])\n",
      "tensor([[1.0948, 1.0705, 0.9232],\n",
      "        [1.0208, 0.9961, 0.7396],\n",
      "        [1.0948, 1.0705, 0.9232],\n",
      "        [1.0208, 0.9961, 0.7396]])\n",
      "tensor([[1.0948, 1.0705, 0.9232, 1.0948, 1.0705, 0.9232],\n",
      "        [1.0208, 0.9961, 0.7396, 1.0208, 0.9961, 0.7396]])\n",
      "3 torch.Size([3, 4]) torch.Size([3])\n",
      "tensor([[1, 1, 0],\n",
      "        [1, 0, 0]], dtype=torch.int32) torch.IntTensor\n",
      "tensor(4.5955)\n",
      "tensor([[-1.8950,  0.7764,  0.5528,  0.0550],\n",
      "        [-1.8070,  0.6563,  0.7743,  0.1300]]) tensor([[-1.8950,  0.7764,  0.5528,  0.0550],\n",
      "        [-1.8070,  0.6563,  0.7743,  0.1300]])\n",
      "tensor([[1.0948, 1.0208],\n",
      "        [1.0705, 0.9961],\n",
      "        [0.9232, 0.7396]]) tensor([[1.0948, 1.0208],\n",
      "        [1.0705, 0.9961],\n",
      "        [0.9232, 0.7396]])\n",
      "argmax :  tensor([0, 0])\n",
      "tensor([ 1.2113, -0.0820, -1.8946]) tensor(-0.7653)\n",
      "tensor([ 0.3028, -0.0205, -0.4736]) tensor(-0.0638)\n",
      "tensor([[-9.3176e-01,  2.4099e-01,  1.0499e+00,  8.5216e-01, -1.1182e+00,\n",
      "         -1.5765e-03],\n",
      "        [ 1.3019e+00, -2.6410e-01,  3.4891e-01,  5.5696e-01, -2.1557e+00,\n",
      "         -6.4474e-01]])\n",
      "tensor([[2.1896, 2.1410, 1.8465],\n",
      "        [2.0416, 1.9922, 1.4791]]) tensor([[1.1986, 1.1460, 0.8524],\n",
      "        [1.0421, 0.9922, 0.5470]]) tensor([[1.1986, 1.1460, 0.8524],\n",
      "        [1.0421, 0.9922, 0.5470]])\n",
      "avant expand  tensor([[[0.2570, 0.0213, 0.5213, 0.2214],\n",
      "         [0.2680, 0.3642, 0.1053, 0.2766],\n",
      "         [0.8573, 0.4540, 0.7981, 0.1140]],\n",
      "\n",
      "        [[0.4262, 0.7778, 0.5594, 0.6745],\n",
      "         [0.6808, 0.2329, 0.8714, 0.1004],\n",
      "         [0.3964, 0.1178, 0.9524, 0.4774]]])\n",
      "après expand  tensor([[[0.2570, 0.0213, 0.5213, 0.2214],\n",
      "         [0.2680, 0.3642, 0.1053, 0.2766],\n",
      "         [0.8573, 0.4540, 0.7981, 0.1140]],\n",
      "\n",
      "        [[0.2570, 0.0213, 0.5213, 0.2214],\n",
      "         [0.2680, 0.3642, 0.1053, 0.2766],\n",
      "         [0.8573, 0.4540, 0.7981, 0.1140]],\n",
      "\n",
      "        [[0.2570, 0.0213, 0.5213, 0.2214],\n",
      "         [0.2680, 0.3642, 0.1053, 0.2766],\n",
      "         [0.8573, 0.4540, 0.7981, 0.1140]],\n",
      "\n",
      "        [[0.4262, 0.7778, 0.5594, 0.6745],\n",
      "         [0.6808, 0.2329, 0.8714, 0.1004],\n",
      "         [0.3964, 0.1178, 0.9524, 0.4774]],\n",
      "\n",
      "        [[0.4262, 0.7778, 0.5594, 0.6745],\n",
      "         [0.6808, 0.2329, 0.8714, 0.1004],\n",
      "         [0.3964, 0.1178, 0.9524, 0.4774]],\n",
      "\n",
      "        [[0.4262, 0.7778, 0.5594, 0.6745],\n",
      "         [0.6808, 0.2329, 0.8714, 0.1004],\n",
      "         [0.3964, 0.1178, 0.9524, 0.4774]]])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]) tensor([0., 0., 0., 0., 0.])\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]]) tensor([[0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Création de tenseurs et caractéristiques\n",
    "## Créer un tenseur (2,3) à partir d'une liste\n",
    "print(torch.tensor([[1.,2.,3.],[2.,3,4.]])) \n",
    "## Créer un tenseur  tenseur rempli de 1 de taille 2x3x4\n",
    "print(torch.ones(2,3,4)) \n",
    "## tenseur de zéros de taille 2x3 de type float\n",
    "print(torch.zeros(2,3,dtype=torch.float))  \n",
    "## tirage uniforme entier entre 10 et 15, \n",
    "## remarquez l'utilisation du _ dans random pour l'opération inplace\n",
    "print(torch.zeros(2,3).random_(10,15)) \n",
    "## tirage suivant la loi normale\n",
    "a=torch.zeros(2,3).normal_(1,0.1)\n",
    "print(a)\n",
    "## equivalent à zeros(3,4).normal_\n",
    "b = torch.randn(3,4) \n",
    "## Création d'un vecteur de 3 flottants selon la loi de normale\n",
    "c = torch.randn(3)\n",
    "## concatenation de tenseurs sur la dimension 0\n",
    "print(torch.cat((a,a),0))\n",
    "## concatenation de tenseurs  sur la dimension 1\n",
    "print(torch.cat((a,a),1))\n",
    "## Taille des tenseurs/vecteurs\n",
    "print(a.size(1),b.shape,c.size())\n",
    "## Conversion de type\n",
    "print(a.int(),a.int().type())\n",
    "\n",
    "# Opérations élémentaires sur les tenseurs\n",
    "## produit scalaire (et contrairement à numpy, que produit scalaire)\n",
    "print(c.dot(c))\n",
    "## produit matriciel : utilisation de @ ou de la fonction mm\n",
    "print(a.mm(b), a @ b)\n",
    "## transposé\n",
    "print(a.t(),a.T)\n",
    "## index du maximum selon une dimension\n",
    "print(\"argmax : \",a.argmax(dim=1))\n",
    "## somme selon une dimension/de tous les éléments\n",
    "print(b.sum(1), b.sum()) \n",
    "## moyenne selon  une dimension/sur tous les éléments\n",
    "print(b.mean(1), b.mean())\n",
    "## changer les dimensions du tenseur (la taille totale doit être inchangée)\n",
    "print(b.view(2,6))\n",
    "## somme/produit/puissance termes a termes\n",
    "print(a+a,a*a,a**2)\n",
    "\n",
    "## Soit un tenseur a de (2,3,4). Le recopier dans une version (2,3,3,4) avec les tenseurs (3,4) \n",
    "## a[0] et a[1] recopiés chacun 3 fois (avec expand)\n",
    "a=torch.rand((2,3,4))\n",
    "print(\"avant expand \",a)\n",
    "a=a.view(2,1,3,-1).expand(-1,3,-1,-1).contiguous().view(-1,3,4)\n",
    "print(\"après expand \",a)\n",
    "\n",
    "\n",
    "## attention ! comme sous numpy, il peut y avoir des pièges ! \n",
    "## Vérifier toujours les dimensions !!\n",
    "a=torch.zeros(5,1)\n",
    "b = torch.zeros(5)\n",
    "print(a,b)\n",
    "## la première opération fait un broadcast et le résultat est 1 tenseur à 2 dimensions,\n",
    "## le résultat de la deuxième opération est une matrice contenant un vecteur unique\n",
    "print(a-b,a.t()-b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span class=\"alert-success\"> Exercice :   Régression linéaire  </span>\n",
    "\n",
    "On souhaite apprendre un modèle de régression linéaire $f$ du type:  $f(x,w,b)=x.w^t+b$  avec $x\\in \\mathbb{R}^{{d}}$ un vecteur d'observations pour lequel on souhaite prédire une sortie $\\hat{y} \\in \\mathbb{R}$, $w\\in\\mathbb{R}^{1,d}$ et $b\\in \\mathbb{R}$ les paramètres du modèle. \n",
    "\n",
    "Pour cela on dispose d'un jeu de données étiquetées $\\{(x,y)\\}$, que l'on découpe en jeu d'entraînement (80%) et de validation (20%). Dans cet exercice, on utilisera le jeu de données très classique *Boston*, le prix des loyers à Boston en fonction de caractéristiques socio-économiques des quartiers.\n",
    "\n",
    "On considèrera un coût moindres carrés pour apprendre le modèle sur le jeu d'entraînement (avec $N$ le nombre de données d'entraînement et $(x^i,y^i)$ le i-ème couple de cet ensemble): $$w^∗,b^∗=argmin_{w,b}\\frac{1}{N} \\sum_{i=1}^N \\|f(x^i,w,b)-y^i\\|^2$$\n",
    "\n",
    "\n",
    "* Définir (en version tensorielle PyTorch) la fonction **flineaire(x,w,b)** qui calcule $f(x,w,b)=x.w^t+b$  avec $x\\in \\mathbb{R}^{{n\\times d}},~w\\in\\mathbb{R}^{1,d}, b\\in \\mathbb{R}$\n",
    "* Donner le code d'une fonction **loss(x,w,b,y)** qui retourne le coût moindre carré du modèle linéaire utilisant **flineaire(x,w,b)** pour un batch de données $x$ et leurs images associées $y$. \n",
    "* Calculer le gradient de l'erreur en fonction de chacun des paramètres $w_i$ et b. Donner le code d'une fonction **getGradient(x,w,b)** (commencer éventuellement avec des boucles avant de réaliser la version matricielle plus efficace). \n",
    "* Complétez le code ci-dessous pour réaliser une descente de gradient et apprendre les paramètres optimaux de la regression linéaire.\n",
    "* Modifier le code ci-dessous pour n'entraîner que sur 80% des données et se tester sur 20%\n",
    "\n",
    "\n",
    "\n",
    "Attention ! \n",
    "* l'algorithme doit converger avec la valeur de epsilon fixée; si ce n'est pas le cas, il y a une erreur (la plupart du temps au niveau du calcul du coût)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T13:30:00.336769947Z",
     "start_time": "2024-02-06T13:30:00.329992186Z"
    }
   },
   "outputs": [],
   "source": [
    "def flineaire(x,w,b):\n",
    "    ## [[student]]\n",
    "    return (x @ w.T)+b\n",
    "    ## [[/student]]\n",
    "    \n",
    "\n",
    "def getLoss(x,w,b,y):\n",
    "    ## [[student]]\n",
    "    y=y.view(-1,1)\n",
    "    return torch.pow(flineaire(x,w,b)-y,2).mean()/2.0\n",
    "    ## [[/student]]\n",
    "    \n",
    "\n",
    "def getGradient(x,w,b,y):\n",
    "    ## [[student]]\n",
    "    y=y.view(-1,1)\n",
    "    yhat=flineaire(x,w,b)\n",
    "    diff=yhat-y\n",
    "    # version boucle\n",
    "#     g=torch.zeros_like(w)\n",
    "#     for i in range(w.size(-1)):\n",
    "#         g[0,i]=(x[:,i].view(-1,1)*diff.mean()\n",
    "#     return g,diff.mean()\n",
    "    # version matricielle\n",
    "    return ((x.t()@diff)/x.shape[0]).t(),(diff.mean())\n",
    "    ## [[/student]]\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T13:30:18.932843916Z",
     "start_time": "2024-02-06T13:30:02.326578278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'exemples :  20640 Dimension :  8\n",
      "iteration : 0, loss train : 405407.8125, loss test : 390859.21875\n",
      "iteration : 100, loss train : 1346.581787109375, loss test : 1243.407958984375\n",
      "iteration : 200, loss train : 1182.19873046875, loss test : 1091.239013671875\n",
      "iteration : 300, loss train : 1038.134765625, loss test : 957.91015625\n",
      "iteration : 400, loss train : 911.8776245117188, loss test : 841.090087890625\n",
      "iteration : 500, loss train : 801.2258911132812, loss test : 738.7359008789062\n",
      "iteration : 600, loss train : 704.2501220703125, loss test : 649.0574340820312\n",
      "iteration : 700, loss train : 619.2598876953125, loss test : 570.4859008789062\n",
      "iteration : 800, loss train : 544.772216796875, loss test : 501.645751953125\n",
      "iteration : 900, loss train : 479.4895324707031, loss test : 441.3331604003906\n",
      "iteration : 1000, loss train : 422.2734375, loss test : 388.49212646484375\n",
      "iteration : 1100, loss train : 372.12652587890625, loss test : 342.1974792480469\n",
      "iteration : 1200, loss train : 328.1751708984375, loss test : 301.6393127441406\n",
      "iteration : 1300, loss train : 289.6532897949219, loss test : 266.1069030761719\n",
      "iteration : 1400, loss train : 255.8896484375, loss test : 234.97801208496094\n",
      "iteration : 1500, loss train : 226.29605102539062, loss test : 207.707275390625\n",
      "iteration : 1600, loss train : 200.35633850097656, loss test : 183.81629943847656\n",
      "iteration : 1700, loss train : 177.61924743652344, loss test : 162.8868408203125\n",
      "iteration : 1800, loss train : 157.68893432617188, loss test : 144.552001953125\n",
      "iteration : 1900, loss train : 140.21824645996094, loss test : 128.49017333984375\n",
      "iteration : 2000, loss train : 124.90303039550781, loss test : 114.41956329345703\n",
      "iteration : 2100, loss train : 111.47677612304688, loss test : 102.0933609008789\n",
      "iteration : 2200, loss train : 99.70608520507812, loss test : 91.2954330444336\n",
      "iteration : 2300, loss train : 89.38612365722656, loss test : 81.83612060546875\n",
      "iteration : 2400, loss train : 80.33768463134766, loss test : 73.54951477050781\n",
      "iteration : 2500, loss train : 72.40340423583984, loss test : 66.2900161743164\n",
      "iteration : 2600, loss train : 65.44572448730469, loss test : 59.930362701416016\n",
      "iteration : 2700, loss train : 59.34382629394531, loss test : 54.35879135131836\n",
      "iteration : 2800, loss train : 53.991966247558594, loss test : 49.477516174316406\n",
      "iteration : 2900, loss train : 49.297393798828125, loss test : 45.2008171081543\n",
      "iteration : 3000, loss train : 45.17878723144531, loss test : 41.45354080200195\n",
      "iteration : 3100, loss train : 41.565025329589844, loss test : 38.16998291015625\n",
      "iteration : 3200, loss train : 38.39366149902344, loss test : 35.29247283935547\n",
      "iteration : 3300, loss train : 35.610050201416016, loss test : 32.77058029174805\n",
      "iteration : 3400, loss train : 33.16627883911133, loss test : 30.56009292602539\n",
      "iteration : 3500, loss train : 31.020265579223633, loss test : 28.622201919555664\n",
      "iteration : 3600, loss train : 29.1352481842041, loss test : 26.923015594482422\n",
      "iteration : 3700, loss train : 27.478988647460938, loss test : 25.432844161987305\n",
      "iteration : 3800, loss train : 26.023164749145508, loss test : 24.125593185424805\n",
      "iteration : 3900, loss train : 24.74304962158203, loss test : 22.978511810302734\n",
      "iteration : 4000, loss train : 23.616941452026367, loss test : 21.971641540527344\n",
      "iteration : 4100, loss train : 22.62578010559082, loss test : 21.087459564208984\n",
      "iteration : 4200, loss train : 21.752859115600586, loss test : 20.310630798339844\n",
      "iteration : 4300, loss train : 20.983657836914062, loss test : 19.62782096862793\n",
      "iteration : 4400, loss train : 20.3053035736084, loss test : 19.027225494384766\n",
      "iteration : 4500, loss train : 19.70662498474121, loss test : 18.49860954284668\n",
      "iteration : 4600, loss train : 19.177703857421875, loss test : 18.032899856567383\n",
      "iteration : 4700, loss train : 18.71001434326172, loss test : 17.62229347229004\n",
      "iteration : 4800, loss train : 18.29593276977539, loss test : 17.259828567504883\n",
      "iteration : 4900, loss train : 17.928850173950195, loss test : 16.93947410583496\n",
      "iteration : 5000, loss train : 17.602998733520508, loss test : 16.655969619750977\n",
      "iteration : 5100, loss train : 17.313220977783203, loss test : 16.404626846313477\n",
      "iteration : 5200, loss train : 17.05517578125, loss test : 16.18149757385254\n",
      "iteration : 5300, loss train : 16.824888229370117, loss test : 15.98297119140625\n",
      "iteration : 5400, loss train : 16.61892318725586, loss test : 15.805937767028809\n",
      "iteration : 5500, loss train : 16.43431854248047, loss test : 15.647712707519531\n",
      "iteration : 5600, loss train : 16.268421173095703, loss test : 15.505903244018555\n",
      "iteration : 5700, loss train : 16.11892318725586, loss test : 15.378433227539062\n",
      "iteration : 5800, loss train : 15.983858108520508, loss test : 15.263522148132324\n",
      "iteration : 5900, loss train : 15.861376762390137, loss test : 15.15952205657959\n",
      "iteration : 6000, loss train : 15.749998092651367, loss test : 15.06510066986084\n",
      "iteration : 6100, loss train : 15.648344993591309, loss test : 14.979024887084961\n",
      "iteration : 6200, loss train : 15.555203437805176, loss test : 14.900218963623047\n",
      "iteration : 6300, loss train : 15.46953010559082, loss test : 14.827754974365234\n",
      "iteration : 6400, loss train : 15.390434265136719, loss test : 14.76084041595459\n",
      "iteration : 6500, loss train : 15.31712818145752, loss test : 14.698780059814453\n",
      "iteration : 6600, loss train : 15.248873710632324, loss test : 14.640926361083984\n",
      "iteration : 6700, loss train : 15.18508243560791, loss test : 14.586758613586426\n",
      "iteration : 6800, loss train : 15.125173568725586, loss test : 14.535782814025879\n",
      "iteration : 6900, loss train : 15.068689346313477, loss test : 14.487585067749023\n",
      "iteration : 7000, loss train : 15.015247344970703, loss test : 14.441844940185547\n",
      "iteration : 7100, loss train : 14.96448802947998, loss test : 14.398244857788086\n",
      "iteration : 7200, loss train : 14.916014671325684, loss test : 14.356450080871582\n",
      "iteration : 7300, loss train : 14.869648933410645, loss test : 14.316309928894043\n",
      "iteration : 7400, loss train : 14.825082778930664, loss test : 14.277558326721191\n",
      "iteration : 7500, loss train : 14.782096862792969, loss test : 14.240020751953125\n",
      "iteration : 7600, loss train : 14.740556716918945, loss test : 14.203570365905762\n",
      "iteration : 7700, loss train : 14.700255393981934, loss test : 14.168044090270996\n",
      "iteration : 7800, loss train : 14.661029815673828, loss test : 14.13331413269043\n",
      "iteration : 7900, loss train : 14.622855186462402, loss test : 14.09933853149414\n",
      "iteration : 8000, loss train : 14.585471153259277, loss test : 14.065927505493164\n",
      "iteration : 8100, loss train : 14.54889965057373, loss test : 14.033089637756348\n",
      "iteration : 8200, loss train : 14.513022422790527, loss test : 14.00073528289795\n",
      "iteration : 8300, loss train : 14.47772216796875, loss test : 13.968771934509277\n",
      "iteration : 8400, loss train : 14.442977905273438, loss test : 13.937182426452637\n",
      "iteration : 8500, loss train : 14.408743858337402, loss test : 13.905924797058105\n",
      "iteration : 8600, loss train : 14.374923706054688, loss test : 13.87493896484375\n",
      "iteration : 8700, loss train : 14.341504096984863, loss test : 13.844218254089355\n",
      "iteration : 8800, loss train : 14.308396339416504, loss test : 13.813668251037598\n",
      "iteration : 8900, loss train : 14.275712966918945, loss test : 13.78341293334961\n",
      "iteration : 9000, loss train : 14.243246078491211, loss test : 13.753266334533691\n",
      "iteration : 9100, loss train : 14.211085319519043, loss test : 13.723332405090332\n",
      "iteration : 9200, loss train : 14.179075241088867, loss test : 13.69344425201416\n",
      "iteration : 9300, loss train : 14.14731502532959, loss test : 13.663727760314941\n",
      "iteration : 9400, loss train : 14.115864753723145, loss test : 13.6342134475708\n",
      "iteration : 9500, loss train : 14.084516525268555, loss test : 13.604718208312988\n",
      "iteration : 9600, loss train : 14.053360939025879, loss test : 13.575353622436523\n",
      "iteration : 9700, loss train : 14.022400856018066, loss test : 13.546114921569824\n",
      "iteration : 9800, loss train : 13.991606712341309, loss test : 13.516988754272461\n",
      "iteration : 9900, loss train : 13.960857391357422, loss test : 13.48784065246582\n",
      "iteration : 10000, loss train : 13.930216789245605, loss test : 13.458755493164062\n",
      "iteration : 10100, loss train : 13.899709701538086, loss test : 13.429758071899414\n",
      "iteration : 10200, loss train : 13.869367599487305, loss test : 13.40086841583252\n",
      "iteration : 10300, loss train : 13.839207649230957, loss test : 13.372103691101074\n",
      "iteration : 10400, loss train : 13.809176445007324, loss test : 13.343420028686523\n",
      "iteration : 10500, loss train : 13.779245376586914, loss test : 13.314801216125488\n",
      "iteration : 10600, loss train : 13.749411582946777, loss test : 13.286249160766602\n",
      "iteration : 10700, loss train : 13.71966552734375, loss test : 13.257755279541016\n",
      "iteration : 10800, loss train : 13.6900053024292, loss test : 13.229324340820312\n",
      "iteration : 10900, loss train : 13.660435676574707, loss test : 13.200950622558594\n",
      "iteration : 11000, loss train : 13.630942344665527, loss test : 13.17263412475586\n",
      "iteration : 11100, loss train : 13.601531982421875, loss test : 13.144369125366211\n",
      "iteration : 11200, loss train : 13.572211265563965, loss test : 13.116165161132812\n",
      "iteration : 11300, loss train : 13.542959213256836, loss test : 13.088008880615234\n",
      "iteration : 11400, loss train : 13.513765335083008, loss test : 13.059901237487793\n",
      "iteration : 11500, loss train : 13.484655380249023, loss test : 13.031844139099121\n",
      "iteration : 11600, loss train : 13.455591201782227, loss test : 13.003829956054688\n",
      "iteration : 11700, loss train : 13.426605224609375, loss test : 12.97586441040039\n",
      "iteration : 11800, loss train : 13.39774227142334, loss test : 12.94802474975586\n",
      "iteration : 11900, loss train : 13.369029998779297, loss test : 12.920308113098145\n",
      "iteration : 12000, loss train : 13.340373039245605, loss test : 12.892637252807617\n",
      "iteration : 12100, loss train : 13.311848640441895, loss test : 12.86505126953125\n",
      "iteration : 12200, loss train : 13.283400535583496, loss test : 12.837522506713867\n",
      "iteration : 12300, loss train : 13.25499153137207, loss test : 12.81003189086914\n",
      "iteration : 12400, loss train : 13.226639747619629, loss test : 12.78258228302002\n",
      "iteration : 12500, loss train : 13.19837760925293, loss test : 12.75521183013916\n",
      "iteration : 12600, loss train : 13.170268058776855, loss test : 12.727997779846191\n",
      "iteration : 12700, loss train : 13.142192840576172, loss test : 12.70081901550293\n",
      "iteration : 12800, loss train : 13.114190101623535, loss test : 12.673686027526855\n",
      "iteration : 12900, loss train : 13.086248397827148, loss test : 12.646597862243652\n",
      "iteration : 13000, loss train : 13.058342933654785, loss test : 12.61954402923584\n",
      "iteration : 13100, loss train : 13.030513763427734, loss test : 12.592570304870605\n",
      "iteration : 13200, loss train : 13.002833366394043, loss test : 12.56574535369873\n",
      "iteration : 13300, loss train : 12.9752197265625, loss test : 12.538962364196777\n",
      "iteration : 13400, loss train : 12.947643280029297, loss test : 12.51221752166748\n",
      "iteration : 13500, loss train : 12.920101165771484, loss test : 12.485506057739258\n",
      "iteration : 13600, loss train : 12.892598152160645, loss test : 12.458830833435059\n",
      "iteration : 13700, loss train : 12.865236282348633, loss test : 12.432304382324219\n",
      "iteration : 13800, loss train : 12.837985038757324, loss test : 12.405862808227539\n",
      "iteration : 13900, loss train : 12.810790061950684, loss test : 12.379465103149414\n",
      "iteration : 14000, loss train : 12.783629417419434, loss test : 12.353100776672363\n",
      "iteration : 14100, loss train : 12.75650405883789, loss test : 12.326769828796387\n",
      "iteration : 14200, loss train : 12.729486465454102, loss test : 12.300552368164062\n",
      "iteration : 14300, loss train : 12.70257568359375, loss test : 12.274444580078125\n",
      "iteration : 14400, loss train : 12.675702095031738, loss test : 12.248370170593262\n",
      "iteration : 14500, loss train : 12.648869514465332, loss test : 12.222332954406738\n",
      "iteration : 14600, loss train : 12.622098922729492, loss test : 12.196335792541504\n",
      "iteration : 14700, loss train : 12.595416069030762, loss test : 12.170426368713379\n",
      "iteration : 14800, loss train : 12.568880081176758, loss test : 12.144658088684082\n",
      "iteration : 14900, loss train : 12.542377471923828, loss test : 12.118926048278809\n",
      "iteration : 15000, loss train : 12.515911102294922, loss test : 12.093225479125977\n",
      "iteration : 15100, loss train : 12.489476203918457, loss test : 12.067559242248535\n",
      "iteration : 15200, loss train : 12.463122367858887, loss test : 12.041973114013672\n",
      "iteration : 15300, loss train : 12.436901092529297, loss test : 12.01652717590332\n",
      "iteration : 15400, loss train : 12.410712242126465, loss test : 11.991113662719727\n",
      "iteration : 15500, loss train : 12.384587287902832, loss test : 11.965738296508789\n",
      "iteration : 15600, loss train : 12.358501434326172, loss test : 11.940399169921875\n",
      "iteration : 15700, loss train : 12.332500457763672, loss test : 11.915141105651855\n",
      "iteration : 15800, loss train : 12.306646347045898, loss test : 11.890029907226562\n",
      "iteration : 15900, loss train : 12.28082275390625, loss test : 11.864947319030762\n",
      "iteration : 16000, loss train : 12.255034446716309, loss test : 11.839900970458984\n",
      "iteration : 16100, loss train : 12.229278564453125, loss test : 11.814885139465332\n",
      "iteration : 16200, loss train : 12.203611373901367, loss test : 11.789961814880371\n",
      "iteration : 16300, loss train : 12.17806339263916, loss test : 11.765161514282227\n",
      "iteration : 16400, loss train : 12.152548789978027, loss test : 11.74039363861084\n",
      "iteration : 16500, loss train : 12.127069473266602, loss test : 11.715657234191895\n",
      "iteration : 16600, loss train : 12.101621627807617, loss test : 11.690957069396973\n",
      "iteration : 16700, loss train : 12.076281547546387, loss test : 11.66634464263916\n",
      "iteration : 16800, loss train : 12.0510892868042, loss test : 11.641870498657227\n",
      "iteration : 16900, loss train : 12.025936126708984, loss test : 11.617429733276367\n",
      "iteration : 17000, loss train : 12.000810623168945, loss test : 11.59302043914795\n",
      "iteration : 17100, loss train : 11.97572135925293, loss test : 11.568645477294922\n",
      "iteration : 17200, loss train : 11.950736999511719, loss test : 11.544379234313965\n",
      "iteration : 17300, loss train : 11.925848960876465, loss test : 11.520212173461914\n",
      "iteration : 17400, loss train : 11.900996208190918, loss test : 11.496079444885254\n",
      "iteration : 17500, loss train : 11.876171112060547, loss test : 11.471977233886719\n",
      "iteration : 17600, loss train : 11.851384162902832, loss test : 11.447908401489258\n",
      "iteration : 17700, loss train : 11.826702117919922, loss test : 11.4239501953125\n",
      "iteration : 17800, loss train : 11.802133560180664, loss test : 11.400096893310547\n",
      "iteration : 17900, loss train : 11.77759838104248, loss test : 11.376276969909668\n",
      "iteration : 18000, loss train : 11.753094673156738, loss test : 11.35248851776123\n",
      "iteration : 18100, loss train : 11.728622436523438, loss test : 11.328729629516602\n",
      "iteration : 18200, loss train : 11.7042818069458, loss test : 11.305083274841309\n",
      "iteration : 18300, loss train : 11.680041313171387, loss test : 11.281538963317871\n",
      "iteration : 18400, loss train : 11.655832290649414, loss test : 11.258026123046875\n",
      "iteration : 18500, loss train : 11.631656646728516, loss test : 11.23454475402832\n",
      "iteration : 18600, loss train : 11.607511520385742, loss test : 11.211092948913574\n",
      "iteration : 18700, loss train : 11.583484649658203, loss test : 11.187766075134277\n",
      "iteration : 18800, loss train : 11.559547424316406, loss test : 11.164525032043457\n",
      "iteration : 18900, loss train : 11.535651206970215, loss test : 11.141318321228027\n",
      "iteration : 19000, loss train : 11.511787414550781, loss test : 11.118144989013672\n",
      "iteration : 19100, loss train : 11.487956047058105, loss test : 11.095001220703125\n",
      "iteration : 19200, loss train : 11.464241981506348, loss test : 11.071979522705078\n",
      "iteration : 19300, loss train : 11.440605163574219, loss test : 11.049038887023926\n",
      "iteration : 19400, loss train : 11.416999816894531, loss test : 11.026130676269531\n",
      "iteration : 19500, loss train : 11.393427848815918, loss test : 11.003250122070312\n",
      "iteration : 19600, loss train : 11.369890213012695, loss test : 10.9804048538208\n",
      "iteration : 19700, loss train : 11.346453666687012, loss test : 10.957661628723145\n",
      "iteration : 19800, loss train : 11.323123931884766, loss test : 10.935018539428711\n",
      "iteration : 19900, loss train : 11.299846649169922, loss test : 10.912412643432617\n",
      "iteration : 20000, loss train : 11.276599884033203, loss test : 10.889836311340332\n",
      "iteration : 20100, loss train : 11.253387451171875, loss test : 10.867290496826172\n",
      "iteration : 20200, loss train : 11.230280876159668, loss test : 10.84485149383545\n",
      "iteration : 20300, loss train : 11.207265853881836, loss test : 10.822508811950684\n",
      "iteration : 20400, loss train : 11.184281349182129, loss test : 10.800196647644043\n",
      "iteration : 20500, loss train : 11.16132926940918, loss test : 10.777911186218262\n",
      "iteration : 20600, loss train : 11.138404846191406, loss test : 10.755659103393555\n",
      "iteration : 20700, loss train : 11.115588188171387, loss test : 10.73350715637207\n",
      "iteration : 20800, loss train : 11.092864990234375, loss test : 10.711457252502441\n",
      "iteration : 20900, loss train : 11.070172309875488, loss test : 10.689434051513672\n",
      "iteration : 21000, loss train : 11.047510147094727, loss test : 10.667439460754395\n",
      "iteration : 21100, loss train : 11.02487564086914, loss test : 10.645476341247559\n",
      "iteration : 21200, loss train : 11.00232219696045, loss test : 10.623591423034668\n",
      "iteration : 21300, loss train : 10.979887962341309, loss test : 10.601826667785645\n",
      "iteration : 21400, loss train : 10.95748519897461, loss test : 10.580094337463379\n",
      "iteration : 21500, loss train : 10.935111045837402, loss test : 10.558388710021973\n",
      "iteration : 21600, loss train : 10.912774085998535, loss test : 10.536713600158691\n",
      "iteration : 21700, loss train : 10.8905029296875, loss test : 10.515087127685547\n",
      "iteration : 21800, loss train : 10.868385314941406, loss test : 10.493616104125977\n",
      "iteration : 21900, loss train : 10.846296310424805, loss test : 10.472173690795898\n",
      "iteration : 22000, loss train : 10.824235916137695, loss test : 10.450758934020996\n",
      "iteration : 22100, loss train : 10.802204132080078, loss test : 10.429372787475586\n",
      "iteration : 22200, loss train : 10.7802095413208, loss test : 10.4080228805542\n",
      "iteration : 22300, loss train : 10.758371353149414, loss test : 10.386834144592285\n",
      "iteration : 22400, loss train : 10.736565589904785, loss test : 10.365674018859863\n",
      "iteration : 22500, loss train : 10.714786529541016, loss test : 10.3445405960083\n",
      "iteration : 22600, loss train : 10.693037033081055, loss test : 10.32343578338623\n",
      "iteration : 22700, loss train : 10.671316146850586, loss test : 10.302359580993652\n",
      "iteration : 22800, loss train : 10.649734497070312, loss test : 10.281426429748535\n",
      "iteration : 22900, loss train : 10.62820816040039, loss test : 10.26054573059082\n",
      "iteration : 23000, loss train : 10.606708526611328, loss test : 10.239690780639648\n",
      "iteration : 23100, loss train : 10.585238456726074, loss test : 10.218867301940918\n",
      "iteration : 23200, loss train : 10.563794136047363, loss test : 10.198066711425781\n",
      "iteration : 23300, loss train : 10.542438507080078, loss test : 10.177359580993652\n",
      "iteration : 23400, loss train : 10.521188735961914, loss test : 10.156754493713379\n",
      "iteration : 23500, loss train : 10.499984741210938, loss test : 10.136180877685547\n",
      "iteration : 23600, loss train : 10.478819847106934, loss test : 10.115638732910156\n",
      "iteration : 23700, loss train : 10.457682609558105, loss test : 10.095120429992676\n",
      "iteration : 23800, loss train : 10.436600685119629, loss test : 10.074663162231445\n",
      "iteration : 23900, loss train : 10.4156494140625, loss test : 10.054337501525879\n",
      "iteration : 24000, loss train : 10.394730567932129, loss test : 10.034040451049805\n",
      "iteration : 24100, loss train : 10.373839378356934, loss test : 10.01377010345459\n",
      "iteration : 24200, loss train : 10.352972984313965, loss test : 9.993526458740234\n",
      "iteration : 24300, loss train : 10.332137107849121, loss test : 9.973310470581055\n",
      "iteration : 24400, loss train : 10.311450004577637, loss test : 9.953251838684082\n",
      "iteration : 24500, loss train : 10.290800094604492, loss test : 9.933223724365234\n",
      "iteration : 24600, loss train : 10.270180702209473, loss test : 9.913224220275879\n",
      "iteration : 24700, loss train : 10.24958324432373, loss test : 9.893250465393066\n",
      "iteration : 24800, loss train : 10.22901725769043, loss test : 9.873303413391113\n",
      "iteration : 24900, loss train : 10.208549499511719, loss test : 9.853458404541016\n",
      "iteration : 25000, loss train : 10.188163757324219, loss test : 9.833698272705078\n",
      "iteration : 25100, loss train : 10.167868614196777, loss test : 9.814003944396973\n",
      "iteration : 25200, loss train : 10.147619247436523, loss test : 9.794349670410156\n",
      "iteration : 25300, loss train : 10.127397537231445, loss test : 9.774720191955566\n",
      "iteration : 25400, loss train : 10.107246398925781, loss test : 9.755165100097656\n",
      "iteration : 25500, loss train : 10.087201118469238, loss test : 9.735719680786133\n",
      "iteration : 25600, loss train : 10.06718921661377, loss test : 9.716301918029785\n",
      "iteration : 25700, loss train : 10.04720401763916, loss test : 9.696911811828613\n",
      "iteration : 25800, loss train : 10.02724552154541, loss test : 9.677547454833984\n",
      "iteration : 25900, loss train : 10.007332801818848, loss test : 9.658227920532227\n",
      "iteration : 26000, loss train : 9.98755168914795, loss test : 9.639045715332031\n",
      "iteration : 26100, loss train : 9.967793464660645, loss test : 9.619887351989746\n",
      "iteration : 26200, loss train : 9.948071479797363, loss test : 9.600760459899902\n",
      "iteration : 26300, loss train : 9.928376197814941, loss test : 9.581655502319336\n",
      "iteration : 26400, loss train : 9.908722877502441, loss test : 9.562581062316895\n",
      "iteration : 26500, loss train : 9.889205932617188, loss test : 9.543645858764648\n",
      "iteration : 26600, loss train : 9.869734764099121, loss test : 9.524754524230957\n",
      "iteration : 26700, loss train : 9.850288391113281, loss test : 9.505887985229492\n",
      "iteration : 26800, loss train : 9.830876350402832, loss test : 9.487049102783203\n",
      "iteration : 26900, loss train : 9.81148910522461, loss test : 9.46823501586914\n",
      "iteration : 27000, loss train : 9.792218208312988, loss test : 9.449541091918945\n",
      "iteration : 27100, loss train : 9.773006439208984, loss test : 9.430906295776367\n",
      "iteration : 27200, loss train : 9.75381851196289, loss test : 9.412296295166016\n",
      "iteration : 27300, loss train : 9.734658241271973, loss test : 9.393706321716309\n",
      "iteration : 27400, loss train : 9.71552848815918, loss test : 9.375144004821777\n",
      "iteration : 27500, loss train : 9.696483612060547, loss test : 9.356670379638672\n",
      "iteration : 27600, loss train : 9.67752456665039, loss test : 9.338284492492676\n",
      "iteration : 27700, loss train : 9.658589363098145, loss test : 9.31992244720459\n",
      "iteration : 27800, loss train : 9.639679908752441, loss test : 9.301583290100098\n",
      "iteration : 27900, loss train : 9.620798110961914, loss test : 9.283272743225098\n",
      "iteration : 28000, loss train : 9.601962089538574, loss test : 9.265006065368652\n",
      "iteration : 28100, loss train : 9.583253860473633, loss test : 9.246870040893555\n",
      "iteration : 28200, loss train : 9.564568519592285, loss test : 9.228759765625\n",
      "iteration : 28300, loss train : 9.545909881591797, loss test : 9.210672378540039\n",
      "iteration : 28400, loss train : 9.527274131774902, loss test : 9.192606925964355\n",
      "iteration : 28500, loss train : 9.50866985321045, loss test : 9.17457103729248\n",
      "iteration : 28600, loss train : 9.490163803100586, loss test : 9.156636238098145\n",
      "iteration : 28700, loss train : 9.47172737121582, loss test : 9.138773918151855\n",
      "iteration : 28800, loss train : 9.453315734863281, loss test : 9.120933532714844\n",
      "iteration : 28900, loss train : 9.434928894042969, loss test : 9.103116989135742\n",
      "iteration : 29000, loss train : 9.41656494140625, loss test : 9.085326194763184\n",
      "iteration : 29100, loss train : 9.398235321044922, loss test : 9.067561149597168\n",
      "iteration : 29200, loss train : 9.380033493041992, loss test : 9.049932479858398\n",
      "iteration : 29300, loss train : 9.361868858337402, loss test : 9.03233814239502\n",
      "iteration : 29400, loss train : 9.343727111816406, loss test : 9.01476764678955\n",
      "iteration : 29500, loss train : 9.325608253479004, loss test : 8.997220039367676\n",
      "iteration : 29600, loss train : 9.307515144348145, loss test : 8.979698181152344\n",
      "iteration : 29700, loss train : 9.289469718933105, loss test : 8.96220588684082\n",
      "iteration : 29800, loss train : 9.271574974060059, loss test : 8.944863319396973\n",
      "iteration : 29900, loss train : 9.253704071044922, loss test : 8.927541732788086\n",
      "iteration : 30000, loss train : 9.235856056213379, loss test : 8.910243034362793\n",
      "iteration : 30100, loss train : 9.21803092956543, loss test : 8.892970085144043\n",
      "iteration : 30200, loss train : 9.20023250579834, loss test : 8.875718116760254\n",
      "iteration : 30300, loss train : 9.182516098022461, loss test : 8.858549118041992\n",
      "iteration : 30400, loss train : 9.164884567260742, loss test : 8.84146785736084\n",
      "iteration : 30500, loss train : 9.14727783203125, loss test : 8.824409484863281\n",
      "iteration : 30600, loss train : 9.129692077636719, loss test : 8.807374954223633\n",
      "iteration : 30700, loss train : 9.112129211425781, loss test : 8.790361404418945\n",
      "iteration : 30800, loss train : 9.09459400177002, loss test : 8.773371696472168\n",
      "iteration : 30900, loss train : 9.077166557312012, loss test : 8.75649356842041\n",
      "iteration : 31000, loss train : 9.059796333312988, loss test : 8.739672660827637\n",
      "iteration : 31100, loss train : 9.042448997497559, loss test : 8.72287368774414\n",
      "iteration : 31200, loss train : 9.025123596191406, loss test : 8.706097602844238\n",
      "iteration : 31300, loss train : 9.007822036743164, loss test : 8.689343452453613\n",
      "iteration : 31400, loss train : 8.990545272827148, loss test : 8.672613143920898\n",
      "iteration : 31500, loss train : 8.973381996154785, loss test : 8.65599536895752\n",
      "iteration : 31600, loss train : 8.956269264221191, loss test : 8.639433860778809\n",
      "iteration : 31700, loss train : 8.939180374145508, loss test : 8.622891426086426\n",
      "iteration : 31800, loss train : 8.922113418579102, loss test : 8.606372833251953\n",
      "iteration : 31900, loss train : 8.905086517333984, loss test : 8.58987808227539\n",
      "iteration : 32000, loss train : 8.888093948364258, loss test : 8.573410034179688\n",
      "iteration : 32100, loss train : 8.871214866638184, loss test : 8.557060241699219\n",
      "iteration : 32200, loss train : 8.854385375976562, loss test : 8.540757179260254\n",
      "iteration : 32300, loss train : 8.837576866149902, loss test : 8.52447509765625\n",
      "iteration : 32400, loss train : 8.820789337158203, loss test : 8.508216857910156\n",
      "iteration : 32500, loss train : 8.804023742675781, loss test : 8.49197769165039\n",
      "iteration : 32600, loss train : 8.787283897399902, loss test : 8.475760459899902\n",
      "iteration : 32700, loss train : 8.770681381225586, loss test : 8.459685325622559\n",
      "iteration : 32800, loss train : 8.754101753234863, loss test : 8.443634033203125\n",
      "iteration : 32900, loss train : 8.737543106079102, loss test : 8.427602767944336\n",
      "iteration : 33000, loss train : 8.72100830078125, loss test : 8.411592483520508\n",
      "iteration : 33100, loss train : 8.704493522644043, loss test : 8.395604133605957\n",
      "iteration : 33200, loss train : 8.688002586364746, loss test : 8.379636764526367\n",
      "iteration : 33300, loss train : 8.671648979187012, loss test : 8.363810539245605\n",
      "iteration : 33400, loss train : 8.655318260192871, loss test : 8.348006248474121\n",
      "iteration : 33500, loss train : 8.639008522033691, loss test : 8.332221984863281\n",
      "iteration : 33600, loss train : 8.622720718383789, loss test : 8.316459655761719\n",
      "iteration : 33700, loss train : 8.606453895568848, loss test : 8.300716400146484\n",
      "iteration : 33800, loss train : 8.590210914611816, loss test : 8.284995079040527\n",
      "iteration : 33900, loss train : 8.574078559875488, loss test : 8.269389152526855\n",
      "iteration : 34000, loss train : 8.557994842529297, loss test : 8.253830909729004\n",
      "iteration : 34100, loss train : 8.541948318481445, loss test : 8.23829460144043\n",
      "iteration : 34200, loss train : 8.525932312011719, loss test : 8.222783088684082\n",
      "iteration : 34300, loss train : 8.50993537902832, loss test : 8.207292556762695\n",
      "iteration : 34400, loss train : 8.493961334228516, loss test : 8.191822052001953\n",
      "iteration : 34500, loss train : 8.478096008300781, loss test : 8.176461219787598\n",
      "iteration : 34600, loss train : 8.4622802734375, loss test : 8.161151885986328\n",
      "iteration : 34700, loss train : 8.446483612060547, loss test : 8.14586067199707\n",
      "iteration : 34800, loss train : 8.430709838867188, loss test : 8.130590438842773\n",
      "iteration : 34900, loss train : 8.414955139160156, loss test : 8.115340232849121\n",
      "iteration : 35000, loss train : 8.399221420288086, loss test : 8.10011100769043\n",
      "iteration : 35100, loss train : 8.383590698242188, loss test : 8.084982872009277\n",
      "iteration : 35200, loss train : 8.368013381958008, loss test : 8.069911003112793\n",
      "iteration : 35300, loss train : 8.352458953857422, loss test : 8.054861068725586\n",
      "iteration : 35400, loss train : 8.336922645568848, loss test : 8.039828300476074\n",
      "iteration : 35500, loss train : 8.321406364440918, loss test : 8.024816513061523\n",
      "iteration : 35600, loss train : 8.305912017822266, loss test : 8.009824752807617\n",
      "iteration : 35700, loss train : 8.290489196777344, loss test : 7.9949049949646\n",
      "iteration : 35800, loss train : 8.275150299072266, loss test : 7.980068683624268\n",
      "iteration : 35900, loss train : 8.259832382202148, loss test : 7.96525239944458\n",
      "iteration : 36000, loss train : 8.24453353881836, loss test : 7.950457572937012\n",
      "iteration : 36100, loss train : 8.229254722595215, loss test : 7.9356794357299805\n",
      "iteration : 36200, loss train : 8.213994026184082, loss test : 7.92092227935791\n",
      "iteration : 36300, loss train : 8.198761940002441, loss test : 7.906184673309326\n",
      "iteration : 36400, loss train : 8.18367862701416, loss test : 7.8915863037109375\n",
      "iteration : 36500, loss train : 8.168618202209473, loss test : 7.877011299133301\n",
      "iteration : 36600, loss train : 8.153576850891113, loss test : 7.862454414367676\n",
      "iteration : 36700, loss train : 8.138556480407715, loss test : 7.8479156494140625\n",
      "iteration : 36800, loss train : 8.123554229736328, loss test : 7.833396911621094\n",
      "iteration : 36900, loss train : 8.108572959899902, loss test : 7.818896770477295\n",
      "iteration : 37000, loss train : 8.093710899353027, loss test : 7.804518222808838\n",
      "iteration : 37100, loss train : 8.07888126373291, loss test : 7.790172100067139\n",
      "iteration : 37200, loss train : 8.064072608947754, loss test : 7.775846004486084\n",
      "iteration : 37300, loss train : 8.049281120300293, loss test : 7.761538505554199\n",
      "iteration : 37400, loss train : 8.034509658813477, loss test : 7.747248649597168\n",
      "iteration : 37500, loss train : 8.019758224487305, loss test : 7.732978820800781\n",
      "iteration : 37600, loss train : 8.005090713500977, loss test : 7.718791961669922\n",
      "iteration : 37700, loss train : 7.990489959716797, loss test : 7.704674243927002\n",
      "iteration : 37800, loss train : 7.9759087562561035, loss test : 7.690574645996094\n",
      "iteration : 37900, loss train : 7.961345195770264, loss test : 7.676492691040039\n",
      "iteration : 38000, loss train : 7.946802139282227, loss test : 7.662430286407471\n",
      "iteration : 38100, loss train : 7.932278156280518, loss test : 7.648386478424072\n",
      "iteration : 38200, loss train : 7.917777061462402, loss test : 7.634364128112793\n",
      "iteration : 38300, loss train : 7.9034037590026855, loss test : 7.620471000671387\n",
      "iteration : 38400, loss train : 7.889047145843506, loss test : 7.606595993041992\n",
      "iteration : 38500, loss train : 7.874709606170654, loss test : 7.592738628387451\n",
      "iteration : 38600, loss train : 7.86039924621582, loss test : 7.578901290893555\n",
      "iteration : 38700, loss train : 7.846121311187744, loss test : 7.56508731842041\n",
      "iteration : 38800, loss train : 7.831864356994629, loss test : 7.5512919425964355\n",
      "iteration : 38900, loss train : 7.817687511444092, loss test : 7.537574291229248\n",
      "iteration : 39000, loss train : 7.803577423095703, loss test : 7.523927688598633\n",
      "iteration : 39100, loss train : 7.789486408233643, loss test : 7.510298252105713\n",
      "iteration : 39200, loss train : 7.7754130363464355, loss test : 7.496686935424805\n",
      "iteration : 39300, loss train : 7.76135778427124, loss test : 7.483093738555908\n",
      "iteration : 39400, loss train : 7.747322082519531, loss test : 7.469519138336182\n",
      "iteration : 39500, loss train : 7.733323574066162, loss test : 7.455979824066162\n",
      "iteration : 39600, loss train : 7.71943473815918, loss test : 7.442552089691162\n",
      "iteration : 39700, loss train : 7.705563545227051, loss test : 7.429141998291016\n",
      "iteration : 39800, loss train : 7.691709995269775, loss test : 7.4157490730285645\n",
      "iteration : 39900, loss train : 7.677873611450195, loss test : 7.402373790740967\n",
      "iteration : 40000, loss train : 7.664056301116943, loss test : 7.3890156745910645\n",
      "iteration : 40100, loss train : 7.650257110595703, loss test : 7.375676155090332\n",
      "iteration : 40200, loss train : 7.636539936065674, loss test : 7.362417697906494\n",
      "iteration : 40300, loss train : 7.6228861808776855, loss test : 7.349224090576172\n",
      "iteration : 40400, loss train : 7.609249114990234, loss test : 7.336047172546387\n",
      "iteration : 40500, loss train : 7.595630645751953, loss test : 7.322888374328613\n",
      "iteration : 40600, loss train : 7.582029819488525, loss test : 7.3097453117370605\n",
      "iteration : 40700, loss train : 7.568447113037109, loss test : 7.2966203689575195\n",
      "iteration : 40800, loss train : 7.554884910583496, loss test : 7.283514022827148\n",
      "iteration : 40900, loss train : 7.541416168212891, loss test : 7.270503520965576\n",
      "iteration : 41000, loss train : 7.528003215789795, loss test : 7.257542133331299\n",
      "iteration : 41100, loss train : 7.514621257781982, loss test : 7.244602203369141\n",
      "iteration : 41200, loss train : 7.501256465911865, loss test : 7.2316789627075195\n",
      "iteration : 41300, loss train : 7.487909317016602, loss test : 7.21877384185791\n",
      "iteration : 41400, loss train : 7.474578857421875, loss test : 7.2058844566345215\n",
      "iteration : 41500, loss train : 7.461289405822754, loss test : 7.19303035736084\n",
      "iteration : 41600, loss train : 7.448101997375488, loss test : 7.180285453796387\n",
      "iteration : 41700, loss train : 7.434931755065918, loss test : 7.167555809020996\n",
      "iteration : 41800, loss train : 7.421777725219727, loss test : 7.154842853546143\n",
      "iteration : 41900, loss train : 7.408641338348389, loss test : 7.142146587371826\n",
      "iteration : 42000, loss train : 7.3955230712890625, loss test : 7.129467010498047\n",
      "iteration : 42100, loss train : 7.382421016693115, loss test : 7.1168036460876465\n",
      "iteration : 42200, loss train : 7.369393825531006, loss test : 7.104214191436768\n",
      "iteration : 42300, loss train : 7.356431484222412, loss test : 7.091692924499512\n",
      "iteration : 42400, loss train : 7.343486785888672, loss test : 7.079187870025635\n",
      "iteration : 42500, loss train : 7.330558776855469, loss test : 7.066697597503662\n",
      "iteration : 42600, loss train : 7.317647457122803, loss test : 7.054225444793701\n",
      "iteration : 42700, loss train : 7.304751873016357, loss test : 7.041769027709961\n",
      "iteration : 42800, loss train : 7.291878700256348, loss test : 7.029329776763916\n",
      "iteration : 42900, loss train : 7.279083251953125, loss test : 7.016971111297607\n",
      "iteration : 43000, loss train : 7.26634407043457, loss test : 7.0046706199646\n",
      "iteration : 43100, loss train : 7.253623008728027, loss test : 6.992386341094971\n",
      "iteration : 43200, loss train : 7.240916728973389, loss test : 6.980118751525879\n",
      "iteration : 43300, loss train : 7.228227138519287, loss test : 6.967865943908691\n",
      "iteration : 43400, loss train : 7.215555191040039, loss test : 6.955630302429199\n",
      "iteration : 43500, loss train : 7.202925205230713, loss test : 6.943418025970459\n",
      "iteration : 43600, loss train : 7.190367698669434, loss test : 6.931280612945557\n",
      "iteration : 43700, loss train : 7.177871227264404, loss test : 6.919205665588379\n",
      "iteration : 43800, loss train : 7.165390968322754, loss test : 6.907145977020264\n",
      "iteration : 43900, loss train : 7.152925968170166, loss test : 6.895102500915527\n",
      "iteration : 44000, loss train : 7.140477657318115, loss test : 6.883073806762695\n",
      "iteration : 44100, loss train : 7.128045558929443, loss test : 6.871062278747559\n",
      "iteration : 44200, loss train : 7.115637302398682, loss test : 6.859068870544434\n",
      "iteration : 44300, loss train : 7.103324890136719, loss test : 6.847175121307373\n",
      "iteration : 44400, loss train : 7.0910444259643555, loss test : 6.835315704345703\n",
      "iteration : 44500, loss train : 7.078780174255371, loss test : 6.8234710693359375\n",
      "iteration : 44600, loss train : 7.066532135009766, loss test : 6.811640739440918\n",
      "iteration : 44700, loss train : 7.0542988777160645, loss test : 6.799827575683594\n",
      "iteration : 44800, loss train : 7.042083263397217, loss test : 6.788029670715332\n",
      "iteration : 44900, loss train : 7.029890537261963, loss test : 6.776249885559082\n",
      "iteration : 45000, loss train : 7.017794132232666, loss test : 6.764571666717529\n",
      "iteration : 45100, loss train : 7.005727767944336, loss test : 6.752923965454102\n",
      "iteration : 45200, loss train : 6.993678092956543, loss test : 6.741291522979736\n",
      "iteration : 45300, loss train : 6.981642723083496, loss test : 6.729673385620117\n",
      "iteration : 45400, loss train : 6.96962308883667, loss test : 6.718070983886719\n",
      "iteration : 45500, loss train : 6.957620620727539, loss test : 6.706484794616699\n",
      "iteration : 45600, loss train : 6.9456400871276855, loss test : 6.694915771484375\n",
      "iteration : 45700, loss train : 6.933729648590088, loss test : 6.683420658111572\n",
      "iteration : 45800, loss train : 6.921830654144287, loss test : 6.671935081481934\n",
      "iteration : 45900, loss train : 6.909976005554199, loss test : 6.660496234893799\n",
      "iteration : 46000, loss train : 6.898161888122559, loss test : 6.649090766906738\n",
      "iteration : 46100, loss train : 6.886373519897461, loss test : 6.6377034187316895\n",
      "iteration : 46200, loss train : 6.874612331390381, loss test : 6.626341819763184\n",
      "iteration : 46300, loss train : 6.86290979385376, loss test : 6.615037441253662\n",
      "iteration : 46400, loss train : 6.851221084594727, loss test : 6.603747367858887\n",
      "iteration : 46500, loss train : 6.839548110961914, loss test : 6.592472553253174\n",
      "iteration : 46600, loss train : 6.8279242515563965, loss test : 6.581249237060547\n",
      "iteration : 46700, loss train : 6.816328525543213, loss test : 6.5700531005859375\n",
      "iteration : 46800, loss train : 6.804747581481934, loss test : 6.558871269226074\n",
      "iteration : 46900, loss train : 6.793194770812988, loss test : 6.547717094421387\n",
      "iteration : 47000, loss train : 6.781696796417236, loss test : 6.536616802215576\n",
      "iteration : 47100, loss train : 6.770214557647705, loss test : 6.525531768798828\n",
      "iteration : 47200, loss train : 6.758747100830078, loss test : 6.51446008682251\n",
      "iteration : 47300, loss train : 6.747323513031006, loss test : 6.503435134887695\n",
      "iteration : 47400, loss train : 6.73593282699585, loss test : 6.492441654205322\n",
      "iteration : 47500, loss train : 6.724555015563965, loss test : 6.481463432312012\n",
      "iteration : 47600, loss train : 6.713194370269775, loss test : 6.470499038696289\n",
      "iteration : 47700, loss train : 6.701898574829102, loss test : 6.459599018096924\n",
      "iteration : 47800, loss train : 6.690619945526123, loss test : 6.448714733123779\n",
      "iteration : 47900, loss train : 6.679354190826416, loss test : 6.437844276428223\n",
      "iteration : 48000, loss train : 6.66811466217041, loss test : 6.427000045776367\n",
      "iteration : 48100, loss train : 6.656925201416016, loss test : 6.416207313537598\n",
      "iteration : 48200, loss train : 6.645749568939209, loss test : 6.4054274559021\n",
      "iteration : 48300, loss train : 6.634590148925781, loss test : 6.3946638107299805\n",
      "iteration : 48400, loss train : 6.62346887588501, loss test : 6.383935451507568\n",
      "iteration : 48500, loss train : 6.612390041351318, loss test : 6.3732500076293945\n",
      "iteration : 48600, loss train : 6.601329803466797, loss test : 6.362579345703125\n",
      "iteration : 48700, loss train : 6.590299606323242, loss test : 6.351926803588867\n",
      "iteration : 48800, loss train : 6.5793137550354, loss test : 6.34132194519043\n",
      "iteration : 48900, loss train : 6.568356990814209, loss test : 6.330747127532959\n",
      "iteration : 49000, loss train : 6.557415008544922, loss test : 6.320185661315918\n",
      "iteration : 49100, loss train : 6.546499729156494, loss test : 6.3096466064453125\n",
      "iteration : 49200, loss train : 6.535637855529785, loss test : 6.299163341522217\n",
      "iteration : 49300, loss train : 6.524790287017822, loss test : 6.288693428039551\n",
      "iteration : 49400, loss train : 6.513955593109131, loss test : 6.278237342834473\n",
      "iteration : 49500, loss train : 6.503159046173096, loss test : 6.267818450927734\n",
      "iteration : 49600, loss train : 6.492399215698242, loss test : 6.257437705993652\n",
      "iteration : 49700, loss train : 6.481651782989502, loss test : 6.247069835662842\n",
      "iteration : 49800, loss train : 6.4709248542785645, loss test : 6.236719131469727\n",
      "iteration : 49900, loss train : 6.46024751663208, loss test : 6.226418495178223\n",
      "iteration : 50000, loss train : 6.449594974517822, loss test : 6.216141700744629\n",
      "iteration : 50100, loss train : 6.438955307006836, loss test : 6.205878734588623\n",
      "iteration : 50200, loss train : 6.428330421447754, loss test : 6.195629119873047\n",
      "iteration : 50300, loss train : 6.417762756347656, loss test : 6.1854400634765625\n",
      "iteration : 50400, loss train : 6.4072089195251465, loss test : 6.175263404846191\n",
      "iteration : 50500, loss train : 6.39667272567749, loss test : 6.165103435516357\n",
      "iteration : 50600, loss train : 6.386159896850586, loss test : 6.15496301651001\n",
      "iteration : 50700, loss train : 6.375698566436768, loss test : 6.144876480102539\n",
      "iteration : 50800, loss train : 6.365250587463379, loss test : 6.134802341461182\n",
      "iteration : 50900, loss train : 6.35481595993042, loss test : 6.12474250793457\n",
      "iteration : 51000, loss train : 6.3444037437438965, loss test : 6.114704608917236\n",
      "iteration : 51100, loss train : 6.33404016494751, loss test : 6.104716777801514\n",
      "iteration : 51200, loss train : 6.32369327545166, loss test : 6.094743251800537\n",
      "iteration : 51300, loss train : 6.313363552093506, loss test : 6.084784984588623\n",
      "iteration : 51400, loss train : 6.303077697753906, loss test : 6.074857711791992\n",
      "iteration : 51500, loss train : 6.292837142944336, loss test : 6.064976692199707\n",
      "iteration : 51600, loss train : 6.28261137008667, loss test : 6.055110931396484\n",
      "iteration : 51700, loss train : 6.272397518157959, loss test : 6.045257091522217\n",
      "iteration : 51800, loss train : 6.262223720550537, loss test : 6.035441875457764\n",
      "iteration : 51900, loss train : 6.252079963684082, loss test : 6.025660037994385\n",
      "iteration : 52000, loss train : 6.241955757141113, loss test : 6.015893459320068\n",
      "iteration : 52100, loss train : 6.231844902038574, loss test : 6.006138801574707\n",
      "iteration : 52200, loss train : 6.221785068511963, loss test : 5.996437072753906\n",
      "iteration : 52300, loss train : 6.21174430847168, loss test : 5.986753940582275\n",
      "iteration : 52400, loss train : 6.20171594619751, loss test : 5.977084159851074\n",
      "iteration : 52500, loss train : 6.191702842712402, loss test : 5.967428207397461\n",
      "iteration : 52600, loss train : 6.181743621826172, loss test : 5.957828998565674\n",
      "iteration : 52700, loss train : 6.17180061340332, loss test : 5.948243141174316\n",
      "iteration : 52800, loss train : 6.161874771118164, loss test : 5.938672065734863\n",
      "iteration : 52900, loss train : 6.1519670486450195, loss test : 5.929118633270264\n",
      "iteration : 53000, loss train : 6.142116546630859, loss test : 5.919622421264648\n",
      "iteration : 53100, loss train : 6.132318496704102, loss test : 5.910170078277588\n",
      "iteration : 53200, loss train : 6.122533321380615, loss test : 5.900730133056641\n",
      "iteration : 53300, loss train : 6.112783432006836, loss test : 5.8913254737854\n",
      "iteration : 53400, loss train : 6.1030755043029785, loss test : 5.881960868835449\n",
      "iteration : 53500, loss train : 6.093383312225342, loss test : 5.872610569000244\n",
      "iteration : 53600, loss train : 6.083707332611084, loss test : 5.863274574279785\n",
      "iteration : 53700, loss train : 6.0740838050842285, loss test : 5.853991508483887\n",
      "iteration : 53800, loss train : 6.064480781555176, loss test : 5.844728946685791\n",
      "iteration : 53900, loss train : 6.054890155792236, loss test : 5.835478782653809\n",
      "iteration : 54000, loss train : 6.045321941375732, loss test : 5.826250076293945\n",
      "iteration : 54100, loss train : 6.035800457000732, loss test : 5.8170695304870605\n",
      "iteration : 54200, loss train : 6.026292324066162, loss test : 5.807901859283447\n",
      "iteration : 54300, loss train : 6.01680326461792, loss test : 5.7987494468688965\n",
      "iteration : 54400, loss train : 6.007347583770752, loss test : 5.7896318435668945\n",
      "iteration : 54500, loss train : 5.997923374176025, loss test : 5.780546188354492\n",
      "iteration : 54600, loss train : 5.988512992858887, loss test : 5.771472930908203\n",
      "iteration : 54700, loss train : 5.979113578796387, loss test : 5.7624125480651855\n",
      "iteration : 54800, loss train : 5.969757556915283, loss test : 5.7533955574035645\n",
      "iteration : 54900, loss train : 5.96042537689209, loss test : 5.7444024085998535\n",
      "iteration : 55000, loss train : 5.951107025146484, loss test : 5.735422611236572\n",
      "iteration : 55100, loss train : 5.941804885864258, loss test : 5.726455211639404\n",
      "iteration : 55200, loss train : 5.932551860809326, loss test : 5.717539310455322\n",
      "iteration : 55300, loss train : 5.923315525054932, loss test : 5.708639144897461\n",
      "iteration : 55400, loss train : 5.914092063903809, loss test : 5.699750900268555\n",
      "iteration : 55500, loss train : 5.904878616333008, loss test : 5.6908745765686035\n",
      "iteration : 55600, loss train : 5.895718097686768, loss test : 5.682051181793213\n",
      "iteration : 55700, loss train : 5.886569976806641, loss test : 5.67324161529541\n",
      "iteration : 55800, loss train : 5.877438545227051, loss test : 5.664444923400879\n",
      "iteration : 55900, loss train : 5.868321895599365, loss test : 5.655661106109619\n",
      "iteration : 56000, loss train : 5.859254837036133, loss test : 5.646928787231445\n",
      "iteration : 56100, loss train : 5.850202560424805, loss test : 5.638210773468018\n",
      "iteration : 56200, loss train : 5.841160774230957, loss test : 5.6295037269592285\n",
      "iteration : 56300, loss train : 5.832131862640381, loss test : 5.620809078216553\n",
      "iteration : 56400, loss train : 5.823147296905518, loss test : 5.612159252166748\n",
      "iteration : 56500, loss train : 5.814182281494141, loss test : 5.603530406951904\n",
      "iteration : 56600, loss train : 5.805235385894775, loss test : 5.59491491317749\n",
      "iteration : 56700, loss train : 5.796299934387207, loss test : 5.586310863494873\n",
      "iteration : 56800, loss train : 5.787400245666504, loss test : 5.5777435302734375\n",
      "iteration : 56900, loss train : 5.778537273406982, loss test : 5.569207668304443\n",
      "iteration : 57000, loss train : 5.769693851470947, loss test : 5.560686111450195\n",
      "iteration : 57100, loss train : 5.760861873626709, loss test : 5.552175998687744\n",
      "iteration : 57200, loss train : 5.752068042755127, loss test : 5.543704032897949\n",
      "iteration : 57300, loss train : 5.743302345275879, loss test : 5.53525972366333\n",
      "iteration : 57400, loss train : 5.734552383422852, loss test : 5.526828765869141\n",
      "iteration : 57500, loss train : 5.7258148193359375, loss test : 5.518409252166748\n",
      "iteration : 57600, loss train : 5.717117786407471, loss test : 5.510031700134277\n",
      "iteration : 57700, loss train : 5.708442687988281, loss test : 5.501676082611084\n",
      "iteration : 57800, loss train : 5.699779033660889, loss test : 5.493331432342529\n",
      "iteration : 57900, loss train : 5.691125869750977, loss test : 5.4849982261657715\n",
      "iteration : 58000, loss train : 5.682514667510986, loss test : 5.476707458496094\n",
      "iteration : 58100, loss train : 5.673924922943115, loss test : 5.468437671661377\n",
      "iteration : 58200, loss train : 5.6653523445129395, loss test : 5.460181713104248\n",
      "iteration : 58300, loss train : 5.656790733337402, loss test : 5.451935768127441\n",
      "iteration : 58400, loss train : 5.64826774597168, loss test : 5.443730354309082\n",
      "iteration : 58500, loss train : 5.639767169952393, loss test : 5.435548305511475\n",
      "iteration : 58600, loss train : 5.6312785148620605, loss test : 5.427376747131348\n",
      "iteration : 58700, loss train : 5.622799873352051, loss test : 5.419216632843018\n",
      "iteration : 58800, loss train : 5.614353179931641, loss test : 5.411087989807129\n",
      "iteration : 58900, loss train : 5.605937957763672, loss test : 5.402990341186523\n",
      "iteration : 59000, loss train : 5.597538948059082, loss test : 5.3949055671691895\n",
      "iteration : 59100, loss train : 5.589150905609131, loss test : 5.386832237243652\n",
      "iteration : 59200, loss train : 5.580782413482666, loss test : 5.378779411315918\n",
      "iteration : 59300, loss train : 5.572454929351807, loss test : 5.370767116546631\n",
      "iteration : 59400, loss train : 5.564138889312744, loss test : 5.362765312194824\n",
      "iteration : 59500, loss train : 5.555832386016846, loss test : 5.354775428771973\n",
      "iteration : 59600, loss train : 5.547537326812744, loss test : 5.346795082092285\n",
      "iteration : 59700, loss train : 5.539288520812988, loss test : 5.338861465454102\n",
      "iteration : 59800, loss train : 5.531060695648193, loss test : 5.33094596862793\n",
      "iteration : 59900, loss train : 5.522842884063721, loss test : 5.32304048538208\n",
      "iteration : 60000, loss train : 5.51463508605957, loss test : 5.315145969390869\n",
      "iteration : 60100, loss train : 5.506453990936279, loss test : 5.307277202606201\n",
      "iteration : 60200, loss train : 5.4983062744140625, loss test : 5.299442291259766\n",
      "iteration : 60300, loss train : 5.490170478820801, loss test : 5.291619777679443\n",
      "iteration : 60400, loss train : 5.482044696807861, loss test : 5.283806324005127\n",
      "iteration : 60500, loss train : 5.473937034606934, loss test : 5.276006698608398\n",
      "iteration : 60600, loss train : 5.465888500213623, loss test : 5.268258571624756\n",
      "iteration : 60700, loss train : 5.45785665512085, loss test : 5.260526180267334\n",
      "iteration : 60800, loss train : 5.449835300445557, loss test : 5.252803802490234\n",
      "iteration : 60900, loss train : 5.441822528839111, loss test : 5.245090961456299\n",
      "iteration : 61000, loss train : 5.4338507652282715, loss test : 5.237420082092285\n",
      "iteration : 61100, loss train : 5.425898551940918, loss test : 5.2297682762146\n",
      "iteration : 61200, loss train : 5.417956352233887, loss test : 5.222126483917236\n",
      "iteration : 61300, loss train : 5.410024642944336, loss test : 5.2144951820373535\n",
      "iteration : 61400, loss train : 5.402129650115967, loss test : 5.206897735595703\n",
      "iteration : 61500, loss train : 5.394261837005615, loss test : 5.199328899383545\n",
      "iteration : 61600, loss train : 5.3864054679870605, loss test : 5.191769599914551\n",
      "iteration : 61700, loss train : 5.37855863571167, loss test : 5.184220314025879\n",
      "iteration : 61800, loss train : 5.370730400085449, loss test : 5.176690101623535\n",
      "iteration : 61900, loss train : 5.362941741943359, loss test : 5.169200420379639\n",
      "iteration : 62000, loss train : 5.355162620544434, loss test : 5.161720275878906\n",
      "iteration : 62100, loss train : 5.34739351272583, loss test : 5.154250144958496\n",
      "iteration : 62200, loss train : 5.33964204788208, loss test : 5.146792888641357\n",
      "iteration : 62300, loss train : 5.331928730010986, loss test : 5.139376163482666\n",
      "iteration : 62400, loss train : 5.324234485626221, loss test : 5.1319780349731445\n",
      "iteration : 62500, loss train : 5.316549777984619, loss test : 5.124588489532471\n",
      "iteration : 62600, loss train : 5.308874130249023, loss test : 5.117208957672119\n",
      "iteration : 62700, loss train : 5.301219463348389, loss test : 5.10984992980957\n",
      "iteration : 62800, loss train : 5.29360294342041, loss test : 5.102529525756836\n",
      "iteration : 62900, loss train : 5.285995006561279, loss test : 5.095218181610107\n",
      "iteration : 63000, loss train : 5.278400897979736, loss test : 5.087918281555176\n",
      "iteration : 63100, loss train : 5.270819187164307, loss test : 5.080628871917725\n",
      "iteration : 63200, loss train : 5.263270378112793, loss test : 5.073372840881348\n",
      "iteration : 63300, loss train : 5.255744457244873, loss test : 5.066140651702881\n",
      "iteration : 63400, loss train : 5.248229503631592, loss test : 5.058919906616211\n",
      "iteration : 63500, loss train : 5.240724086761475, loss test : 5.051706790924072\n",
      "iteration : 63600, loss train : 5.2332282066345215, loss test : 5.044503211975098\n",
      "iteration : 63700, loss train : 5.225775718688965, loss test : 5.037343502044678\n",
      "iteration : 63800, loss train : 5.218355178833008, loss test : 5.030205249786377\n",
      "iteration : 63900, loss train : 5.2109479904174805, loss test : 5.023078918457031\n",
      "iteration : 64000, loss train : 5.203549861907959, loss test : 5.015961170196533\n",
      "iteration : 64100, loss train : 5.19618034362793, loss test : 5.008872985839844\n",
      "iteration : 64200, loss train : 5.188838481903076, loss test : 5.001812934875488\n",
      "iteration : 64300, loss train : 5.181506156921387, loss test : 4.994761943817139\n",
      "iteration : 64400, loss train : 5.174183368682861, loss test : 4.987720012664795\n",
      "iteration : 64500, loss train : 5.166874408721924, loss test : 4.9806928634643555\n",
      "iteration : 64600, loss train : 5.159607410430908, loss test : 4.973708629608154\n",
      "iteration : 64700, loss train : 5.1523542404174805, loss test : 4.966734409332275\n",
      "iteration : 64800, loss train : 5.145111083984375, loss test : 4.959770679473877\n",
      "iteration : 64900, loss train : 5.137877941131592, loss test : 4.952815532684326\n",
      "iteration : 65000, loss train : 5.130678176879883, loss test : 4.945895195007324\n",
      "iteration : 65100, loss train : 5.123499870300293, loss test : 4.938995838165283\n",
      "iteration : 65200, loss train : 5.116330623626709, loss test : 4.932106018066406\n",
      "iteration : 65300, loss train : 5.109170436859131, loss test : 4.925225257873535\n",
      "iteration : 65400, loss train : 5.10202169418335, loss test : 4.918355941772461\n",
      "iteration : 65500, loss train : 5.094918727874756, loss test : 4.911531925201416\n",
      "iteration : 65600, loss train : 5.087829113006592, loss test : 4.90471887588501\n",
      "iteration : 65700, loss train : 5.080748081207275, loss test : 4.897914409637451\n",
      "iteration : 65800, loss train : 5.073675632476807, loss test : 4.891118049621582\n",
      "iteration : 65900, loss train : 5.066626071929932, loss test : 4.884345531463623\n",
      "iteration : 66000, loss train : 5.059607982635498, loss test : 4.877604961395264\n",
      "iteration : 66100, loss train : 5.0525994300842285, loss test : 4.87087345123291\n",
      "iteration : 66200, loss train : 5.045600414276123, loss test : 4.8641510009765625\n",
      "iteration : 66300, loss train : 5.038609027862549, loss test : 4.857436180114746\n",
      "iteration : 66400, loss train : 5.031651020050049, loss test : 4.850752353668213\n",
      "iteration : 66500, loss train : 5.024720668792725, loss test : 4.844096660614014\n",
      "iteration : 66600, loss train : 5.01779842376709, loss test : 4.837448596954346\n",
      "iteration : 66700, loss train : 5.010885238647461, loss test : 4.830809593200684\n",
      "iteration : 66800, loss train : 5.003981113433838, loss test : 4.824178695678711\n",
      "iteration : 66900, loss train : 4.997105121612549, loss test : 4.817577362060547\n",
      "iteration : 67000, loss train : 4.9902544021606445, loss test : 4.811001300811768\n",
      "iteration : 67100, loss train : 4.983423709869385, loss test : 4.80443811416626\n",
      "iteration : 67200, loss train : 4.9766082763671875, loss test : 4.797887325286865\n",
      "iteration : 67300, loss train : 4.969804763793945, loss test : 4.791345119476318\n",
      "iteration : 67400, loss train : 4.963042259216309, loss test : 4.78484582901001\n",
      "iteration : 67500, loss train : 4.956291198730469, loss test : 4.778358459472656\n",
      "iteration : 67600, loss train : 4.949550628662109, loss test : 4.771881103515625\n",
      "iteration : 67700, loss train : 4.942817687988281, loss test : 4.765410900115967\n",
      "iteration : 67800, loss train : 4.936106204986572, loss test : 4.758962154388428\n",
      "iteration : 67900, loss train : 4.929425239562988, loss test : 4.752545356750488\n",
      "iteration : 68000, loss train : 4.922754287719727, loss test : 4.7461371421813965\n",
      "iteration : 68100, loss train : 4.916092872619629, loss test : 4.739738941192627\n",
      "iteration : 68200, loss train : 4.9094438552856445, loss test : 4.733349800109863\n",
      "iteration : 68300, loss train : 4.902828693389893, loss test : 4.72699499130249\n",
      "iteration : 68400, loss train : 4.896231651306152, loss test : 4.7206597328186035\n",
      "iteration : 68500, loss train : 4.88964319229126, loss test : 4.71433162689209\n",
      "iteration : 68600, loss train : 4.883063316345215, loss test : 4.708012580871582\n",
      "iteration : 68700, loss train : 4.876491546630859, loss test : 4.701700687408447\n",
      "iteration : 68800, loss train : 4.869961738586426, loss test : 4.695432662963867\n",
      "iteration : 68900, loss train : 4.863440990447998, loss test : 4.689173698425293\n",
      "iteration : 69000, loss train : 4.856932640075684, loss test : 4.682924270629883\n",
      "iteration : 69100, loss train : 4.850435256958008, loss test : 4.676684379577637\n",
      "iteration : 69200, loss train : 4.843947887420654, loss test : 4.670454502105713\n",
      "iteration : 69300, loss train : 4.837501525878906, loss test : 4.664267063140869\n",
      "iteration : 69400, loss train : 4.831063270568848, loss test : 4.658087253570557\n",
      "iteration : 69500, loss train : 4.824633598327637, loss test : 4.651915550231934\n",
      "iteration : 69600, loss train : 4.818212032318115, loss test : 4.645751953125\n",
      "iteration : 69700, loss train : 4.811798572540283, loss test : 4.639595985412598\n",
      "iteration : 69800, loss train : 4.805428504943848, loss test : 4.633484363555908\n",
      "iteration : 69900, loss train : 4.799069881439209, loss test : 4.627382755279541\n",
      "iteration : 70000, loss train : 4.792720794677734, loss test : 4.621288776397705\n",
      "iteration : 70100, loss train : 4.786379814147949, loss test : 4.615203380584717\n",
      "iteration : 70200, loss train : 4.78004789352417, loss test : 4.609126091003418\n",
      "iteration : 70300, loss train : 4.773751258850098, loss test : 4.603085994720459\n",
      "iteration : 70400, loss train : 4.76746940612793, loss test : 4.59705924987793\n",
      "iteration : 70500, loss train : 4.761194705963135, loss test : 4.59104061126709\n",
      "iteration : 70600, loss train : 4.754937648773193, loss test : 4.585033893585205\n",
      "iteration : 70700, loss train : 4.748693943023682, loss test : 4.579037666320801\n",
      "iteration : 70800, loss train : 4.742489814758301, loss test : 4.573080062866211\n",
      "iteration : 70900, loss train : 4.736302375793457, loss test : 4.567137241363525\n",
      "iteration : 71000, loss train : 4.7301225662231445, loss test : 4.561201572418213\n",
      "iteration : 71100, loss train : 4.723949909210205, loss test : 4.555274486541748\n",
      "iteration : 71200, loss train : 4.717789173126221, loss test : 4.549357891082764\n",
      "iteration : 71300, loss train : 4.711666584014893, loss test : 4.543481826782227\n",
      "iteration : 71400, loss train : 4.7055511474609375, loss test : 4.537611961364746\n",
      "iteration : 71500, loss train : 4.69944429397583, loss test : 4.5317511558532715\n",
      "iteration : 71600, loss train : 4.693345069885254, loss test : 4.5258965492248535\n",
      "iteration : 71700, loss train : 4.687267303466797, loss test : 4.52006196975708\n",
      "iteration : 71800, loss train : 4.681222438812256, loss test : 4.514260292053223\n",
      "iteration : 71900, loss train : 4.6751861572265625, loss test : 4.5084662437438965\n",
      "iteration : 72000, loss train : 4.669157028198242, loss test : 4.502679824829102\n",
      "iteration : 72100, loss train : 4.663134574890137, loss test : 4.496901035308838\n",
      "iteration : 72200, loss train : 4.657131195068359, loss test : 4.4911394119262695\n",
      "iteration : 72300, loss train : 4.651158332824707, loss test : 4.485410213470459\n",
      "iteration : 72400, loss train : 4.645192623138428, loss test : 4.47968864440918\n",
      "iteration : 72500, loss train : 4.639235496520996, loss test : 4.473973751068115\n",
      "iteration : 72600, loss train : 4.633288383483887, loss test : 4.468268394470215\n",
      "iteration : 72700, loss train : 4.627358913421631, loss test : 4.462577819824219\n",
      "iteration : 72800, loss train : 4.621462821960449, loss test : 4.45692253112793\n",
      "iteration : 72900, loss train : 4.615573406219482, loss test : 4.4512739181518555\n",
      "iteration : 73000, loss train : 4.6096930503845215, loss test : 4.445633411407471\n",
      "iteration : 73100, loss train : 4.603818893432617, loss test : 4.439999103546143\n",
      "iteration : 73200, loss train : 4.597952842712402, loss test : 4.434372901916504\n",
      "iteration : 73300, loss train : 4.592125415802002, loss test : 4.428786754608154\n",
      "iteration : 73400, loss train : 4.586307048797607, loss test : 4.423208236694336\n",
      "iteration : 73500, loss train : 4.580498218536377, loss test : 4.417638778686523\n",
      "iteration : 73600, loss train : 4.574700355529785, loss test : 4.412078380584717\n",
      "iteration : 73700, loss train : 4.568910121917725, loss test : 4.406524658203125\n",
      "iteration : 73800, loss train : 4.563143253326416, loss test : 4.40099573135376\n",
      "iteration : 73900, loss train : 4.557401180267334, loss test : 4.3954901695251465\n",
      "iteration : 74000, loss train : 4.551665306091309, loss test : 4.389991760253906\n",
      "iteration : 74100, loss train : 4.545936107635498, loss test : 4.384500980377197\n",
      "iteration : 74200, loss train : 4.540215492248535, loss test : 4.379016876220703\n",
      "iteration : 74300, loss train : 4.534516334533691, loss test : 4.3735480308532715\n",
      "iteration : 74400, loss train : 4.5288567543029785, loss test : 4.368118762969971\n",
      "iteration : 74500, loss train : 4.5232086181640625, loss test : 4.362699031829834\n",
      "iteration : 74600, loss train : 4.517568111419678, loss test : 4.357285976409912\n",
      "iteration : 74700, loss train : 4.511934280395508, loss test : 4.351880073547363\n",
      "iteration : 74800, loss train : 4.506314754486084, loss test : 4.346487522125244\n",
      "iteration : 74900, loss train : 4.500727653503418, loss test : 4.341129779815674\n",
      "iteration : 75000, loss train : 4.495147705078125, loss test : 4.33577823638916\n",
      "iteration : 75100, loss train : 4.489575386047363, loss test : 4.330433368682861\n",
      "iteration : 75200, loss train : 4.484010219573975, loss test : 4.325096130371094\n",
      "iteration : 75300, loss train : 4.478456497192383, loss test : 4.319769859313965\n",
      "iteration : 75400, loss train : 4.472939968109131, loss test : 4.314480781555176\n",
      "iteration : 75500, loss train : 4.467432975769043, loss test : 4.309199333190918\n",
      "iteration : 75600, loss train : 4.461933612823486, loss test : 4.303924560546875\n",
      "iteration : 75700, loss train : 4.456436634063721, loss test : 4.298651218414307\n",
      "iteration : 75800, loss train : 4.4509453773498535, loss test : 4.2933831214904785\n",
      "iteration : 75900, loss train : 4.445492267608643, loss test : 4.2881550788879395\n",
      "iteration : 76000, loss train : 4.440046310424805, loss test : 4.282933235168457\n",
      "iteration : 76100, loss train : 4.43460750579834, loss test : 4.2777180671691895\n",
      "iteration : 76200, loss train : 4.42917537689209, loss test : 4.2725090980529785\n",
      "iteration : 76300, loss train : 4.423750877380371, loss test : 4.267307281494141\n",
      "iteration : 76400, loss train : 4.418361186981201, loss test : 4.262139320373535\n",
      "iteration : 76500, loss train : 4.412987232208252, loss test : 4.256986141204834\n",
      "iteration : 76600, loss train : 4.407619476318359, loss test : 4.251839637756348\n",
      "iteration : 76700, loss train : 4.402258396148682, loss test : 4.246699333190918\n",
      "iteration : 76800, loss train : 4.396904945373535, loss test : 4.241566181182861\n",
      "iteration : 76900, loss train : 4.391567230224609, loss test : 4.236449241638184\n",
      "iteration : 77000, loss train : 4.386258125305176, loss test : 4.231360912322998\n",
      "iteration : 77100, loss train : 4.380955219268799, loss test : 4.226280689239502\n",
      "iteration : 77200, loss train : 4.375659942626953, loss test : 4.221205711364746\n",
      "iteration : 77300, loss train : 4.370373725891113, loss test : 4.21613883972168\n",
      "iteration : 77400, loss train : 4.365096569061279, loss test : 4.211080074310303\n",
      "iteration : 77500, loss train : 4.3598480224609375, loss test : 4.20604944229126\n",
      "iteration : 77600, loss train : 4.354616165161133, loss test : 4.201035499572754\n",
      "iteration : 77700, loss train : 4.349389553070068, loss test : 4.196027755737305\n",
      "iteration : 77800, loss train : 4.344170570373535, loss test : 4.19102668762207\n",
      "iteration : 77900, loss train : 4.338958740234375, loss test : 4.186032295227051\n",
      "iteration : 78000, loss train : 4.3337531089782715, loss test : 4.181044578552246\n",
      "iteration : 78100, loss train : 4.3285813331604, loss test : 4.176091194152832\n",
      "iteration : 78200, loss train : 4.323419094085693, loss test : 4.17114782333374\n",
      "iteration : 78300, loss train : 4.318275451660156, loss test : 4.166215896606445\n",
      "iteration : 78400, loss train : 4.313144207000732, loss test : 4.161293983459473\n",
      "iteration : 78500, loss train : 4.308019161224365, loss test : 4.156377792358398\n",
      "iteration : 78600, loss train : 4.302915096282959, loss test : 4.151483058929443\n",
      "iteration : 78700, loss train : 4.297834396362305, loss test : 4.146612167358398\n",
      "iteration : 78800, loss train : 4.292760372161865, loss test : 4.141748428344727\n",
      "iteration : 78900, loss train : 4.287692546844482, loss test : 4.136889934539795\n",
      "iteration : 79000, loss train : 4.2826313972473145, loss test : 4.13203763961792\n",
      "iteration : 79100, loss train : 4.277585506439209, loss test : 4.127200126647949\n",
      "iteration : 79200, loss train : 4.272567272186279, loss test : 4.122392177581787\n",
      "iteration : 79300, loss train : 4.2675604820251465, loss test : 4.117591857910156\n",
      "iteration : 79400, loss train : 4.262560844421387, loss test : 4.112798690795898\n",
      "iteration : 79500, loss train : 4.257566928863525, loss test : 4.108011245727539\n",
      "iteration : 79600, loss train : 4.252580165863037, loss test : 4.103229999542236\n",
      "iteration : 79700, loss train : 4.247625827789307, loss test : 4.098483562469482\n",
      "iteration : 79800, loss train : 4.24268102645874, loss test : 4.093746185302734\n",
      "iteration : 79900, loss train : 4.2377424240112305, loss test : 4.089015007019043\n",
      "iteration : 80000, loss train : 4.232810020446777, loss test : 4.08428955078125\n",
      "iteration : 80100, loss train : 4.227883815765381, loss test : 4.0795698165893555\n",
      "iteration : 80200, loss train : 4.222976207733154, loss test : 4.074868679046631\n",
      "iteration : 80300, loss train : 4.21809720993042, loss test : 4.070194721221924\n",
      "iteration : 80400, loss train : 4.2132248878479, loss test : 4.065526008605957\n",
      "iteration : 80500, loss train : 4.208358287811279, loss test : 4.0608649253845215\n",
      "iteration : 80600, loss train : 4.203498363494873, loss test : 4.056208610534668\n",
      "iteration : 80700, loss train : 4.198644161224365, loss test : 4.051558971405029\n",
      "iteration : 80800, loss train : 4.193817615509033, loss test : 4.046936511993408\n",
      "iteration : 80900, loss train : 4.1890058517456055, loss test : 4.04232931137085\n",
      "iteration : 81000, loss train : 4.184200763702393, loss test : 4.037728309631348\n",
      "iteration : 81100, loss train : 4.1793999671936035, loss test : 4.0331315994262695\n",
      "iteration : 81200, loss train : 4.174606800079346, loss test : 4.028542518615723\n",
      "iteration : 81300, loss train : 4.169825077056885, loss test : 4.023961067199707\n",
      "iteration : 81400, loss train : 4.165072441101074, loss test : 4.019411563873291\n",
      "iteration : 81500, loss train : 4.160331726074219, loss test : 4.014872074127197\n",
      "iteration : 81600, loss train : 4.15559720993042, loss test : 4.010339260101318\n",
      "iteration : 81700, loss train : 4.150867462158203, loss test : 4.0058112144470215\n",
      "iteration : 81800, loss train : 4.146145343780518, loss test : 4.001289367675781\n",
      "iteration : 81900, loss train : 4.141428470611572, loss test : 3.9967732429504395\n",
      "iteration : 82000, loss train : 4.136739730834961, loss test : 3.992285966873169\n",
      "iteration : 82100, loss train : 4.132062911987305, loss test : 3.987811803817749\n",
      "iteration : 82200, loss train : 4.12739372253418, loss test : 3.983344078063965\n",
      "iteration : 82300, loss train : 4.122735500335693, loss test : 3.978883743286133\n",
      "iteration : 82400, loss train : 4.118083477020264, loss test : 3.9744296073913574\n",
      "iteration : 82500, loss train : 4.113446235656738, loss test : 3.9699854850769043\n",
      "iteration : 82600, loss train : 4.108837604522705, loss test : 3.9655697345733643\n",
      "iteration : 82700, loss train : 4.104243278503418, loss test : 3.9611690044403076\n",
      "iteration : 82800, loss train : 4.099654674530029, loss test : 3.9567737579345703\n",
      "iteration : 82900, loss train : 4.095071792602539, loss test : 3.9523844718933105\n",
      "iteration : 83000, loss train : 4.0904951095581055, loss test : 3.948000431060791\n",
      "iteration : 83100, loss train : 4.085931777954102, loss test : 3.9436302185058594\n",
      "iteration : 83200, loss train : 4.081395149230957, loss test : 3.9392879009246826\n",
      "iteration : 83300, loss train : 4.076868534088135, loss test : 3.9349520206451416\n",
      "iteration : 83400, loss train : 4.072348594665527, loss test : 3.9306223392486572\n",
      "iteration : 83500, loss train : 4.06783390045166, loss test : 3.9262986183166504\n",
      "iteration : 83600, loss train : 4.063324928283691, loss test : 3.921980619430542\n",
      "iteration : 83700, loss train : 4.058841705322266, loss test : 3.917687177658081\n",
      "iteration : 83800, loss train : 4.054372310638428, loss test : 3.91340970993042\n",
      "iteration : 83900, loss train : 4.049909591674805, loss test : 3.90913724899292\n",
      "iteration : 84000, loss train : 4.04545259475708, loss test : 3.9048707485198975\n",
      "iteration : 84100, loss train : 4.0410003662109375, loss test : 3.9006094932556152\n",
      "iteration : 84200, loss train : 4.036555290222168, loss test : 3.8963537216186523\n",
      "iteration : 84300, loss train : 4.032140254974365, loss test : 3.8921306133270264\n",
      "iteration : 84400, loss train : 4.027739524841309, loss test : 3.8879165649414062\n",
      "iteration : 84500, loss train : 4.023343086242676, loss test : 3.8837087154388428\n",
      "iteration : 84600, loss train : 4.0189528465271, loss test : 3.8795058727264404\n",
      "iteration : 84700, loss train : 4.0145673751831055, loss test : 3.8753087520599365\n",
      "iteration : 84800, loss train : 4.010188102722168, loss test : 3.871116876602173\n",
      "iteration : 84900, loss train : 4.005839824676514, loss test : 3.866957187652588\n",
      "iteration : 85000, loss train : 4.001500129699707, loss test : 3.862805128097534\n",
      "iteration : 85100, loss train : 3.9971649646759033, loss test : 3.8586580753326416\n",
      "iteration : 85200, loss train : 3.9928348064422607, loss test : 3.85451602935791\n",
      "iteration : 85300, loss train : 3.988511085510254, loss test : 3.8503801822662354\n",
      "iteration : 85400, loss train : 3.984196186065674, loss test : 3.846250295639038\n",
      "iteration : 85500, loss train : 3.979905605316162, loss test : 3.8421471118927\n",
      "iteration : 85600, loss train : 3.9756290912628174, loss test : 3.838057041168213\n",
      "iteration : 85700, loss train : 3.9713590145111084, loss test : 3.833972692489624\n",
      "iteration : 85800, loss train : 3.967094898223877, loss test : 3.8298935890197754\n",
      "iteration : 85900, loss train : 3.9628360271453857, loss test : 3.825819969177246\n",
      "iteration : 86000, loss train : 3.958582639694214, loss test : 3.821751356124878\n",
      "iteration : 86100, loss train : 3.9543440341949463, loss test : 3.8176980018615723\n",
      "iteration : 86200, loss train : 3.9501311779022217, loss test : 3.813669443130493\n",
      "iteration : 86300, loss train : 3.9459240436553955, loss test : 3.809645891189575\n",
      "iteration : 86400, loss train : 3.9417223930358887, loss test : 3.8056278228759766\n",
      "iteration : 86500, loss train : 3.937525510787964, loss test : 3.80161452293396\n",
      "iteration : 86600, loss train : 3.9333343505859375, loss test : 3.7976067066192627\n",
      "iteration : 86700, loss train : 3.929150104522705, loss test : 3.793604612350464\n",
      "iteration : 86800, loss train : 3.9250004291534424, loss test : 3.789632797241211\n",
      "iteration : 86900, loss train : 3.9208648204803467, loss test : 3.7856760025024414\n",
      "iteration : 87000, loss train : 3.9167346954345703, loss test : 3.781724452972412\n",
      "iteration : 87100, loss train : 3.9126105308532715, loss test : 3.777777671813965\n",
      "iteration : 87200, loss train : 3.908492088317871, loss test : 3.773836374282837\n",
      "iteration : 87300, loss train : 3.90437912940979, loss test : 3.769900321960449\n",
      "iteration : 87400, loss train : 3.9003002643585205, loss test : 3.7659988403320312\n",
      "iteration : 87500, loss train : 3.896226644515991, loss test : 3.762101411819458\n",
      "iteration : 87600, loss train : 3.8921585083007812, loss test : 3.758208990097046\n",
      "iteration : 87700, loss train : 3.888094902038574, loss test : 3.7543222904205322\n",
      "iteration : 87800, loss train : 3.8840384483337402, loss test : 3.750441074371338\n",
      "iteration : 87900, loss train : 3.8799915313720703, loss test : 3.746568441390991\n",
      "iteration : 88000, loss train : 3.8759725093841553, loss test : 3.742725372314453\n",
      "iteration : 88100, loss train : 3.8719589710235596, loss test : 3.738887310028076\n",
      "iteration : 88200, loss train : 3.867950201034546, loss test : 3.735053777694702\n",
      "iteration : 88300, loss train : 3.863947629928589, loss test : 3.7312252521514893\n",
      "iteration : 88400, loss train : 3.8599510192871094, loss test : 3.727402448654175\n",
      "iteration : 88500, loss train : 3.855961799621582, loss test : 3.7235870361328125\n",
      "iteration : 88600, loss train : 3.8520026206970215, loss test : 3.719801664352417\n",
      "iteration : 88700, loss train : 3.848047971725464, loss test : 3.7160212993621826\n",
      "iteration : 88800, loss train : 3.8440988063812256, loss test : 3.7122457027435303\n",
      "iteration : 88900, loss train : 3.840156316757202, loss test : 3.7084758281707764\n",
      "iteration : 89000, loss train : 3.836219072341919, loss test : 3.7047107219696045\n",
      "iteration : 89100, loss train : 3.832286834716797, loss test : 3.7009506225585938\n",
      "iteration : 89200, loss train : 3.828381061553955, loss test : 3.6972172260284424\n",
      "iteration : 89300, loss train : 3.8244850635528564, loss test : 3.6934943199157715\n",
      "iteration : 89400, loss train : 3.8205955028533936, loss test : 3.6897754669189453\n",
      "iteration : 89500, loss train : 3.81671142578125, loss test : 3.686063528060913\n",
      "iteration : 89600, loss train : 3.812833070755005, loss test : 3.6823556423187256\n",
      "iteration : 89700, loss train : 3.8089587688446045, loss test : 3.678652286529541\n",
      "iteration : 89800, loss train : 3.8050971031188965, loss test : 3.6749610900878906\n",
      "iteration : 89900, loss train : 3.8012592792510986, loss test : 3.6712942123413086\n",
      "iteration : 90000, loss train : 3.797428846359253, loss test : 3.667633533477783\n",
      "iteration : 90100, loss train : 3.793602466583252, loss test : 3.6639769077301025\n",
      "iteration : 90200, loss train : 3.7897815704345703, loss test : 3.660325527191162\n",
      "iteration : 90300, loss train : 3.7859654426574707, loss test : 3.6566784381866455\n",
      "iteration : 90400, loss train : 3.7821543216705322, loss test : 3.653035879135132\n",
      "iteration : 90500, loss train : 3.7783620357513428, loss test : 3.6494123935699463\n",
      "iteration : 90600, loss train : 3.7745890617370605, loss test : 3.6458072662353516\n",
      "iteration : 90700, loss train : 3.770820140838623, loss test : 3.642206907272339\n",
      "iteration : 90800, loss train : 3.7670559883117676, loss test : 3.638611078262329\n",
      "iteration : 90900, loss train : 3.763296365737915, loss test : 3.6350200176239014\n",
      "iteration : 91000, loss train : 3.7595417499542236, loss test : 3.6314327716827393\n",
      "iteration : 91100, loss train : 3.755794048309326, loss test : 3.6278514862060547\n",
      "iteration : 91200, loss train : 3.752063751220703, loss test : 3.6242878437042236\n",
      "iteration : 91300, loss train : 3.748353958129883, loss test : 3.6207432746887207\n",
      "iteration : 91400, loss train : 3.744655132293701, loss test : 3.6172077655792236\n",
      "iteration : 91500, loss train : 3.7409613132476807, loss test : 3.613677501678467\n",
      "iteration : 91600, loss train : 3.7372727394104004, loss test : 3.610151529312134\n",
      "iteration : 91700, loss train : 3.733590602874756, loss test : 3.606630802154541\n",
      "iteration : 91800, loss train : 3.7299182415008545, loss test : 3.603120803833008\n",
      "iteration : 91900, loss train : 3.7262721061706543, loss test : 3.5996363162994385\n",
      "iteration : 92000, loss train : 3.722630023956299, loss test : 3.596156358718872\n",
      "iteration : 92100, loss train : 3.7189927101135254, loss test : 3.5926804542541504\n",
      "iteration : 92200, loss train : 3.7153615951538086, loss test : 3.589209794998169\n",
      "iteration : 92300, loss train : 3.711735725402832, loss test : 3.5857439041137695\n",
      "iteration : 92400, loss train : 3.708116054534912, loss test : 3.582284688949585\n",
      "iteration : 92500, loss train : 3.7045247554779053, loss test : 3.5788538455963135\n",
      "iteration : 92600, loss train : 3.7009377479553223, loss test : 3.575427293777466\n",
      "iteration : 92700, loss train : 3.697355270385742, loss test : 3.572005033493042\n",
      "iteration : 92800, loss train : 3.693779945373535, loss test : 3.5685887336730957\n",
      "iteration : 92900, loss train : 3.6902081966400146, loss test : 3.565176010131836\n",
      "iteration : 93000, loss train : 3.6866416931152344, loss test : 3.561767578125\n",
      "iteration : 93100, loss train : 3.6830971240997314, loss test : 3.558382511138916\n",
      "iteration : 93200, loss train : 3.6795644760131836, loss test : 3.55500864982605\n",
      "iteration : 93300, loss train : 3.676037073135376, loss test : 3.5516395568847656\n",
      "iteration : 93400, loss train : 3.672515392303467, loss test : 3.548275947570801\n",
      "iteration : 93500, loss train : 3.6689977645874023, loss test : 3.5449159145355225\n",
      "iteration : 93600, loss train : 3.6654858589172363, loss test : 3.5415611267089844\n",
      "iteration : 93700, loss train : 3.6619791984558105, loss test : 3.5382115840911865\n",
      "iteration : 93800, loss train : 3.6585004329681396, loss test : 3.5348904132843018\n",
      "iteration : 93900, loss train : 3.655026912689209, loss test : 3.531574010848999\n",
      "iteration : 94000, loss train : 3.6515588760375977, loss test : 3.5282623767852783\n",
      "iteration : 94100, loss train : 3.64809513092041, loss test : 3.5249547958374023\n",
      "iteration : 94200, loss train : 3.644636392593384, loss test : 3.5216517448425293\n",
      "iteration : 94300, loss train : 3.641181230545044, loss test : 3.518352508544922\n",
      "iteration : 94400, loss train : 3.637735605239868, loss test : 3.5150625705718994\n",
      "iteration : 94500, loss train : 3.634315252304077, loss test : 3.5117974281311035\n",
      "iteration : 94600, loss train : 3.6309003829956055, loss test : 3.5085372924804688\n",
      "iteration : 94700, loss train : 3.6274895668029785, loss test : 3.505281686782837\n",
      "iteration : 94800, loss train : 3.6240830421447754, loss test : 3.5020294189453125\n",
      "iteration : 94900, loss train : 3.6206812858581543, loss test : 3.498781681060791\n",
      "iteration : 95000, loss train : 3.6172831058502197, loss test : 3.495537757873535\n",
      "iteration : 95100, loss train : 3.6138925552368164, loss test : 3.492300271987915\n",
      "iteration : 95200, loss train : 3.610529899597168, loss test : 3.489091157913208\n",
      "iteration : 95300, loss train : 3.6071715354919434, loss test : 3.4858858585357666\n",
      "iteration : 95400, loss train : 3.603816509246826, loss test : 3.4826841354370117\n",
      "iteration : 95500, loss train : 3.600466728210449, loss test : 3.479487657546997\n",
      "iteration : 95600, loss train : 3.597121477127075, loss test : 3.476294755935669\n",
      "iteration : 95700, loss train : 3.5937819480895996, loss test : 3.4731063842773438\n",
      "iteration : 95800, loss train : 3.5904464721679688, loss test : 3.4699227809906006\n",
      "iteration : 95900, loss train : 3.587130546569824, loss test : 3.4667575359344482\n",
      "iteration : 96000, loss train : 3.5838279724121094, loss test : 3.463606834411621\n",
      "iteration : 96100, loss train : 3.5805294513702393, loss test : 3.4604597091674805\n",
      "iteration : 96200, loss train : 3.577240228652954, loss test : 3.457320213317871\n",
      "iteration : 96300, loss train : 3.573960065841675, loss test : 3.4541873931884766\n",
      "iteration : 96400, loss train : 3.570683717727661, loss test : 3.451058864593506\n",
      "iteration : 96500, loss train : 3.5674121379852295, loss test : 3.447934150695801\n",
      "iteration : 96600, loss train : 3.564159870147705, loss test : 3.444828987121582\n",
      "iteration : 96700, loss train : 3.5609211921691895, loss test : 3.4417383670806885\n",
      "iteration : 96800, loss train : 3.557687520980835, loss test : 3.4386513233184814\n",
      "iteration : 96900, loss train : 3.5544590950012207, loss test : 3.4355685710906982\n",
      "iteration : 97000, loss train : 3.551257610321045, loss test : 3.4325125217437744\n",
      "iteration : 97100, loss train : 3.5480711460113525, loss test : 3.429471731185913\n",
      "iteration : 97200, loss train : 3.544893503189087, loss test : 3.426438808441162\n",
      "iteration : 97300, loss train : 3.5417444705963135, loss test : 3.423435926437378\n",
      "iteration : 97400, loss train : 3.538600206375122, loss test : 3.420436382293701\n",
      "iteration : 97500, loss train : 3.5354604721069336, loss test : 3.4174416065216064\n",
      "iteration : 97600, loss train : 3.5323259830474854, loss test : 3.4144508838653564\n",
      "iteration : 97700, loss train : 3.5291943550109863, loss test : 3.411463499069214\n",
      "iteration : 97800, loss train : 3.5260674953460693, loss test : 3.408479928970337\n",
      "iteration : 97900, loss train : 3.5229604244232178, loss test : 3.405517339706421\n",
      "iteration : 98000, loss train : 3.5198659896850586, loss test : 3.40256667137146\n",
      "iteration : 98100, loss train : 3.516777276992798, loss test : 3.39962100982666\n",
      "iteration : 98200, loss train : 3.5136923789978027, loss test : 3.396678924560547\n",
      "iteration : 98300, loss train : 3.510611057281494, loss test : 3.393740653991699\n",
      "iteration : 98400, loss train : 3.5075345039367676, loss test : 3.390805721282959\n",
      "iteration : 98500, loss train : 3.5044612884521484, loss test : 3.3878753185272217\n",
      "iteration : 98600, loss train : 3.5014142990112305, loss test : 3.3849713802337646\n",
      "iteration : 98700, loss train : 3.4983742237091064, loss test : 3.3820724487304688\n",
      "iteration : 98800, loss train : 3.495338201522827, loss test : 3.379178047180176\n",
      "iteration : 98900, loss train : 3.492307424545288, loss test : 3.376288414001465\n",
      "iteration : 99000, loss train : 3.4892804622650146, loss test : 3.373401641845703\n",
      "iteration : 99100, loss train : 3.4862570762634277, loss test : 3.3705193996429443\n",
      "iteration : 99200, loss train : 3.483240842819214, loss test : 3.3676435947418213\n",
      "iteration : 99300, loss train : 3.4802515506744385, loss test : 3.364793539047241\n",
      "iteration : 99400, loss train : 3.477266550064087, loss test : 3.361948013305664\n",
      "iteration : 99500, loss train : 3.4742848873138428, loss test : 3.359105348587036\n",
      "iteration : 99600, loss train : 3.471306800842285, loss test : 3.356266975402832\n",
      "iteration : 99700, loss train : 3.4683327674865723, loss test : 3.3534319400787354\n",
      "iteration : 99800, loss train : 3.465362071990967, loss test : 3.350600242614746\n",
      "iteration : 99900, loss train : 3.462405204772949, loss test : 3.3477816581726074\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Chargement des données California et transformation en tensor.\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing() ## chargement des données\n",
    "data_x = torch.tensor(housing['data'],dtype=torch.float)\n",
    "data_y = torch.tensor(housing['target'],dtype=torch.float)\n",
    "\n",
    "print(\"Nombre d'exemples : \",data_x.size(0), \"Dimension : \",data_x.size(1))\n",
    "#print(data_x,data_y)\n",
    "\n",
    "\n",
    "torch.random.manual_seed(1)\n",
    "\n",
    "#initialisation aléatoire de w et b\n",
    "w = torch.randn(1,data_x.size(1),requires_grad=True)\n",
    "b =  torch.randn(1,1,requires_grad=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## [[student]] \n",
    "inds=torch.randperm(data_x.shape[0])\n",
    "n_train=int(data_x.shape[0]*0.8)\n",
    "\n",
    "train_x=data_x[inds[:n_train]]\n",
    "train_y=data_y[inds[:n_train]]\n",
    "test_x=data_x[inds[n_train:]]\n",
    "test_y=data_y[inds[n_train:]]\n",
    "## [[/student]] \n",
    "\n",
    "\n",
    "\n",
    "EPOCHS = 100000\n",
    "EPS = 1e-7\n",
    "for i in range(EPOCHS):\n",
    "    ## [[student]] \n",
    "    if i % 100==0:  \n",
    "        print(f\"iteration : {i}, loss train : {getLoss(train_x,w,b,train_y)}, loss test : {getLoss(test_x,w,b,test_y)}\")\n",
    "        \n",
    "    #calcul du gradient\n",
    "    w_grad,b_grad=getGradient(train_x,w,b,train_y)\n",
    "    # Maj des paramètres\n",
    "    w = w-EPS*w_grad\n",
    "    b = b-EPS*b_grad\n",
    "    ## [[/student]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T13:30:23.698874470Z",
     "start_time": "2024-02-06T13:30:23.694728773Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T13:30:24.122026748Z",
     "start_time": "2024-02-06T13:30:24.111602080Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T13:30:24.512587942Z",
     "start_time": "2024-02-06T13:30:24.499086564Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T13:30:24.966563851Z",
     "start_time": "2024-02-06T13:30:24.960506775Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DeepLearning fc TP1 2020-2021-correction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
